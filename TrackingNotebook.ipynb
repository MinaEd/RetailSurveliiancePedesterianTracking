{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T14:06:00.715986Z",
     "iopub.status.busy": "2025-03-21T14:06:00.715792Z",
     "iopub.status.idle": "2025-03-21T14:06:07.976264Z",
     "shell.execute_reply": "2025-03-21T14:06:07.975175Z",
     "shell.execute_reply.started": "2025-03-21T14:06:00.715965Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.94-py3-none-any.whl.metadata (35 kB)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n",
      "Downloading ultralytics-8.3.94-py3-none-any.whl (949 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: ultralytics-thop, ultralytics\n",
      "Successfully installed ultralytics-8.3.94 ultralytics-thop-2.0.14\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T01:13:37.664151Z",
     "iopub.status.busy": "2025-03-17T01:13:37.663907Z",
     "iopub.status.idle": "2025-03-17T01:13:39.181396Z",
     "shell.execute_reply": "2025-03-17T01:13:39.180320Z",
     "shell.execute_reply.started": "2025-03-17T01:13:37.664127Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deep_sort_realtime\n",
      "  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.91)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deep_sort_realtime) (1.26.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from deep_sort_realtime) (1.13.1)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from deep_sort_realtime) (4.10.0.84)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->deep_sort_realtime) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->deep_sort_realtime) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->deep_sort_realtime) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->deep_sort_realtime) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->deep_sort_realtime) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->deep_sort_realtime) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->deep_sort_realtime) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->deep_sort_realtime) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->deep_sort_realtime) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->deep_sort_realtime) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->deep_sort_realtime) (2024.2.0)\n",
      "Downloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install deep_sort_realtime ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.252Z",
     "iopub.execute_input": "2025-03-17T01:13:39.183789Z",
     "iopub.status.busy": "2025-03-17T01:13:39.183544Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'BoT-SORT'...\n",
      "remote: Enumerating objects: 1013, done.\u001b[K\n",
      "remote: Total 1013 (delta 0), reused 0 (delta 0), pack-reused 1013 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1013/1013), 55.77 MiB | 53.12 MiB/s, done.\n",
      "Resolving deltas: 100% (286/286), done.\n",
      "Cloning into 'Yolov8_DeepSORT_OSNet'...\n",
      "Username for 'https://github.com': "
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NirAharon/BoT-SORT.git\n",
    "!git clone https://github.com/mikel-brostrom/Yolov8_DeepSORT_OSNet.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "model = YOLO(\"yolov8m.pt\")\n",
    "\n",
    "\n",
    "model.train(\n",
    "    data=\"/kaggle/input/trackingfull/trackingfull/fawry.yaml\",  \n",
    "    epochs=50,          \n",
    "    imgsz=704,            \n",
    "    batch=8,             \n",
    "    workers=2,           \n",
    "    optimizer=\"Adam\",     \n",
    "    lr0=0.0001,           \n",
    "    dropout=0.1,          \n",
    "    augment=True,         \n",
    "    hsv_h=0.015,          \n",
    "    hsv_s=0.7,            \n",
    "    hsv_v=0.4,            \n",
    "    flipud=0.3,           \n",
    "    fliplr=0.5,           \n",
    "    mosaic=0.5,           \n",
    "    mixup=0.0,            \n",
    "    degrees=5,            \n",
    "    translate=0.1,        \n",
    "    scale=0.5,            \n",
    "    shear=2,              \n",
    "    perspective=0.001,\n",
    "    save_period=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from ultralytics import YOLO  # Assuming you used YOLO for tracking\n",
    "\n",
    "# Load your trained model\n",
    "model_path = \"/kaggle/input/model-trial/pytorch/default/1/best (2).pt\"  # Update with your model path\n",
    "tracking_model = YOLO(model_path)  \n",
    "\n",
    "# Path to test video frames\n",
    "test_frames_dir = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"  # Update with your test frames directory\n",
    "output_csv = \"/kaggle/working/tracking_results.csv\"\n",
    "\n",
    "# Get list of image frames\n",
    "frame_files = sorted([f for f in os.listdir(test_frames_dir) if f.endswith(\".jpg\")])\n",
    "total_frames = len(frame_files)\n",
    "\n",
    "# Initialize an empty list to store tracking results\n",
    "tracking_data = []\n",
    "\n",
    "# Process each test frame\n",
    "for frame_idx, frame_name in enumerate(frame_files, start=1):  \n",
    "    frame_path = os.path.join(test_frames_dir, frame_name)\n",
    "    frame = cv2.imread(frame_path)\n",
    "\n",
    "    if frame is None:\n",
    "        print(f\"Warning: Could not load frame {frame_path}. Skipping.\")\n",
    "        continue  # Skip this frame\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB for YOLO\n",
    "\n",
    "    # Run model inference for tracking\n",
    "    results = tracking_model.track(frame, persist=False)  # Disable persist to avoid tracking errors\n",
    "\n",
    "    for result in results:\n",
    "        if result.boxes:\n",
    "            for box in result.boxes:\n",
    "                x, y, w, h = map(int, box.xywh[0])  # Bounding box (x, y, width, height)\n",
    "                confidence = float(box.conf[0])  # Confidence score\n",
    "                track_id = int(box.id[0]) if box.id is not None else -1  # Handle missing track IDs\n",
    "\n",
    "                tracking_data.append({\n",
    "                    \"frame\": frame_idx,\n",
    "                    \"track_id\": track_id,\n",
    "                    \"x\": x,\n",
    "                    \"y\": y,\n",
    "                    \"width\": w,\n",
    "                    \"height\": h,\n",
    "                    \"confidence\": confidence\n",
    "                })\n",
    "\n",
    "# Save results to CSV\n",
    "df = pd.DataFrame(tracking_data)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"Tracking results saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tracking_model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tracking_model.train(data=\"/kaggle/input/trackingfinal/trackingfinal/fawry.yaml\",\n",
    "    epochs=20,\n",
    "    imgsz=704,\n",
    "    batch=8,\n",
    "    workers=2,\n",
    "    save_period=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model=YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\")\n",
    "model.train(epochs=20,\n",
    "    imgsz=704,\n",
    "    batch=8,\n",
    "    workers=2,\n",
    "    save_period=10,\n",
    "           resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model2=YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (2).pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model2.train(epochs=1,\n",
    "    imgsz=704,\n",
    "    batch=8,\n",
    "    workers=2,\n",
    "    save_period=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model3=YOLO(\"/kaggle/working/runs/detect/train/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "image_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1/000051.jpg\"  # Replace with your test image path\n",
    "image = cv2.imread(image_path)\n",
    "model2=YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\")\n",
    "# Resize the image to 704x704 while keeping aspect ratio\n",
    "image_resized = cv2.resize(image, (704, 704))\n",
    "\n",
    "# Run YOLO inference on the resized image\n",
    "results = model2(image_resized)\n",
    "\n",
    "# Convert image to RGB for display\n",
    "image_resized_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Iterate through detections\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box coordinates\n",
    "        conf = float(box.conf[0])  # Confidence score\n",
    "        cls = int(box.cls[0])  # Class ID\n",
    "        label = f\"{model2.names[cls]}: {conf:.2f}\"\n",
    "\n",
    "        # Draw bounding box and label\n",
    "        cv2.rectangle(image_resized_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image_resized_rgb, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with bounding boxes\n",
    "plt.imshow(image_resized_rgb)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Reload the tracking results\n",
    "output_csv = \"/kaggle/working/tracking_results.csv\"\n",
    "df = pd.read_csv(output_csv)\n",
    "\n",
    "# Directory containing the frames\n",
    "test_frames_dir = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"  # Update if needed\n",
    "\n",
    "# Select a frame to visualize\n",
    "frame_to_show = df[\"frame\"].iloc[0]  # Get the first tracked frame\n",
    "frame_path = os.path.join(test_frames_dir, f\"{frame_to_show:06d}.jpg\")  # Adjust if needed\n",
    "frame = cv2.imread(frame_path)\n",
    "\n",
    "if frame is None:\n",
    "    print(f\"Error loading frame {frame_path}\")\n",
    "else:\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Get bounding boxes for the selected frame\n",
    "    detections = df[df[\"frame\"] == frame_to_show]\n",
    "\n",
    "    for _, row in detections.iterrows():\n",
    "        x, y, w, h = int(row[\"x\"]), int(row[\"y\"]), int(row[\"width\"]), int(row[\"height\"])\n",
    "        track_id = int(row[\"track_id\"])\n",
    "        \n",
    "        # Draw bounding box (fixed)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, f\"ID: {track_id}\", (x, y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    # Show image\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(frame)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load your trained model\n",
    "model_path = \"/kaggle/input/model-trial/pytorch/default/1/best (2).pt\"  # Update with your model path\n",
    "tracking_model = YOLO(model_path)\n",
    "\n",
    "# Input frame\n",
    "frame_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1/000016.jpg\"  # Update with your image path\n",
    "frame = cv2.imread(frame_path)\n",
    "\n",
    "if frame is None:\n",
    "    print(f\"Error: Could not load frame {frame_path}\")\n",
    "else:\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB for YOLO\n",
    "\n",
    "    # Run tracking on the frame\n",
    "    results = tracking_model.track(frame, persist=False)  # Disable persist to avoid errors\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    for result in results:\n",
    "        if result.boxes:\n",
    "            for box in result.boxes:\n",
    "                x, y, w, h = map(int, box.xywh[0])  # Convert bounding box to int\n",
    "                confidence = float(box.conf[0])  # Confidence score\n",
    "                track_id = int(box.id[0]) if box.id is not None else -1  # Assign track ID\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "                cv2.putText(frame, f\"ID: {track_id}\", (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    # Convert back to BGR for saving (OpenCV saves in BGR format)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Save the output image\n",
    "    output_path = \"/kaggle/working/tracked_frame.jpg\"  # Save output image\n",
    "    cv2.imwrite(output_path, frame)\n",
    "\n",
    "    # Display the image\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"✅ Tracking results saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define image and label paths\n",
    "image_folder = \"/kaggle/input/tracking/tracking/tracking/train/02/img1/\"  # Update with your actual folder\n",
    "label_path = \"/kaggle/input/gtof02/gt.txt\"  # Update with actual label path\n",
    "\n",
    "# Define frame of interest\n",
    "frame_of_interest = 1478  # Update this with your desired frame\n",
    "\n",
    "# Generate corresponding image filename\n",
    "image_filename = f\"{frame_of_interest:06d}.jpg\"  # Converts 1312 to \"001312.jpg\"\n",
    "image_path = os.path.join(image_folder, image_filename)\n",
    "\n",
    "# Check if image exists\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"Image for frame {frame_of_interest} not found: {image_path}\")\n",
    "else:\n",
    "    print(f\"Image for frame {frame_of_interest} found: {image_path}\")\n",
    "\n",
    "    # Load and display the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Frame {frame_of_interest}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define image and label paths (update accordingly)\n",
    "image_path = \"/kaggle/input/tracking/tracking/tracking/train/02/img1/000025.jpg\"  # Update this with your actual image path\n",
    "label_path = \"/kaggle/input/gtof02/gt.txt\"  # Update this with your actual label path\n",
    "\n",
    "# Extract frame number from image filename\n",
    "image_filename = os.path.basename(image_path)\n",
    "frame_of_interest = int(image_filename.split('.')[0])  # Convert to integer\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB (since OpenCV loads in BGR format)\n",
    "height, width, _ = image.shape\n",
    "\n",
    "# Read MOT label file and draw bounding boxes\n",
    "unique_classes = set()\n",
    "\n",
    "with open(label_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        data = line.strip().split(',')  # Split by comma since MOT format is CSV-like\n",
    "        frame_num = int(data[0])\n",
    "        \n",
    "        if frame_num == frame_of_interest:  # Process only the required frame\n",
    "            track_id = int(data[1])\n",
    "            x1 = int(float(data[2]))  # X (top-left corner)\n",
    "            y1 = int(float(data[3]))  # Y (top-left corner)\n",
    "            w = int(float(data[4]))   # Width\n",
    "            h = int(float(data[5]))   # Height\n",
    "            confidence = float(data[6])  # Confidence score\n",
    "            class_id = int(data[7])  # Class ID from the file\n",
    "            visibility = float(data[8])  # Visibility score\n",
    "            \n",
    "            unique_classes.add(class_id)\n",
    "            \n",
    "            x2 = x1 + w\n",
    "            y2 = y1 + h\n",
    "            \n",
    "            # Draw bounding box\n",
    "            color = (255, 0, 0)  # Red color for bounding box\n",
    "            thickness = 2\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
    "            \n",
    "            # Put class label\n",
    "            label_text = f\"Class: {class_id} (ID: {track_id})\"\n",
    "            cv2.putText(image, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "# Display image\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Print unique class IDs\n",
    "print(\"Unique Class IDs in Frame:\", unique_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define image and label paths (update accordingly)\n",
    "image_path = \"/kaggle/input/tracking/tracking/tracking/train/05/img1/000001.jpg\"  # Update this with your actual image path\n",
    "label_path = \"/kaggle/input/gtof05/gt.txt\"  # Update this with your actual label path\n",
    "\n",
    "# Extract frame number from image filename\n",
    "image_filename = os.path.basename(image_path)\n",
    "frame_of_interest = int(image_filename.split('.')[0])  # Convert to integer\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB (since OpenCV loads in BGR format)\n",
    "height, width, _ = image.shape\n",
    "\n",
    "# Read MOT label file and draw bounding boxes\n",
    "unique_classes = set()\n",
    "\n",
    "with open(label_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        data = line.strip().split(',')  # Split by comma since MOT format is CSV-like\n",
    "        frame_num = int(data[0])\n",
    "        \n",
    "        if frame_num == frame_of_interest:  # Process only the required frame\n",
    "            track_id = int(data[1])\n",
    "            x1 = int(float(data[2]))  # X (top-left corner)\n",
    "            y1 = int(float(data[3]))  # Y (top-left corner)\n",
    "            w = int(float(data[4]))   # Width\n",
    "            h = int(float(data[5]))   # Height\n",
    "            confidence = float(data[6])  # Confidence score\n",
    "            class_id = int(data[7])  # Class ID from the file\n",
    "            visibility = float(data[8])  # Visibility score\n",
    "            \n",
    "            unique_classes.add(class_id)\n",
    "            \n",
    "            if class_id == 1:  # Filter to show only class 6\n",
    "                x2 = x1 + w\n",
    "                y2 = y1 + h\n",
    "                \n",
    "                # Draw bounding box\n",
    "                color = (255, 0, 0)  # Red color for bounding box\n",
    "                thickness = 2\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
    "                \n",
    "                # Put class label\n",
    "                label_text = f\"Class: {class_id} (ID: {track_id})\"\n",
    "                cv2.putText(image, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "# Display image\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Print unique class IDs\n",
    "print(\"Unique Class IDs in Frame:\", unique_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Script to extract unique class IDs from the entire GT file\n",
    "\n",
    "import os\n",
    "\n",
    "# Define GT file path\n",
    "label_path = \"/kaggle/input/gtof05/gt.txt\"  # Update this with your actual label path\n",
    "\n",
    "# Read MOT label file and extract unique class IDs\n",
    "unique_classes = set()\n",
    "\n",
    "with open(label_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        data = line.strip().split(',')  # Split by comma since MOT format is CSV-like\n",
    "        class_id = int(data[7])  # Extract class ID\n",
    "        unique_classes.add(class_id)\n",
    "\n",
    "# Print all unique class IDs\n",
    "print(\"Unique Class IDs in the GT file:\", unique_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"path/to/your/mot_annotations.txt\"\n",
    "\n",
    "# Load the MOT annotation file (adjust separator if needed)\n",
    "df = pd.read_csv(file_path, sep=\" \", header=None)\n",
    "\n",
    "# The class column is the second-to-last (-2)\n",
    "class_counts = df.iloc[:, -2].value_counts()\n",
    "\n",
    "print(\"Class distribution in the dataset:\\n\", class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define paths (update accordingly)\n",
    "image_path = \"/kaggle/input/tracking/tracking/tracking/train/03/img1/000063.jpg\"  # Update this\n",
    "label_path = \"/kaggle/input/gtof03/000063.txt\"  # Update this\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "height, width, _ = image.shape\n",
    "\n",
    "# Read YOLO label file\n",
    "with open(label_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Loop through each bounding box\n",
    "for line in lines:\n",
    "    data = line.strip().split()\n",
    "    class_id = int(data[0])  # Class ID\n",
    "    x_center = float(data[1]) * width\n",
    "    y_center = float(data[2]) * height\n",
    "    bbox_width = float(data[3]) * width\n",
    "    bbox_height = float(data[4]) * height\n",
    "\n",
    "    # Convert to (x1, y1, x2, y2) format\n",
    "    x1 = int(x_center - bbox_width / 2)\n",
    "    y1 = int(y_center - bbox_height / 2)\n",
    "    x2 = int(x_center + bbox_width / 2)\n",
    "    y2 = int(y_center + bbox_height / 2)\n",
    "\n",
    "    # Draw bounding box\n",
    "    color = (255, 0, 0)  # Red color\n",
    "    thickness = 2\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "    # Display class label\n",
    "    label_text = f\"Class: {class_id}\"\n",
    "    cv2.putText(image, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "# Display the image with bounding boxes\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"YOLOv8 Bounding Boxes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install git+https://github.com/ifzhang/ByteTrack.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.float = np.float32  # Temporary fix for deprecated alias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install deep-sort-realtime ultralytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model2 = YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\")\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "gt_file_path = \"/kaggle/input/testgt/gt.txt\"\n",
    "output_mot_path = \"/kaggle/working/modeltrackingresults.txt\"\n",
    "visualization_output = \"/kaggle/working/modeltracking_output\"\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(visualization_output, exist_ok=True)\n",
    "\n",
    "# Resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704  # YOLOv8 input size\n",
    "target_height = 704\n",
    "\n",
    "# Tracking results storage\n",
    "tracking_results = []\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "    # Load and resize image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "    # Run YOLOv8 tracking with persistent IDs\n",
    "    results = model2.track(img_resized, persist=True)[0]  \n",
    "\n",
    "    for track in results.boxes:\n",
    "        x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "        track_id = int(track.id[0].item()) if track.id is not None else -1  # Ensure track_id persists\n",
    "        conf = track.conf[0].item()\n",
    "\n",
    "        # Keep only person class (1 & 7 mapped to 1)\n",
    "        cls = int(track.cls[0].item())\n",
    "        if cls == 0:\n",
    "            # Convert to original image size\n",
    "            x_center *= (original_width / target_width)\n",
    "            y_center *= (original_height / target_height)\n",
    "            w *= (original_width / target_width)\n",
    "            h *= (original_height / target_height)\n",
    "\n",
    "            # Convert to top-left format\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "            w = int(w)\n",
    "            h = int(h)\n",
    "\n",
    "            # Save tracking result in MOT format\n",
    "            tracking_results.append([frame_number, track_id, x, y, w, h, round(conf,4), 1, 1])\n",
    "\n",
    "            # Draw bounding boxes on image\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\"ID {track_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Save visualization\n",
    "    cv2.imwrite(os.path.join(visualization_output, img_name), img)\n",
    "\n",
    "# Sort tracking results by track_id first, then by frame number\n",
    "track_id_data = {}\n",
    "\n",
    "for row in tracking_results:\n",
    "    frame_number, track_id = row[0], row[1]\n",
    "    if track_id not in track_id_data:\n",
    "        track_id_data[track_id] = []\n",
    "    track_id_data[track_id].append(row)\n",
    "\n",
    "# Save results in MOT format\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for track_id in sorted(track_id_data.keys()):  # Sort by track_id first\n",
    "        for row in sorted(track_id_data[track_id], key=lambda x: x[0]):  # Sort frames within each ID\n",
    "            f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")\n",
    "print(f\"Visualized results saved in {visualization_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Define folder paths\n",
    "visualization_output = \"/kaggle/working/modeltracking_output\"\n",
    "zip_path = \"/kaggle/working/visualized_modeltrack_output.zip\"\n",
    "\n",
    "# Zip the folder\n",
    "shutil.make_archive(zip_path.replace(\".zip\", \"\"), 'zip', visualization_output)\n",
    "\n",
    "print(f\"Zipped visualization folder saved at: {zip_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bytetrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T01:16:55.766578Z",
     "iopub.status.busy": "2025-03-17T01:16:55.766286Z",
     "iopub.status.idle": "2025-03-17T01:16:59.685989Z",
     "shell.execute_reply": "2025-03-17T01:16:59.685213Z",
     "shell.execute_reply.started": "2025-03-17T01:16:55.766555Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.91)\n",
      "Requirement already satisfied: lap in /usr/local/lib/python3.10/dist-packages (0.5.12)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (3.0.11)\n",
      "Collecting motmetrics\n",
      "  Downloading motmetrics-1.4.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting onemetric\n",
      "  Downloading onemetric-0.1.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\n",
      "Collecting xmltodict>=0.12.0 (from motmetrics)\n",
      "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from onemetric) (0.6.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->onemetric) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->onemetric) (0.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->onemetric) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n",
      "Downloading motmetrics-1.4.0-py3-none-any.whl (161 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onemetric-0.1.2-py3-none-any.whl (21 kB)\n",
      "Downloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: xmltodict, onemetric, motmetrics\n",
      "Successfully installed motmetrics-1.4.0 onemetric-0.1.2 xmltodict-0.14.2\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics lap cython motmetrics onemetric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ifzhang/ByteTrack.git\n",
    "!pip install -e ByteTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install cython_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/working/ByteTrack\")  # Ensure ByteTrack is in path\n",
    "\n",
    "from yolox.tracker.byte_tracker import BYTETracker  # Correct import\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install yolox.tracker.tracking_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U yolox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pyyaml  # (Skip this step if PyYAML is already installed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install filterpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the path to your bytetrack.yaml file\n",
    "file_path = \"/kaggle/working/bytetrack.yaml\"\n",
    "\n",
    "# Load the existing YAML file\n",
    "with open(file_path, \"r\") as file:\n",
    "    bytetrack_config = yaml.safe_load(file)\n",
    "# tracking_0.6_0.1_0.3_0.85_30_0.2_True.txt\n",
    "\n",
    "# \"tracker_type\": \"bytetrack\",\n",
    "#         \"track_high_thresh\": params[0],\n",
    "#         \"track_low_thresh\": params[1],\n",
    "#         \"iou_thresh\": params[2],\n",
    "#         \"match_thresh\": params[3],\n",
    "#         \"track_buffer\": params[4],\n",
    "#         \"new_track_thresh\": params[5],\n",
    "#         \"fuse_score\": params[6],\n",
    "\n",
    "\n",
    "# Modify the values with optimized settings\n",
    "bytetrack_config[\"tracker_type\"] = \"bytetrack\"\n",
    "bytetrack_config[\"track_high_thresh\"] = 0.6  # Higher confidence filtering for stable tracks ##effect\n",
    "bytetrack_config[\"track_low_thresh\"] = 0.05  # Keep lower-confidence detections to maintain recall ##no\n",
    "bytetrack_config[\"new_track_thresh\"] = 0.2  # Prevent frequent new ID assignments #no\n",
    "bytetrack_config[\"iou_thresh\"] = 0.7  # Higher IoU helps in tracking objects with slight movement #no\n",
    "bytetrack_config[\"match_thresh\"] = 0.83  # Reduce ID switches by making matching stricter #bigeffect\n",
    "bytetrack_config[\"track_buffer\"] = 30  # Increase buffer to maintain identity over more frames #small \n",
    "bytetrack_config[\"fuse_score\"] = True  # Enable score fusion for better tracking #true better\n",
    "bytetrack_config[\"frame_rate\"] = 25  # Ensure tracking is adjusted for 25 FPS videos\n",
    "bytetrack_config[\"det_thresh\"] = 0.5  # Detection confidence threshold #no effect\n",
    "bytetrack_config[\"lost_track_thresh\"] = 10  # Use confidence fusion for better tracking #no effect\n",
    "# Save the updated YAML file\n",
    "with open(file_path, \"w\") as file:\n",
    "    yaml.dump(bytetrack_config, file, default_flow_style=False)\n",
    "\n",
    "print(\"bytetrack.yaml has been updated successfully with optimized settings!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q strongsort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NirAharon/BoT-SORT.git\n",
    "%cd BoT-SORT\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/working/BoT-SORT\")  # Adjust this path if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install fastreid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "if not hasattr(collections, 'Mapping'):\n",
    "    import collections.abc\n",
    "    collections.Mapping = collections.abc.Mapping\n",
    "file_path = \"/kaggle/working/BoT-SORT/fast_reid/fastreid/data/build.py\"\n",
    "\n",
    "# Read the file and modify the import statement\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "new_lines = []\n",
    "for line in lines:\n",
    "    if \"from collections import Mapping\" in line:\n",
    "        new_lines.append(\"from collections.abc import Mapping\\n\")\n",
    "    else:\n",
    "        new_lines.append(line)\n",
    "\n",
    "# Write the modified content back to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.writelines(new_lines)\n",
    "\n",
    "print(\"✅ Fix applied successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/working/BoT-SORT/fast_reid/fastreid/evaluation/testing.py\"\n",
    "\n",
    "# Read the file and modify the import statement\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "new_lines = []\n",
    "for line in lines:\n",
    "    if \"from collections.abc import Mapping, OrderedDict\" in line:\n",
    "        new_lines.append(\"from collections.abc import Mapping\\nfrom collections import OrderedDict\\n\")\n",
    "    else:\n",
    "        new_lines.append(line)\n",
    "\n",
    "# Write the modified content back to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.writelines(new_lines)\n",
    "\n",
    "print(\"✅ Fix applied successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!sed -i 's/from collections import Mapping, OrderedDict/from collections.abc import Mapping, OrderedDict/' /kaggle/working/BoT-SORT/fast_reid/fastreid/evaluation/testing.py\n",
    "!grep \"Mapping\" /kaggle/working/BoT-SORT/fast_reid/fastreid/evaluation/testing.py\n",
    "file_path = \"/kaggle/working/BoT-SORT/fast_reid/fastreid/data/build.py\"\n",
    "\n",
    "# Read the file and modify the import statement\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "new_lines = []\n",
    "for line in lines:\n",
    "    if \"from torch._six import string_classes\" in line:\n",
    "        new_lines.append(\"string_classes = (str,)\\n\")\n",
    "    else:\n",
    "        new_lines.append(line)\n",
    "\n",
    "# Write the modified content back to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.writelines(new_lines)\n",
    "\n",
    "print(\"✅ Fix applied successfully!\")\n",
    "file_path = \"/kaggle/working/BoT-SORT/fast_reid/fastreid/evaluation/testing.py\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Modify only the incorrect import\n",
    "for i in range(len(lines)):\n",
    "    if \"from collections.abc import Mapping, OrderedDict\" in lines[i]:\n",
    "        lines[i] = \"from collections import OrderedDict\\nfrom collections.abc import Mapping\\n\"\n",
    "\n",
    "# Write back the corrected file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.writelines(lines)\n",
    "\n",
    "print(f\"File '{file_path}' has been updated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall fastreid -y\n",
    "!pip install git+https://github.com/JDAI-CV/fast-reid.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/working/BoT-SORT/fast_reid/fastreid/data/build.py\"\n",
    "\n",
    "# Read the file and modify the import statement\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "new_lines = []\n",
    "for line in lines:\n",
    "    if \"from torch._six import string_classes\" in line:\n",
    "        new_lines.append(\"string_classes = (str,)\\n\")\n",
    "    else:\n",
    "        new_lines.append(line)\n",
    "\n",
    "# Write the modified content back to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.writelines(new_lines)\n",
    "\n",
    "print(\"✅ Fix applied successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fastreid.config import get_cfg\n",
    "from fastreid.engine import DefaultTrainer\n",
    "\n",
    "# Load FastReID model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"/kaggle/working/BoT-SORT/fast_reid/configs/MOT20/sbs_S50.yml\")  # Config file\n",
    "cfg.MODEL.WEIGHTS = \"/kaggle/input/reidsbs/mot20_sbs_S50.pth\"\n",
    "\n",
    "reid_model = DefaultTrainer.build_model(cfg)\n",
    "reid_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(dir(torch._subclasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install fastreid --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch==1.12.1 torchvision==0.13.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from fastreid.config import get_cfg\n",
    "from fastreid.engine import DefaultTrainer\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load YOLOv8 fine-tuned model\n",
    "model2 = YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\")\n",
    "\n",
    "# Load ReID model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"/kaggle/working/BoT-SORT/fast_reid/configs/MOT20/sbs_S50.yml\")  \n",
    "cfg.MODEL.WEIGHTS = \"/kaggle/input/reidsbs/mot20_sbs_S50.pth\"\n",
    "reid_model = DefaultTrainer.build_model(cfg)\n",
    "reid_model.eval().cuda()\n",
    "\n",
    "# Transformation for ReID model\n",
    "reid_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 128)),  # Expected input size for ReID\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "gt_file_path = \"/kaggle/input/testgt/gt.txt\"\n",
    "output_mot_path = \"/kaggle/working/bytetracking_ftlast_results.txt\"\n",
    "\n",
    "# Resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704  # YOLOv8 input size\n",
    "target_height = 704\n",
    "\n",
    "# Tracking results storage\n",
    "tracking_results = []\n",
    "\n",
    "# Dictionary to store ReID embeddings per track_id\n",
    "track_embeddings = {}\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "    # Load and resize image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "    # Run YOLOv8 tracking with ByteTrack\n",
    "    results = model2.track(img_resized, persist=True, tracker=\"/kaggle/working/bytetrack.yaml\")[0]  \n",
    "\n",
    "    for track in results.boxes:\n",
    "        x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "        track_id = int(track.id[0].item()) if track.id is not None else -1\n",
    "        conf = track.conf[0].item()\n",
    "        cls = int(track.cls[0].item())\n",
    "\n",
    "        # Process only persons (class 0)\n",
    "        if cls == 0:\n",
    "            # Convert coordinates to original image size\n",
    "            x_center *= (original_width / target_width)\n",
    "            y_center *= (original_height / target_height)\n",
    "            w *= (original_width / target_width)\n",
    "            h *= (original_height / target_height)\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "            w, h = int(w), int(h)\n",
    "\n",
    "            # Extract person crop\n",
    "            person_crop = img[max(0, y): min(original_height, y + h), max(0, x): min(original_width, x + w)]\n",
    "            \n",
    "            if person_crop.size > 0:\n",
    "                # Extract ReID embedding\n",
    "                person_tensor = reid_transform(person_crop).unsqueeze(0).cuda()\n",
    "                with torch.no_grad():\n",
    "                    embedding = reid_model(person_tensor).cpu().numpy()\n",
    "\n",
    "                # Store embeddings for track_id\n",
    "                if track_id in track_embeddings:\n",
    "                    track_embeddings[track_id].append(embedding)\n",
    "                else:\n",
    "                    track_embeddings[track_id] = [embedding]\n",
    "\n",
    "                # Update track_id based on ReID similarity\n",
    "                best_match_id = track_id\n",
    "                best_match_score = float('inf')\n",
    "\n",
    "                for existing_id, embeddings in track_embeddings.items():\n",
    "                    avg_embedding = np.mean(embeddings, axis=0)\n",
    "                    dist = np.linalg.norm(embedding - avg_embedding)\n",
    "\n",
    "                    if dist < best_match_score:\n",
    "                        best_match_score = dist\n",
    "                        best_match_id = existing_id\n",
    "\n",
    "                track_id = best_match_id  # Assign best match ID\n",
    "\n",
    "            # Save tracking result in MOT format\n",
    "            tracking_results.append([frame_number, track_id, x, y, w, h, round(conf, 4), 1, 1])\n",
    "\n",
    "            # Draw bounding boxes on image\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\"ID {track_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Sort tracking results by track_id first, then by frame number\n",
    "track_id_data = {}\n",
    "\n",
    "for row in tracking_results:\n",
    "    frame_number, track_id = row[0], row[1]\n",
    "    if track_id not in track_id_data:\n",
    "        track_id_data[track_id] = []\n",
    "    track_id_data[track_id].append(row)\n",
    "\n",
    "# Save results in MOT format\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for track_id in sorted(track_id_data.keys()):\n",
    "        for row in sorted(track_id_data[track_id], key=lambda x: x[0]):\n",
    "            f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model2 = YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\")\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "gt_file_path = \"/kaggle/input/testgt/gt.txt\"\n",
    "output_mot_path = \"/kaggle/working/bytetracking_ftlast_results.txt\"\n",
    "#visualization_output = \"/kaggle/working/bytetracking_ft2_output\"\n",
    "\n",
    "# Create output folder\n",
    "#os.makedirs(visualization_output, exist_ok=True)\n",
    "\n",
    "# Resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704  # YOLOv8 input size\n",
    "target_height = 704\n",
    "\n",
    "# Tracking results storage\n",
    "tracking_results = []\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "    # Load and resize image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "    # Run YOLOv8 tracking with StrongSORT and persistent IDs\n",
    "    results = model2.track(img_resized, persist=True, tracker=\"/kaggle/working/bytetrack.yaml\")[0]  \n",
    "\n",
    "    for track in results.boxes:\n",
    "        x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "        track_id = int(track.id[0].item()) if track.id is not None else -1  # Ensure track_id persists\n",
    "        conf = track.conf[0].item()\n",
    "\n",
    "        # Keep only person class (1 & 7 mapped to 1)\n",
    "        cls = int(track.cls[0].item())\n",
    "        if cls == 0:\n",
    "            # Convert to original image size\n",
    "            x_center *= (original_width / target_width)\n",
    "            y_center *= (original_height / target_height)\n",
    "            w *= (original_width / target_width)\n",
    "            h *= (original_height / target_height)\n",
    "\n",
    "            # Convert to top-left format\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "            w = int(w)\n",
    "            h = int(h)\n",
    "\n",
    "            # Save tracking result in MOT format\n",
    "            tracking_results.append([frame_number, track_id, x, y, w, h, round(conf, 4), 1, 1])\n",
    "\n",
    "            # Draw bounding boxes on image\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\"ID {track_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Save visualization\n",
    "    #cv2.imwrite(os.path.join(visualization_output, img_name), img)\n",
    "\n",
    "# Sort tracking results by track_id first, then by frame number\n",
    "track_id_data = {}\n",
    "\n",
    "for row in tracking_results:\n",
    "    frame_number, track_id = row[0], row[1]\n",
    "    if track_id not in track_id_data:\n",
    "        track_id_data[track_id] = []\n",
    "    track_id_data[track_id].append(row)\n",
    "\n",
    "# Save results in MOT format\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for track_id in sorted(track_id_data.keys()):  # Sort by track_id first\n",
    "        for row in sorted(track_id_data[track_id], key=lambda x: x[0]):  # Sort frames within each ID\n",
    "            f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")\n",
    "#print(f\"Visualized results saved in {visualization_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create folder for extracted Re-ID dataset\n",
    "reid_dataset_path = \"/kaggle/working/reid_dataset\"\n",
    "os.makedirs(reid_dataset_path, exist_ok=True)\n",
    "\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])  \n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    img_resized = cv2.resize(img, (target_width, target_height))\n",
    "    results = model2.track(img_resized, persist=True, tracker=\"/kaggle/working/bytetrack.yaml\")[0]  \n",
    "\n",
    "    for track in results.boxes:\n",
    "        x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "        track_id = int(track.id[0].item()) if track.id is not None else -1  \n",
    "        conf = track.conf[0].item()\n",
    "        cls = int(track.cls[0].item())\n",
    "\n",
    "        if cls == 0:\n",
    "            x_center *= (original_width / target_width)\n",
    "            y_center *= (original_height / target_height)\n",
    "            w *= (original_width / target_width)\n",
    "            h *= (original_height / target_height)\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "            w = int(w)\n",
    "            h = int(h)\n",
    "\n",
    "            # Save cropped images for Re-ID training\n",
    "            person_crop = img[y:y + h, x:x + w]\n",
    "            if person_crop.shape[0] > 10 and person_crop.shape[1] > 10:  # Avoid small false detections\n",
    "                crop_filename = f\"{reid_dataset_path}/{track_id}_{frame_number}.jpg\"\n",
    "                cv2.imwrite(crop_filename, person_crop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchvision opencv-python\n",
    "!git clone https://github.com/JDAI-CV/fast-reid.git\n",
    "!cd fast-reid\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Change dataset path\n",
    "DATASET_PATH = \"/kaggle/working/reid_dataset\"\n",
    "!export PYTHONPATH=\"/kaggle/working/fast-reid:$PYTHONPATH\"\n",
    "!python /kaggle/working/fast-reid/tools/train_net.py \\\n",
    "  --config-file /kaggle/working/fast-reid/configs/Market1501/bagtricks_R50.yml \\\n",
    "  --num-gpus 1 \\\n",
    "  OUTPUT_DIR logs/reid_mot20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Define folder paths\n",
    "visualization_output = \"/kaggle/working/bytebestmodel_output\"\n",
    "zip_path = \"/kaggle/working/visualizedbytebest_output.zip\"\n",
    "\n",
    "# Zip the folder\n",
    "shutil.make_archive(zip_path.replace(\".zip\", \"\"), 'zip', visualization_output)\n",
    "\n",
    "print(f\"Zipped visualization folder saved at: {zip_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEP SORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install deep_sort_realtime ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# Load YOLO model\n",
    "model2 = YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\")\n",
    "\n",
    "# Initialize DeepSORT with tuned parameters for MOT20\n",
    "deep_sort = DeepSort(\n",
    "    max_age=70,  # Increased for MOT20 longer occlusions\n",
    "    n_init=3,  # Lower for faster ID assignment\n",
    "    nms_max_overlap=0.7,  # Adjust overlap threshold\n",
    "    max_cosine_distance=0.2,  # Lower distance for better ID matching\n",
    "    nn_budget=100  # Larger budget for MOT20 complexity\n",
    ")\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "output_mot_path = \"/kaggle/working/deepsort_mot20_results.txt\"\n",
    "\n",
    "# Resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704  # YOLOv8 input size\n",
    "target_height = 704\n",
    "\n",
    "# Tracking results storage\n",
    "tracking_results = []\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "    # Load and resize image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "    # Run YOLO inference\n",
    "    results = model2.predict(img_resized, conf=0.5, iou=0.5)[0]  # Adjust confidence & IOU thresholds\n",
    "    detections = []\n",
    "    track_confidences = {}  # Store confidences separately\n",
    "\n",
    "    for det in results.boxes:\n",
    "        x_center, y_center, w, h = det.xywh[0].cpu().numpy()\n",
    "        conf = det.conf[0].item()\n",
    "        cls = int(det.cls[0].item())\n",
    "\n",
    "        # Keep only person class\n",
    "        if cls == 0:\n",
    "            x_center *= (original_width / target_width)\n",
    "            y_center *= (original_height / target_height)\n",
    "            w *= (original_width / target_width)\n",
    "            h *= (original_height / target_height)\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "\n",
    "            bbox = [x, y, w, h]\n",
    "            detections.append((bbox, conf, cls))\n",
    "            track_confidences[str(bbox)] = conf  # Store confidence with bbox as key\n",
    "\n",
    "    # Run DeepSORT tracking\n",
    "    tracks = deep_sort.update_tracks(detections, frame=img)\n",
    "\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "        bbox = track.to_ltrb()  # Get left-top-right-bottom format\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "\n",
    "        # Retrieve confidence from stored values\n",
    "        conf = track_confidences.get(str([x1, y1, w, h]), 0.0)\n",
    "\n",
    "        # Store tracking results in MOT format\n",
    "        tracking_results.append([frame_number, track_id, x1, y1, w, h, round(conf, 4), 1, 1])\n",
    "\n",
    "        # Draw bounding box and ID\n",
    "        cv2.rectangle(img, (x1, y1), (x1 + w, y1 + h), (0, 255, 0), 2)\n",
    "        cv2.putText(img, f\"ID {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Save results in MOT format\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for row in sorted(tracking_results, key=lambda x: (x[1], x[0])):  # Sort by ID, then frame\n",
    "        f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Define folder paths\n",
    "visualization_output = \"/kaggle/working/visualized_deepsort_best_output\"\n",
    "zip_path = \"/kaggle/working/visualized_deep_output.zip\"\n",
    "\n",
    "# Zip the folder\n",
    "shutil.make_archive(zip_path.replace(\".zip\", \"\"), 'zip', visualization_output)\n",
    "\n",
    "print(f\"Zipped visualization folder saved at: {zip_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong SORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T18:11:30.850841Z",
     "iopub.status.busy": "2025-03-22T18:11:30.850436Z",
     "iopub.status.idle": "2025-03-22T18:11:35.782157Z",
     "shell.execute_reply": "2025-03-22T18:11:35.781101Z",
     "shell.execute_reply.started": "2025-03-22T18:11:30.850813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.94)\n",
      "Requirement already satisfied: lap in /usr/local/lib/python3.10/dist-packages (0.5.12)\n",
      "Collecting thop\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n",
      "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: thop\n",
      "Successfully installed thop-0.1.1.post2209072238\n",
      "--2025-03-22 18:11:34--  https://raw.githubusercontent.com/ultralytics/ultralytics/main/ultralytics/cfg/trackers/bytetrack.yaml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 886 [text/plain]\n",
      "Saving to: ‘bytetrack.yaml’\n",
      "\n",
      "bytetrack.yaml      100%[===================>]     886  --.-KB/s    in 0s      \n",
      "\n",
      "2025-03-22 18:11:35 (35.2 MB/s) - ‘bytetrack.yaml’ saved [886/886]\n",
      "\n",
      "--2025-03-22 18:11:35--  https://raw.githubusercontent.com/ultralytics/ultralytics/main/ultralytics/cfg/trackers/strongsort.yaml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2025-03-22 18:11:35 ERROR 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics lap thop torchvision\n",
    "!wget https://raw.githubusercontent.com/ultralytics/ultralytics/main/ultralytics/cfg/trackers/bytetrack.yaml\n",
    "!wget https://raw.githubusercontent.com/ultralytics/ultralytics/main/ultralytics/cfg/trackers/strongsort.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!echo \"tracker_type: strongsort\" > strongsort.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "strongsort_yaml = \"\"\"\n",
    "tracker_type: strongsort\n",
    "model_weights: osnet_x0_25_market1501.pt\n",
    "max_age: 30\n",
    "min_hits: 3\n",
    "iou_threshold: 0.3\n",
    "\"\"\"\n",
    "\n",
    "with open(\"strongsort.yaml\", \"w\") as f:\n",
    "    f.write(strongsort_yaml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model2 = YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\")\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "gt_file_path = \"/kaggle/input/testgt/gt.txt\"\n",
    "output_mot_path = \"/kaggle/working/sstrackingresults.txt\"\n",
    "visualization_output = \"/kaggle/working/sstracking_output\"\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(visualization_output, exist_ok=True)\n",
    "\n",
    "# Resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704  # YOLOv8 input size\n",
    "target_height = 704\n",
    "\n",
    "# Tracking results storage\n",
    "tracking_results = []\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "    # Load and resize image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "    # Run YOLOv8 tracking with StrongSORT and persistent IDs\n",
    "    results = model2.track(img_resized, persist=True, tracker=\"/kaggle/working/strongsort.yaml\")[0]  \n",
    "\n",
    "    for track in results.boxes:\n",
    "        x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "        track_id = int(track.id[0].item()) if track.id is not None else -1  # Ensure track_id persists\n",
    "        conf = track.conf[0].item()\n",
    "\n",
    "        # Keep only person class (1 & 7 mapped to 1)\n",
    "        cls = int(track.cls[0].item())\n",
    "        if cls == 0:\n",
    "            # Convert to original image size\n",
    "            x_center *= (original_width / target_width)\n",
    "            y_center *= (original_height / target_height)\n",
    "            w *= (original_width / target_width)\n",
    "            h *= (original_height / target_height)\n",
    "\n",
    "            # Convert to top-left format\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "            w = int(w)\n",
    "            h = int(h)\n",
    "\n",
    "            # Save tracking result in MOT format\n",
    "            tracking_results.append([frame_number, track_id, x, y, w, h, round(conf, 4), 1, 1])\n",
    "\n",
    "            # Draw bounding boxes on image\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\"ID {track_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Save visualization\n",
    "    cv2.imwrite(os.path.join(visualization_output, img_name), img)\n",
    "\n",
    "# Sort tracking results by track_id first, then by frame number\n",
    "track_id_data = {}\n",
    "\n",
    "for row in tracking_results:\n",
    "    frame_number, track_id = row[0], row[1]\n",
    "    if track_id not in track_id_data:\n",
    "        track_id_data[track_id] = []\n",
    "    track_id_data[track_id].append(row)\n",
    "\n",
    "# Save results in MOT format\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for track_id in sorted(track_id_data.keys()):  # Sort by track_id first\n",
    "        for row in sorted(track_id_data[track_id], key=lambda x: x[0]):  # Sort frames within each ID\n",
    "            f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")\n",
    "print(f\"Visualized results saved in {visualization_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOT-SORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "botsort_config = \"\"\"\n",
    "tracker_type: botsort\n",
    "track_high_thresh: 0.6\n",
    "track_low_thresh: 0.03\n",
    "new_track_thresh: 0.2\n",
    "iou_thresh: 0.35\n",
    "match_thresh: 0.9\n",
    "track_buffer: 90\n",
    "fuse_score: True\n",
    "frame_rate: 25\n",
    "det_thresh: 0.5\n",
    "lost_track_thresh: 120\n",
    "proximity_thresh: 0.5\n",
    "appearance_thresh: 0.28\n",
    "with_reid: True\n",
    "reid_model: /kaggle/input/reidsbs/mot20_sbs_S50.pth\n",
    "gmc_method: sparseOptFlow\n",
    "\"\"\"\n",
    "\n",
    "with open(\"/kaggle/working/botsort.yaml\", \"w\") as file:\n",
    "    file.write(botsort_config)\n",
    "\n",
    "print(\"BoT-SORT YAML saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from botsort import BoTSORT\n",
    "\n",
    "# Load BoT-SORT with ReID enabled\n",
    "tracker = BoTSORT(\"/kaggle/working/botsort.yaml\")\n",
    "\n",
    "# Print ReID model status\n",
    "if tracker.model is None:\n",
    "    print(\"❌ ReID model NOT loaded\")\n",
    "else:\n",
    "    print(\"✅ ReID model successfully loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLO model\n",
    "model2 = YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\")\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "output_mot_path = \"/kaggle/working/botsort_results.txt\"\n",
    "\n",
    "# Image resize settings\n",
    "ORIG_W, ORIG_H = 1920, 1080\n",
    "TARGET_W, TARGET_H = 704, 704\n",
    "\n",
    "# Store tracking results\n",
    "tracking_results = []\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_num = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "    # Load image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Skipping {img_path} (load error)\")\n",
    "        continue\n",
    "\n",
    "    # Resize image for YOLO\n",
    "    img_resized = cv2.resize(img, (TARGET_W, TARGET_H))\n",
    "\n",
    "    # Run BoT-SORT tracking\n",
    "    results = model2.track(img_resized, persist=True, tracker=\"/kaggle/working/botsort.yaml\", verbose=True)[0]\n",
    "\n",
    "    for track in results.boxes:\n",
    "        x_c, y_c, w, h = track.xywh[0].cpu().numpy()\n",
    "        track_id = int(track.id[0].item()) if track.id is not None else -1\n",
    "        conf = track.conf[0].item()\n",
    "        cls = int(track.cls[0].item())\n",
    "\n",
    "        if cls == 0 and conf > 0.25:  # Keep only 'person' class with high confidence\n",
    "            # Scale back to original image size\n",
    "            x_c *= (ORIG_W / TARGET_W)\n",
    "            y_c *= (ORIG_H / TARGET_H)\n",
    "            w *= (ORIG_W / TARGET_W)\n",
    "            h *= (ORIG_H / TARGET_H)\n",
    "\n",
    "            # Convert to top-left format\n",
    "            x, y = int(x_c - w / 2), int(y_c - h / 2)\n",
    "            w, h = int(w), int(h)\n",
    "\n",
    "            # Save results in MOT format\n",
    "            tracking_results.append([frame_num, track_id, x, y, w, h, round(conf, 4), 1, 1])\n",
    "\n",
    "            # Draw bounding boxes\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\"ID {track_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Sort results: Track ID first, then frame number\n",
    "track_id_data = {}\n",
    "\n",
    "for row in tracking_results:\n",
    "    f_num, t_id = row[0], row[1]\n",
    "    if t_id not in track_id_data:\n",
    "        track_id_data[t_id] = []\n",
    "    track_id_data[t_id].append(row)\n",
    "\n",
    "# Save in MOT format\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for t_id in sorted(track_id_data.keys()):  \n",
    "        for row in sorted(track_id_data[t_id], key=lambda x: x[0]):  \n",
    "            f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "reid_model_path = \"/kaggle/input/reidsbs/mot20_sbs_S50.pth\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "try:\n",
    "    reid_model = torch.load(reid_model_path, map_location=device)\n",
    "    print(\"✅ ReID model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load ReID model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOTA EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T18:35:37.168009Z",
     "iopub.status.busy": "2025-03-24T18:35:37.167572Z",
     "iopub.status.idle": "2025-03-24T18:35:40.964810Z",
     "shell.execute_reply": "2025-03-24T18:35:40.963747Z",
     "shell.execute_reply.started": "2025-03-24T18:35:37.167985Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting motmetrics\n",
      "  Downloading motmetrics-1.4.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from motmetrics) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from motmetrics) (2.2.3)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from motmetrics) (1.13.1)\n",
      "Collecting xmltodict>=0.12.0 (from motmetrics)\n",
      "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.1->motmetrics) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.1->motmetrics) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.1->motmetrics) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.1->motmetrics) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.1->motmetrics) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.1->motmetrics) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->motmetrics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->motmetrics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->motmetrics) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.23.1->motmetrics) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.12.1->motmetrics) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.12.1->motmetrics) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.12.1->motmetrics) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.12.1->motmetrics) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.12.1->motmetrics) (2024.2.0)\n",
      "Downloading motmetrics-1.4.0-py3-none-any.whl (161 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: xmltodict, motmetrics\n",
      "Successfully installed motmetrics-1.4.0 xmltodict-0.14.2\n"
     ]
    }
   ],
   "source": [
    "!pip install motmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y motmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade git+https://github.com/cheind/py-motmetrics.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import motmetrics as mm\n",
    "import re\n",
    "\n",
    "# Locate mot.py file inside the motmetrics package\n",
    "mot_file_path = mm.__file__.replace('__init__.py', 'mot.py')\n",
    "\n",
    "# Read and fix the deprecated np.bool usage\n",
    "with open(mot_file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Replace np.bool with np.bool_\n",
    "fixed_content = re.sub(r'np.bool', 'np.bool_', content)\n",
    "\n",
    "# Write the fixed version back\n",
    "with open(mot_file_path, 'w') as file:\n",
    "    file.write(fixed_content)\n",
    "\n",
    "print(\"Patched motmetrics successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import motmetrics as mm\n",
    "import numpy as np\n",
    "\n",
    "# Load ground truth and predicted tracks\n",
    "def load_tracks(file_path):\n",
    "    data = np.loadtxt(file_path, delimiter=',')\n",
    "    return data\n",
    "\n",
    "gt_file = '/kaggle/input/filteredgt/filtered_gt.txt'\n",
    "pred_file = '/kaggle/working/bytetracking_ftlast_results.txt'\n",
    "\n",
    "gt_data = load_tracks(gt_file)\n",
    "pred_data = load_tracks(pred_file)\n",
    "\n",
    "# Create an accumulator to store results\n",
    "acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "# Iterate through each frame\n",
    "for frame in np.unique(gt_data[:, 0]):\n",
    "    # Extract ground truth and predictions for the current frame\n",
    "    gt_objects = gt_data[gt_data[:, 0] == frame]\n",
    "    pred_objects = pred_data[pred_data[:, 0] == frame]\n",
    "\n",
    "    # Extract bounding boxes and IDs\n",
    "    gt_boxes = gt_objects[:, 2:6]  # x, y, width, height\n",
    "    gt_ids = gt_objects[:, 1]\n",
    "\n",
    "    pred_boxes = pred_objects[:, 2:6]\n",
    "    pred_ids = pred_objects[:, 1]\n",
    "\n",
    "    # Compute pairwise distances (e.g., IoU)\n",
    "    dist_matrix = mm.distances.iou_matrix(gt_boxes, pred_boxes, max_iou=0.5)\n",
    "\n",
    "    # Update the accumulator\n",
    "    acc.update(gt_ids, pred_ids, dist_matrix)\n",
    "\n",
    "# Compute metrics\n",
    "mh = mm.metrics.create()\n",
    "summary = mh.compute(acc, metrics=['mota', 'motp', 'idf1', 'precision', 'recall'], name='acc')\n",
    "\n",
    "# Print the results\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T18:24:30.891974Z",
     "iopub.status.busy": "2025-03-24T18:24:30.891456Z",
     "iopub.status.idle": "2025-03-24T18:24:33.547460Z",
     "shell.execute_reply": "2025-03-24T18:24:33.546725Z",
     "shell.execute_reply.started": "2025-03-24T18:24:30.891944Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as: /kaggle/working/50.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load MOT tracking results\n",
    "def load_mot_results(mot_file):\n",
    "    tracking_data = {}\n",
    "    with open(mot_file, 'r') as f:\n",
    "        for line in f:\n",
    "            frame_id, track_id, x, y, w, h, conf, *_ = map(float, line.strip().split(','))\n",
    "            if int(frame_id) not in tracking_data:\n",
    "                tracking_data[int(frame_id)] = []\n",
    "            tracking_data[int(frame_id)].append({\n",
    "                \"tracked_id\": track_id, \"x\": int(x), \"y\": int(y),\n",
    "                \"w\": int(w), \"h\": int(h), \"confidence\": conf\n",
    "            })\n",
    "    return tracking_data\n",
    "\n",
    "# Load submission template\n",
    "def load_submission_template(submission_file):\n",
    "    df = pd.read_csv(submission_file)\n",
    "    return df\n",
    "\n",
    "# Process and fill submission file\n",
    "def fill_submission_file(mot_file, submission_file, output_file):\n",
    "    tracking_data = load_mot_results(mot_file)\n",
    "    df = load_submission_template(submission_file)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i, \"objective\"] == \"tracking\":\n",
    "            frame_id = int(df.loc[i, \"frame\"])  # Get frame from submission template\n",
    "            if frame_id in tracking_data:\n",
    "                df.loc[i, \"objects\"] = str(tracking_data[frame_id])  # Fill tracking results\n",
    "        elif df.loc[i, \"objective\"] == \"face_reid\":\n",
    "            df.loc[i, \"frame\"] = -1  # Ensure face_reid has frame -1\n",
    "            df.loc[i, \"objects\"] = str(ast.literal_eval(df.loc[i, \"objects\"]))  # Keep original\n",
    "\n",
    "    # Save final submission file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Submission file saved as: {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "mot_file = \"/kaggle/working/modeltrackingresults.txt\"\n",
    "submission_file = \"/kaggle/input/fawrysub1/sub1.csv\"\n",
    "output_file = \"/kaggle/working/50.csv\"\n",
    "\n",
    "fill_submission_file(mot_file, submission_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define paths\n",
    "file_path = \"/kaggle/working/bytetrack.yaml\"\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "output_dir = \"/kaggle/working/mot_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Optimized parameter ranges for tuning\n",
    "param_grid = {\n",
    "    \"track_high_thresh\": [0.55, 0.6, 0.65],\n",
    "    \"track_low_thresh\": [0.05, 0.1, 0.15],\n",
    "    \"iou_thresh\": [0.25, 0.3, 0.35],\n",
    "    \"match_thresh\": [0.8, 0.85, 0.9],\n",
    "    \"track_buffer\": [20, 30, 40],\n",
    "    \"new_track_thresh\": [0.15, 0.2, 0.25],\n",
    "    \"fuse_score\": [True]\n",
    "}\n",
    "\n",
    "# Generate all parameter combinations\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "\n",
    "# Model setup\n",
    "model = YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\")\n",
    "\n",
    "# Image resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704\n",
    "target_height = 704\n",
    "\n",
    "for params in param_combinations:\n",
    "    # Update tracker configuration\n",
    "    bytetrack_config = {\n",
    "        \"tracker_type\": \"bytetrack\",\n",
    "        \"track_high_thresh\": params[0],\n",
    "        \"track_low_thresh\": params[1],\n",
    "        \"iou_thresh\": params[2],\n",
    "        \"match_thresh\": params[3],\n",
    "        \"track_buffer\": params[4],\n",
    "        \"new_track_thresh\": params[5],\n",
    "        \"fuse_score\": params[6],\n",
    "    }\n",
    "    \n",
    "    with open(file_path, \"w\") as file:\n",
    "        yaml.dump(bytetrack_config, file, default_flow_style=False)\n",
    "    \n",
    "    # Generate unique filename\n",
    "    param_str = \"_\".join(map(str, params))\n",
    "    mot_output_path = os.path.join(output_dir, f\"tracking_{param_str}.txt\")\n",
    "    tracking_results = []\n",
    "    \n",
    "    # Process test images\n",
    "    for img_name in sorted(os.listdir(test_images_path)):\n",
    "        frame_number = int(img_name.split(\".\")[0])\n",
    "        img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        img_resized = cv2.resize(img, (target_width, target_height))\n",
    "        results = model.track(img_resized, persist=True, tracker=file_path)[0]\n",
    "\n",
    "        for track in results.boxes:\n",
    "            x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "            track_id = int(track.id[0].item()) if track.id is not None else -1\n",
    "            conf = track.conf[0].item()\n",
    "            cls = int(track.cls[0].item())\n",
    "            if cls == 0:\n",
    "                x_center *= (original_width / target_width)\n",
    "                y_center *= (original_height / target_height)\n",
    "                w *= (original_width / target_width)\n",
    "                h *= (original_height / target_height)\n",
    "                x = int(x_center - w / 2)\n",
    "                y = int(y_center - h / 2)\n",
    "                w = int(w)\n",
    "                h = int(h)\n",
    "                tracking_results.append([frame_number, track_id, x, y, w, h, round(conf, 4), 1, 1])\n",
    "    \n",
    "    # Save results\n",
    "    with open(mot_output_path, \"w\") as f:\n",
    "        for row in sorted(tracking_results, key=lambda x: (x[1], x[0])):\n",
    "            f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "    \n",
    "    print(f\"Saved results to {mot_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import motmetrics as mm\n",
    "import numpy as np\n",
    "\n",
    "def load_tracks(file_path):\n",
    "    \"\"\"Load tracking data from a file.\"\"\"\n",
    "    return np.loadtxt(file_path, delimiter=',')\n",
    "\n",
    "# Define paths\n",
    "gt_file = \"/kaggle/input/filteredgt/filtered_gt.txt\"  # Ground truth file path\n",
    "results_dir = \"/kaggle/input/gttrials\"  # Directory with tracking result files\n",
    "\n",
    "gt_data = load_tracks(gt_file)\n",
    "best_mota = -np.inf\n",
    "best_file = None\n",
    "\n",
    "# Iterate over each tracking result file\n",
    "for file_name in os.listdir(results_dir):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        result_path = os.path.join(results_dir, file_name)\n",
    "        pred_data = load_tracks(result_path)\n",
    "        \n",
    "        # Create an accumulator to store results\n",
    "        acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "        # Iterate through each frame\n",
    "        for frame in np.unique(gt_data[:, 0]):\n",
    "            # Extract ground truth and predictions for the current frame\n",
    "            gt_objects = gt_data[gt_data[:, 0] == frame]\n",
    "            pred_objects = pred_data[pred_data[:, 0] == frame]\n",
    "\n",
    "            # Extract bounding boxes and IDs\n",
    "            gt_boxes = gt_objects[:, 2:6]  # x, y, width, height\n",
    "            gt_ids = gt_objects[:, 1]\n",
    "\n",
    "            pred_boxes = pred_objects[:, 2:6]\n",
    "            pred_ids = pred_objects[:, 1]\n",
    "\n",
    "            # Compute pairwise distances (IoU-based)\n",
    "            dist_matrix = mm.distances.iou_matrix(gt_boxes, pred_boxes, max_iou=0.5)\n",
    "            \n",
    "            # Update the accumulator\n",
    "            acc.update(gt_ids, pred_ids, dist_matrix)\n",
    "\n",
    "        # Compute metrics\n",
    "        mh = mm.metrics.create()\n",
    "        summary = mh.compute(acc, metrics=['mota'], name=file_name)\n",
    "        mota_score = summary.loc[file_name]['mota']\n",
    "        print(f\"{file_name}: MOTA = {mota_score}\")\n",
    "        \n",
    "        # Check for the best MOTA score\n",
    "        if mota_score > best_mota:\n",
    "            best_mota = mota_score\n",
    "            best_file = file_name\n",
    "\n",
    "print(f\"\\nBest Tracking Result: {best_file} with MOTA = {best_mota}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ls /kaggle/working/models/vit_kprpe/RPE/rpe_ops/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install setuptools wheel ninja\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls build/\n",
    "!ls rpe_index.egg-info/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls -l /kaggle/working/models/vit_kprpe/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python setup.py build_ext --inplace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ls -l /kaggle/working/models/vit_kprpe/RPE/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install -e /kaggle/working/models/vit_kprpe/RPE/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ls -l /kaggle/working/models/vit_kprpe/RPE/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ls -l /kaggle/working/models/vit_kprpe/RPE/rpe_ops/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cd /kaggle/working/models/vit_kprpe/RPE/rpe_ops/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python /kaggle/working/models/vit_kprpe/RPE/rpe_ops/setup.py build_ext --inplace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/working/models/vit_kprpe/RPE/\")\n",
    "import rpe_ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOT OR OC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T13:17:38.744193Z",
     "iopub.status.busy": "2025-03-17T13:17:38.743822Z",
     "iopub.status.idle": "2025-03-17T13:17:38.749967Z",
     "shell.execute_reply": "2025-03-17T13:17:38.749277Z",
     "shell.execute_reply.started": "2025-03-17T13:17:38.744168Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML files created successfully in /kaggle/working!\n"
     ]
    }
   ],
   "source": [
    "# # BoT-SORT Configuration\n",
    "botsort_config = \"\"\"\n",
    "tracker_type: botsort\n",
    "track_high_thresh: 0.6\n",
    "track_low_thresh: 0.1\n",
    "new_track_thresh: 0.2\n",
    "iou_thresh: 0.3\n",
    "match_thresh: 0.85\n",
    "track_buffer: 30\n",
    "fuse_score: True\n",
    "frame_rate: 25\n",
    "det_thresh: 0.5\n",
    "lost_track_thresh: 50\n",
    "proximity_thresh: 0.5\n",
    "appearance_thresh: 0.25\n",
    "with_reid: False\n",
    "gmc_method: sparseOptFlow\n",
    "\"\"\"\n",
    "\n",
    "# OC-SORT Configuration\n",
    "ocsort_config = \"\"\"\n",
    "tracker_type: ocsort\n",
    "track_high_thresh: 0.6\n",
    "track_low_thresh: 0.1\n",
    "new_track_thresh: 0.2\n",
    "iou_thresh: 0.3\n",
    "match_thresh: 0.85\n",
    "track_buffer: 30\n",
    "fuse_score: True\n",
    "frame_rate: 25\n",
    "det_thresh: 0.5\n",
    "lost_track_thresh: 50\n",
    "\"\"\"\n",
    "\n",
    "# Save BoT-SORT configuration to botsort.yaml\n",
    "with open(\"/kaggle/working/botsort.yaml\", \"w\") as file:\n",
    "    file.write(botsort_config)\n",
    "\n",
    "# Save OC-SORT configuration to ocsort.yaml\n",
    "with open(\"/kaggle/working/ocsort.yaml\", \"w\") as file:\n",
    "    file.write(ocsort_config)\n",
    "\n",
    "print(\"YAML files created successfully in /kaggle/working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model2 = YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\")\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "gt_file_path = \"/kaggle/input/testgt/gt.txt\"\n",
    "output_mot_path_botsort = \"/kaggle/working/botsort_results.txt\"\n",
    "output_mot_path_ocsort = \"/kaggle/working/ocsort_results.txt\"\n",
    "\n",
    "# Resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704  # YOLOv8 input size\n",
    "target_height = 704\n",
    "\n",
    "# Function to run tracking\n",
    "def run_tracking(tracker_config, output_mot_path):\n",
    "    tracking_results = []\n",
    "\n",
    "    # Process test frames\n",
    "    for img_name in sorted(os.listdir(test_images_path)):\n",
    "        frame_number = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "        img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "        # Load and resize image\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Error loading {img_path}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "        # Run YOLOv8 tracking with the specified tracker\n",
    "        results = model2.track(img_resized, persist=True, tracker=\"/kaggle/working/botsort.yaml\")[0]  \n",
    "\n",
    "        for track in results.boxes:\n",
    "            x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "            track_id = int(track.id[0].item()) if track.id is not None else -1  # Ensure track_id persists\n",
    "            conf = track.conf[0].item()\n",
    "\n",
    "            # Keep only person class (1 & 7 mapped to 1)\n",
    "            cls = int(track.cls[0].item())\n",
    "            if cls == 0:\n",
    "                # Convert to original image size\n",
    "                x_center *= (original_width / target_width)\n",
    "                y_center *= (original_height / target_height)\n",
    "                w *= (original_width / target_width)\n",
    "                h *= (original_height / target_height)\n",
    "\n",
    "                # Convert to top-left format\n",
    "                x = int(x_center - w / 2)\n",
    "                y = int(y_center - h / 2)\n",
    "                w = int(w)\n",
    "                h = int(h)\n",
    "\n",
    "                # Save tracking result in MOT format\n",
    "                tracking_results.append([frame_number, track_id, x, y, w, h, round(conf, 4), 1, 1])\n",
    "\n",
    "    # Sort tracking results by track_id first, then by frame number\n",
    "    track_id_data = {}\n",
    "\n",
    "    for row in tracking_results:\n",
    "        frame_number, track_id = row[0], row[1]\n",
    "        if track_id not in track_id_data:\n",
    "            track_id_data[track_id] = []\n",
    "        track_id_data[track_id].append(row)\n",
    "\n",
    "    # Save results in MOT format\n",
    "    with open(output_mot_path, \"w\") as f:\n",
    "        for track_id in sorted(track_id_data.keys()):  # Sort by track_id first\n",
    "            for row in sorted(track_id_data[track_id], key=lambda x: x[0]):  # Sort frames within each ID\n",
    "                f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "    print(f\"Tracking results saved to {output_mot_path}\")\n",
    "\n",
    "# Run BoT-SORT\n",
    "run_tracking(\"/kaggle/working/botsort.yaml\", output_mot_path_botsort)\n",
    "\n",
    "# Run OC-SORT\n",
    "run_tracking(\"/kaggle/working/ocsort.yaml\", output_mot_path_ocsort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bot with REID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# BoT-SORT Configuration with ReID\n",
    "botsort_config = \"\"\"\n",
    "tracker_type: botsort\n",
    "track_high_thresh: 0.6\n",
    "track_low_thresh: 0.05\n",
    "new_track_thresh: 0.15\n",
    "iou_thresh: 0.4\n",
    "match_thresh: 0.9\n",
    "track_buffer: 60\n",
    "fuse_score: True\n",
    "frame_rate: 25\n",
    "det_thresh: 0.5\n",
    "lost_track_thresh: 100\n",
    "proximity_thresh: 0.4\n",
    "appearance_thresh: 0.2\n",
    "with_reid: True  # Enable ReID\n",
    "reid_model: /kaggle/input/reidsbs/mot20_sbs_S50.pth  # Path to the ReID model\n",
    "gmc_method: sparseOptFlow\n",
    "\"\"\"\n",
    "\n",
    "# Save BoT-SORT configuration to botsort.yaml\n",
    "with open(\"/kaggle/working/botsort.yaml\", \"w\") as file:\n",
    "    file.write(botsort_config)\n",
    "\n",
    "print(\"botsort.yaml updated successfully in /kaggle/working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NirAharon/BOT-SORT.git\n",
    "%cd BOT-SORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip list | grep bot-sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working/BOT-SORT\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install cython_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!find /kaggle/working/BOT-SORT/fast_reid -type f -name \"*.py\" -exec sed -i 's/from collections import Mapping/from collections.abc import Mapping/g' {} +\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install faiss-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the matching.py file\n",
    "file_path = \"/kaggle/working/BOT-SORT/tracker/matching.py\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Replace np.float with np.float64\n",
    "with open(file_path, \"w\") as file:\n",
    "    for line in lines:\n",
    "        file.write(line.replace(\"np.float\", \"np.float64\"))\n",
    "\n",
    "print(f\"Updated {file_path} to replace np.float with np.float64.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install /path/to/numpy-1.19.3-cpXX-cpXX-win_amd64.whl\n",
    "\n",
    "!pip install numpy==1.19.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "from ultralytics import YOLO\n",
    "from bot_sort import BoTSORT  # Import BoT-SORT\n",
    "\n",
    "# Initialize the YOLO model\n",
    "def load_yolo_model(model_path):\n",
    "    return YOLO(model_path)\n",
    "\n",
    "# Initialize BoT-SORT tracker\n",
    "def initialize_tracker(args):\n",
    "    return BoTSORT(args, frame_rate=25)\n",
    "\n",
    "# Resize image while maintaining the aspect ratio\n",
    "def resize_image(image, target_width, target_height):\n",
    "    return cv2.resize(image, (target_width, target_height))\n",
    "\n",
    "# Process YOLOv8 detections and convert to bounding box format\n",
    "def process_yolo_detections(results, original_width, original_height, target_width, target_height):\n",
    "    detections = []\n",
    "    for box in results.boxes:\n",
    "        x_center, y_center, w, h = box.xywh[0].cpu().numpy()\n",
    "        conf = box.conf[0].item()\n",
    "        cls = int(box.cls[0].item())\n",
    "\n",
    "        if cls == 0:  # Only process 'person' class\n",
    "            # Rescale bounding box to original size\n",
    "            x_center *= (original_width / target_width)\n",
    "            y_center *= (original_height / target_height)\n",
    "            w *= (original_width / target_width)\n",
    "            h *= (original_height / target_height)\n",
    "\n",
    "            # Convert to top-left format\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "            w = int(w)\n",
    "            h = int(h)\n",
    "\n",
    "            detections.append([x, y, x + w, y + h, conf, cls])  # Ensure correct shape\n",
    "    return np.array(detections, dtype=np.float64) if len(detections) > 0 else np.empty((0, 6), dtype=np.float64)\n",
    "\n",
    "# Save tracking results to a file in MOT format\n",
    "def save_tracking_results(tracking_results, output_path):\n",
    "    track_id_data = {}\n",
    "    for row in tracking_results:\n",
    "        frame_number, track_id = row[0], row[1]\n",
    "        if track_id not in track_id_data:\n",
    "            track_id_data[track_id] = []\n",
    "        track_id_data[track_id].append(row)\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for track_id in sorted(track_id_data.keys()):\n",
    "            for row in sorted(track_id_data[track_id], key=lambda x: x[0]):\n",
    "                f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "# Main tracking loop\n",
    "def main(model_path, reid_weights, test_images_path, output_mot_path, target_width=704, target_height=704):\n",
    "    # Load models and initialize tracker\n",
    "    yolo_model = load_yolo_model(model_path)\n",
    "    args = argparse.Namespace(\n",
    "        reid_weights=reid_weights,\n",
    "        track_high_thresh=0.5,         \n",
    "        track_low_thresh=0.1, \n",
    "        name=\"BoTSORT\", \n",
    "        ablation=False, \n",
    "        new_track_thresh=0.7,          \n",
    "        track_buffer=30,               \n",
    "        match_thresh=0.8,              \n",
    "        aspect_ratio_thresh=1.6,       \n",
    "        min_box_area=10,               \n",
    "        mot20=False,                   \n",
    "        proximity_thresh=0.5,          \n",
    "        appearance_thresh=0.25,        \n",
    "        with_reid=True, \n",
    "        fast_reid_config=\"/kaggle/working/BOT-SORT/fast_reid/configs/MOT20/sbs_S50.yml\",\n",
    "        fast_reid_weights=\"/kaggle/input/reidsbs/mot20_sbs_S50.pth\",\n",
    "        device=\"cuda\",  \n",
    "        cmc_method=\"sparseOptFlow\"\n",
    "    )\n",
    "    tracker = initialize_tracker(args)\n",
    "\n",
    "    # Tracking results storage\n",
    "    tracking_results = []\n",
    "\n",
    "    # Process test frames\n",
    "    for img_name in sorted(os.listdir(test_images_path)):\n",
    "        frame_number = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "        img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "        # Load and resize image\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Error loading {img_path}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        img_resized = resize_image(img, target_width, target_height)\n",
    "\n",
    "        # Run YOLOv8 detection\n",
    "        results = yolo_model(img_resized)[0]\n",
    "        detections = process_yolo_detections(results, 1920, 1080, target_width, target_height)\n",
    "\n",
    "        # Run BoT-SORT tracking\n",
    "        tracks = tracker.update(detections, img)\n",
    "\n",
    "        for track in tracks:\n",
    "            if len(track) < 6:  # Ensure track has correct columns\n",
    "                continue\n",
    "\n",
    "            track_id = int(track[4])  # Get track ID\n",
    "            x, y, x2, y2 = map(int, track[:4])  # Convert bounding box coordinates\n",
    "            w, h = x2 - x, y2 - y  # Convert to width-height format\n",
    "            conf = round(float(track[5]), 4)\n",
    "\n",
    "            # Save tracking result in MOT format\n",
    "            tracking_results.append([frame_number, track_id, x, y, w, h, conf, 1, 1])\n",
    "\n",
    "            # Draw bounding boxes on image\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\"ID {track_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Save tracking results to MOT format\n",
    "    save_tracking_results(tracking_results, output_mot_path)\n",
    "    print(f\"Tracking results saved to {output_mot_path}\")\n",
    "\n",
    "# Define paths\n",
    "model_path = \"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\"\n",
    "reid_weights = \"/kaggle/input/reidsbs/mot20_sbs_S50.pth\"\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "output_mot_path = \"/kaggle/working/botsort_results.txt\"\n",
    "\n",
    "# Run the main function\n",
    "main(model_path, reid_weights, test_images_path, output_mot_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Find all occurrences of dtype=float in the BoT-SORT directory\n",
    "!grep -r \"dtype=float\" /kaggle/working/BOT-SORT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Regex patterns to find and replace\n",
    "patterns = [\n",
    "    (r'dtype=float\\)', r'dtype=np.float64)'),\n",
    "    (r'dtype=float,', r'dtype=np.float64,'),\n",
    "    (r'dtype=float32\\)', r'dtype=np.float32)'),\n",
    "    (r'dtype=float32,', r'dtype=np.float32,'),\n",
    "    (r'dtype=float64\\)', r'dtype=np.float64)'),\n",
    "    (r'dtype=float64,', r'dtype=np.float64,'),\n",
    "    (r'dtype=float_\\)', r'dtype=np.float64)'),  # Assuming float_ was meant to be float64\n",
    "]\n",
    "\n",
    "# Get all Python files in the BOT-SORT directory\n",
    "def fix_numpy_types(directory):\n",
    "    fixed_files = 0\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Read the file content\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Apply all patterns\n",
    "                original_content = content\n",
    "                for pattern, replacement in patterns:\n",
    "                    content = re.sub(pattern, replacement, content)\n",
    "                \n",
    "                # If any changes were made, write the file back\n",
    "                if content != original_content:\n",
    "                    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(content)\n",
    "                    fixed_files += 1\n",
    "                    print(f\"Fixed {file_path}\")\n",
    "    \n",
    "    return fixed_files\n",
    "\n",
    "# Run the fix on BOT-SORT directory\n",
    "bot_sort_dir = \"/kaggle/working/BOT-SORT\"\n",
    "total_fixed = fix_numpy_types(bot_sort_dir)\n",
    "print(f\"\\nTotal files fixed: {total_fixed}\")\n",
    "\n",
    "# Fix the specific matching.py file that caused the error\n",
    "matching_file = \"/kaggle/working/BOT-SORT/tracker/matching.py\"\n",
    "if os.path.exists(matching_file):\n",
    "    with open(matching_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Replace specific instances in matching.py\n",
    "    content = content.replace(\"dtype=float\", \"dtype=np.float64\")\n",
    "    \n",
    "    with open(matching_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"Specifically fixed {matching_file}\")\n",
    "\n",
    "print(\"\\nDone! Try running your tracking code again now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/working/BOT-SORT/tracker/bot_sort.py\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Replace np.float with float\n",
    "updated_content = content.replace(\"np.float\", \"float\")\n",
    "\n",
    "# Write the updated content back to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(updated_content)\n",
    "\n",
    "print(\"File updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def ious(atlbrs, btlbrs):\n",
    "    \"\"\"\n",
    "    Compute cost based on IoU\n",
    "    :type atlbrs: list[tlbr] | np.ndarray\n",
    "    :type atlbrs: list[tlbr] | np.ndarray\n",
    "\n",
    "    :rtype ious np.ndarray\n",
    "    \"\"\"\n",
    "    ious = np.zeros((len(atlbrs), len(btlbrs)), dtype=float)  # Fixed: Use float instead of np.float64\n",
    "    if ious.size == 0:\n",
    "        return ious\n",
    "\n",
    "    ious = bbox_ious(\n",
    "        np.ascontiguousarray(atlbrs, dtype=float),  # Fixed: Use float instead of np.float64\n",
    "        np.ascontiguousarray(btlbrs, dtype=float)   # Fixed: Use float instead of np.float64\n",
    "    )\n",
    "\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/working/BOT-SORT/tracker/matching.py\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Replace the ious function with the corrected version\n",
    "corrected_function = \"\"\"\n",
    "def ious(atlbrs, btlbrs):\n",
    "    \\\"\\\"\\\"\n",
    "    Compute cost based on IoU\n",
    "    :type atlbrs: list[tlbr] | np.ndarray\n",
    "    :type atlbrs: list[tlbr] | np.ndarray\n",
    "\n",
    "    :rtype ious np.ndarray\n",
    "    \\\"\\\"\\\"\n",
    "    ious = np.zeros((len(atlbrs), len(btlbrs)), dtype=float)  # Fixed: Use float instead of np.float64\n",
    "    if ious.size == 0:\n",
    "        return ious\n",
    "\n",
    "    ious = bbox_ious(\n",
    "        np.ascontiguousarray(atlbrs, dtype=float),  # Fixed: Use float instead of np.float64\n",
    "        np.ascontiguousarray(btlbrs, dtype=float)   # Fixed: Use float instead of np.float64\n",
    "    )\n",
    "\n",
    "    return ious\n",
    "\"\"\"\n",
    "\n",
    "# Replace the old ious function with the corrected version\n",
    "updated_content = content.replace(\n",
    "    \"\"\"\n",
    "def ious(atlbrs, btlbrs):\n",
    "    \\\"\\\"\\\"\n",
    "    Compute cost based on IoU\n",
    "    :type atlbrs: list[tlbr] | np.ndarray\n",
    "    :type atlbrs: list[tlbr] | np.ndarray\n",
    "\n",
    "    :rtype ious np.ndarray\n",
    "    \\\"\\\"\\\"\n",
    "    ious = np.zeros((len(atlbrs), len(btlbrs)), dtype=np.float64)\n",
    "    if ious.size == 0:\n",
    "        return ious\n",
    "\n",
    "    ious = bbox_ious(\n",
    "        np.ascontiguousarray(atlbrs, dtype=np.float64),\n",
    "        np.ascontiguousarray(btlbrs, dtype=np.float64)\n",
    "    )\n",
    "\n",
    "    return ious\n",
    "\"\"\",\n",
    "    corrected_function\n",
    ")\n",
    "\n",
    "# Write the updated content back to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(updated_content)\n",
    "\n",
    "print(\"File updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/working/BOT-SORT/tracker/matching.py\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        if \"dtype=float\" in line:\n",
    "            print(\"Change found:\", line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Navigate to the BOT-SORT directory\n",
    "%cd /kaggle/working/BOT-SORT\n",
    "\n",
    "# Clean the build directory (if it exists)\n",
    "!rm -rf build\n",
    "\n",
    "# Rebuild the project\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/working/BOT-SORT/tracker/matching.py\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Replace np.float64 with float\n",
    "updated_content = content.replace(\"dtype=np.float64\", \"dtype=float\")\n",
    "\n",
    "# Write the updated content back to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(updated_content)\n",
    "\n",
    "print(\"File updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/working/BOT-SORT/tracker/matching.py\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Replace float64 with np.float64\n",
    "updated_content = content.replace(\"dtype=float64\", \"dtype=np.float64\")\n",
    "\n",
    "# Write the updated content back to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(updated_content)\n",
    "\n",
    "print(\"File updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def search_for_np_float(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    for line_num, line in enumerate(lines, 1):\n",
    "                        if \"np.float\" in line:\n",
    "                            print(f\"Found 'np.float' in {file_path}, line {line_num}: {line.strip()}\")\n",
    "\n",
    "# Specify the BOT-SORT directory\n",
    "bot_sort_directory = \"/kaggle/working/BOT-SORT\"\n",
    "search_for_np_float(bot_sort_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def replace_np_float(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Replace np.float with np.float32\n",
    "                updated_content = content.replace(\"np.float\", \"np.float32\")\n",
    "                \n",
    "                # Write the updated content back to the file\n",
    "                with open(file_path, \"w\") as f:\n",
    "                    f.write(updated_content)\n",
    "\n",
    "# Specify the BOT-SORT directory\n",
    "bot_sort_directory = \"/kaggle/working/BOT-SORT\"\n",
    "replace_np_float(bot_sort_directory)\n",
    "\n",
    "print(\"All occurrences of 'np.float' have been replaced with 'np.float32'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def fix_damaged_lines(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Fix np.float3232 -> np.float32\n",
    "                content = content.replace(\"np.float3232\", \"np.float32\")\n",
    "                # Fix np.float3264 -> np.float64\n",
    "                content = content.replace(\"np.float3264\", \"np.float64\")\n",
    "                \n",
    "                # Write the updated content back to the file\n",
    "                with open(file_path, \"w\") as f:\n",
    "                    f.write(content)\n",
    "\n",
    "# Specify the BOT-SORT directory\n",
    "bot_sort_directory = \"/kaggle/working/BOT-SORT\"\n",
    "fix_damaged_lines(bot_sort_directory)\n",
    "\n",
    "print(\"All damaged lines have been fixed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Navigate to the BOT-SORT directory\n",
    "%cd /kaggle/working/BOT-SORT\n",
    "\n",
    "# Clean the build directory (if it exists)\n",
    "!rm -rf build\n",
    "\n",
    "# Rebuild the project\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/working/BOT-SORT/tracker/matching.py\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Replace np.float with np.float64\n",
    "updated_content = content.replace(\"np.float\", \"np.float64\")\n",
    "\n",
    "# Write the updated content back to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(updated_content)\n",
    "\n",
    "print(\"File updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/working/BOT-SORT/tracker/matching.py\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        if \"np.float64\" in line:\n",
    "            print(\"Change found:\", line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/working/BOT-SORT/tracker/matching.py\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        if \"np.float64\" in line:  # Properly indented\n",
    "            print(\"Change found:\", line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/working/BOT-SORT/tracker/matching.py\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/working/BOT-SORT/tracker/matching.py\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Replace float64 with np.float64\n",
    "updated_content = content.replace(\"dtype=float64\", \"dtype=np.float64\")\n",
    "\n",
    "# Write the updated content back to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(updated_content)\n",
    "\n",
    "print(\"File updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "reid_model_path = \"/kaggle/working/BOT-SORT/fast_reid/mot17_sbs_S50.pth\"\n",
    "\n",
    "try:\n",
    "    model_data = torch.load(reid_model_path, map_location=\"cpu\")\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!find /kaggle/working/BOT-SORT -type f -name \"*.py\" -exec sed -i 's/from torch._six import string_classes/import collections\\nstring_classes = (str, bytes)/g' {} +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ls -lh /kaggle/working/BOT-SORT/fast_reid/mot17_sbs_S50.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working/BOT-SORT/fast_reid/fastreid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!wget -O mot20_sbs_S50.pth https://github.com/NirAharon/BoT-SORT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -f /kaggle/working/BOT-SORT/fast_reid/fastreid/mot20_sbs_S50.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install yacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/working/BOT-SORT/tracker\")  # Add the tracker directory to Python path\n",
    "from bot_sort import BoTSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!sed -i 's/from collections.abc import Mapping, OrderedDict/from collections.abc import Mapping\\nfrom collections import OrderedDict/' /kaggle/working/BOT-SORT/fast_reid/fastreid/evaluation/testing.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mv /kaggle/input/reidsbs/mot20_sbs_S50.pth /kaggle/working/BOT-SORT/fast_reid/fastreid/mot20_sbs_S50.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(help(BoTSORT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# StrongSORT Configuration\n",
    "strongsort_config = \"\"\"\n",
    "tracker_type: strongsort\n",
    "track_high_thresh: 0.6\n",
    "track_low_thresh: 0.05\n",
    "new_track_thresh: 0.1\n",
    "iou_thresh: 0.5\n",
    "match_thresh: 0.95\n",
    "track_buffer: 90\n",
    "fuse_score: True\n",
    "frame_rate: 25\n",
    "det_thresh: 0.5\n",
    "lost_track_thresh: 120\n",
    "proximity_thresh: 0.3\n",
    "appearance_thresh: 0.15\n",
    "with_reid: True\n",
    "reid_model: /kaggle/input/face-reid/market_bot_R50-ibn.pth\n",
    "gmc_method: sparseOptFlow\n",
    "\"\"\"\n",
    "\n",
    "# Save StrongSORT configuration to strongsort.yaml\n",
    "with open(\"/kaggle/working/strongsort.yaml\", \"w\") as file:\n",
    "    file.write(strongsort_config)\n",
    "\n",
    "print(\"strongsort.yaml created successfully in /kaggle/working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run StrongSORT\n",
    "run_tracking(\"/kaggle/working/sort.yaml\", \"/kaggle/working/strongsort_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run StrongSORT\n",
    "run_tracking(\"/kaggle/working/ocsort.yaml\", \"/kaggle/working/ocsort_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torchreid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torchreid\n",
    "print(torchreid.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y torchreid\n",
    "!pip install torchreid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchreid import models\n",
    "\n",
    "# Load a ResNet-50 model\n",
    "reid_model = models.build_model(name=\"resnet50\", num_classes=1000, pretrained=True)\n",
    "reid_model.eval()\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(\"/kaggle/input/face-reid/market_bot_R50-ibn.pth\")\n",
    "\n",
    "# Extract the model weights\n",
    "model_weights = state_dict[\"model\"]\n",
    "\n",
    "# Load the model weights into the ReID model\n",
    "reid_model.load_state_dict(model_weights)\n",
    "reid_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"/kaggle/input/face-reid/market_bot_R50-ibn.pth\")\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"/kaggle/input/face-reid/market_bot_R50-ibn.pth\")\n",
    "model_weights = state_dict[\"model\"]\n",
    "print(model_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "class CustomReIDModel(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(CustomReIDModel, self).__init__()\n",
    "        # Load a ResNet-50 backbone\n",
    "        self.backbone = resnet50(pretrained=False)\n",
    "        \n",
    "        # Remove the final classification layer\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        # Add a bottleneck layer (bnneck)\n",
    "        self.bnneck = nn.BatchNorm1d(2048)\n",
    "        self.bnneck.bias.requires_grad_(False)  # No bias for BatchNorm\n",
    "        \n",
    "        # Add a classifier layer\n",
    "        self.classifier = nn.Linear(2048, num_classes, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features from the backbone\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Apply bottleneck layer\n",
    "        features = self.bnneck(features)\n",
    "        \n",
    "        # Apply classifier\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return features, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize the custom model\n",
    "reid_model = CustomReIDModel(num_classes=1000)\n",
    "reid_model.eval()\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(\"/kaggle/input/face-reid/market_bot_R50-ibn.pth\")\n",
    "\n",
    "# Extract the model weights\n",
    "model_weights = state_dict[\"model\"]\n",
    "\n",
    "# Load the model weights into the custom model\n",
    "reid_model.load_state_dict(model_weights, strict=False)\n",
    "reid_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Preprocess the detection crop\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 128)),  # Resize to the input size of the ReID model\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_features(image_crop):\n",
    "    \"\"\"\n",
    "    Extract appearance features from an image crop using the ReID model.\n",
    "    \"\"\"\n",
    "    image_crop = preprocess(image_crop).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        features, _ = reid_model(image_crop)  # Extract features (ignore logits)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Compute appearance similarity\n",
    "def compute_appearance_similarity(features1, features2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two sets of features.\n",
    "    \"\"\"\n",
    "    return 1 - cdist(features1, features2, metric=\"cosine\")\n",
    "\n",
    "# Combine IoU and appearance similarity\n",
    "def combine_matching(iou_matrix, appearance_matrix, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Combine IoU and appearance similarity into a single matching score.\n",
    "    \"\"\"\n",
    "    return alpha * iou_matrix + (1 - alpha) * appearance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "class CustomReIDModel(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(CustomReIDModel, self).__init__()\n",
    "        # Load a ResNet-50 backbone\n",
    "        self.backbone = resnet50(pretrained=False)\n",
    "        \n",
    "        # Remove the final classification layer\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        # Add a bottleneck layer (bnneck)\n",
    "        self.bnneck = nn.BatchNorm1d(2048)\n",
    "        self.bnneck.bias.requires_grad_(False)  # No bias for BatchNorm\n",
    "        \n",
    "        # Add a classifier layer\n",
    "        self.classifier = nn.Linear(2048, num_classes, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features from the backbone\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Apply bottleneck layer\n",
    "        features = self.bnneck(features)\n",
    "        \n",
    "        # Apply classifier\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return features, logits\n",
    "\n",
    "# Initialize the custom model\n",
    "reid_model = CustomReIDModel(num_classes=1000)\n",
    "reid_model.eval()\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(\"/kaggle/input/face-reid/market_bot_R50-ibn.pth\")\n",
    "\n",
    "# Extract the model weights\n",
    "model_weights = state_dict[\"model\"]\n",
    "\n",
    "# Load the model weights into the custom model\n",
    "reid_model.load_state_dict(model_weights, strict=False)\n",
    "reid_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the YOLO model\n",
    "model2 = YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\")\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "gt_file_path = \"/kaggle/input/testgt/gt.txt\"\n",
    "output_mot_path = \"/kaggle/working/bytetracking_ftlast_results.txt\"\n",
    "\n",
    "# Resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704  # YOLOv8 input size\n",
    "target_height = 704\n",
    "\n",
    "# Tracking results storage\n",
    "tracking_results = []\n",
    "\n",
    "# Preprocess function for ReID\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 128)),  # Resize to the input size of the ReID model\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_features(image_crop):\n",
    "    \"\"\"\n",
    "    Extract appearance features from an image crop using the ReID model.\n",
    "    \"\"\"\n",
    "    image_crop = preprocess(image_crop).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        features, _ = reid_model(image_crop)  # Extract features (ignore logits)\n",
    "    return features\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "    # Load and resize image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "    # Run YOLOv8 tracking with ByteTrack\n",
    "    results = model2.track(img_resized, persist=True, tracker=\"/kaggle/working/bytetrack.yaml\")[0]  \n",
    "\n",
    "    for track in results.boxes:\n",
    "        x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "        track_id = int(track.id[0].item()) if track.id is not None else -1  # Ensure track_id persists\n",
    "        conf = track.conf[0].item()\n",
    "\n",
    "        # Keep only person class (1 & 7 mapped to 1)\n",
    "        cls = int(track.cls[0].item())\n",
    "        if cls == 0:\n",
    "            # Convert to original image size\n",
    "            x_center *= (original_width / target_width)\n",
    "            y_center *= (original_height / target_height)\n",
    "            w *= (original_width / target_width)\n",
    "            h *= (original_height / target_height)\n",
    "\n",
    "            # Convert to top-left format\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "            w = int(w)\n",
    "            h = int(h)\n",
    "\n",
    "            # Extract appearance features\n",
    "            image_crop = img[y:y+h, x:x+w]  # Crop the detection\n",
    "            if image_crop.size == 0:\n",
    "                continue  # Skip empty crops\n",
    "            image_crop = Image.fromarray(cv2.cvtColor(image_crop, cv2.COLOR_BGR2RGB))\n",
    "            features = extract_features(image_crop)\n",
    "\n",
    "            # Save tracking result in MOT format\n",
    "            tracking_results.append([frame_number, track_id, x, y, w, h, round(conf, 4), 1, 1])\n",
    "\n",
    "            # Draw bounding boxes on image\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\"ID {track_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Sort tracking results by track_id first, then by frame number\n",
    "track_id_data = {}\n",
    "\n",
    "for row in tracking_results:\n",
    "    frame_number, track_id = row[0], row[1]\n",
    "    if track_id not in track_id_data:\n",
    "        track_id_data[track_id] = []\n",
    "    track_id_data[track_id].append(row)\n",
    "\n",
    "# Save results in MOT format\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for track_id in sorted(track_id_data.keys()):  # Sort by track_id first\n",
    "        for row in sorted(track_id_data[track_id], key=lambda x: x[0]):  # Sort frames within each ID\n",
    "            f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BYTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install cython scipy lap motmetrics ultralytics\n",
    "!git clone https://github.com/ifzhang/ByteTrack.git\n",
    "%cd ByteTrack\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir pretrained\n",
    "!wget -O pretrained/bytetrack_x_mot20.pth.tar \"https://github.com/ifzhang/ByteTrack/releases/download/v0.1.0/bytetrack_x_mot20.pth.tar\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "weights_dir = \"/kaggle/working/fast-reid/\"\n",
    "for root, dirs, files in os.walk(weights_dir):\n",
    "    for file in files:\n",
    "        print(os.path.join(root, file))  # Print all files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL:\n",
    "  BACKBONE:\n",
    "    NAME: \"build_resnet_backbone\"\n",
    "    DEPTH: 50\n",
    "  HEADS:\n",
    "    NAME: \"EmbeddingHead\"\n",
    "    IN_FEAT: 2048\n",
    "    NUM_CLASSES: 751\n",
    "    POOL_LAYER: \"avgpool\"\n",
    "  META_ARCHITECTURE: \"Baseline\"\n",
    "  PIXEL_MEAN: [123.675, 116.28, 103.53]\n",
    "  PIXEL_STD: [58.395, 57.12, 57.375]\n",
    "\n",
    "DATASETS:\n",
    "  NAMES: (\"Market1501\",)\n",
    "  TESTS: (\"Market1501\",)\n",
    "\n",
    "DATALOADER:\n",
    "  NUM_WORKERS: 8\n",
    "\n",
    "SOLVER:\n",
    "  IMS_PER_BATCH: 64\n",
    "  BASE_LR: 0.00035\n",
    "  MAX_ITER: 12000\n",
    "  STEPS: [4000, 8000]\n",
    "  CHECKPOINT_PERIOD: 2000\n",
    "\n",
    "TEST:\n",
    "  IMS_PER_BATCH: 128\n",
    "  EVAL_PERIOD: 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!wget -O /kaggle/working/fast-reid/model_final.pth https://github.com/JDAI-CV/fast-reid/releases/download/v0.1.1/market_mgn_R50-ibn.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import fastreid\n",
    "from fastreid.config import get_cfg\n",
    "from fastreid.modeling import build_model\n",
    "from fastreid.utils.checkpoint import Checkpointer\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# Load FastReID Model\n",
    "def load_reid_model(config_path, weights_path):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(config_path)\n",
    "    model = build_model(cfg)\n",
    "    Checkpointer(model).load(weights_path)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Extract Features for ReID\n",
    "def extract_features(model, img):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((256, 128)),  # Standard ReID input size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        features = model(img_tensor)\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "def cosine_similarity(feat1, feat2):\n",
    "    return np.dot(feat1, feat2.T) / (np.linalg.norm(feat1) * np.linalg.norm(feat2))\n",
    "\n",
    "# Match ReID Features to Improve Tracking\n",
    "def match_reid_tracks(existing_tracks, new_detections, model):\n",
    "    matched_tracks = {}\n",
    "    for track_id, track_feat in existing_tracks.items():\n",
    "        best_match = None\n",
    "        best_score = 0.0\n",
    "        for det_id, det_img in new_detections.items():\n",
    "            det_feat = extract_features(model, det_img)\n",
    "            score = cosine_similarity(track_feat, det_feat)\n",
    "            if score > best_score:\n",
    "                best_match = det_id\n",
    "                best_score = score\n",
    "        if best_match and best_score > 0.7:  # Threshold to avoid mismatches\n",
    "            matched_tracks[track_id] = best_match\n",
    "    return matched_tracks\n",
    "\n",
    "# Load ReID model\n",
    "reid_model = load_reid_model(\"/kaggle/working/fast-reid/configs/Base-MGN.yml\", \"/kaggle/working/fast-reid/model_final.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from fastreid.config import get_cfg\n",
    "from fastreid.modeling import build_model\n",
    "from fastreid.utils.checkpoint import Checkpointer\n",
    "from torchvision import transforms\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "output_mot_path = \"/kaggle/working/bytetrack_results.txt\"\n",
    "\n",
    "# Load YOLO Model\n",
    "model2 = YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\")\n",
    "\n",
    "# Load FastReID Model\n",
    "def load_reid_model(config_path, weights_path):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(config_path)\n",
    "    model = build_model(cfg)\n",
    "    Checkpointer(model).load(weights_path)\n",
    "    model.eval().cuda()\n",
    "    return model\n",
    "\n",
    "reid_model = load_reid_model(\"/kaggle/working/fast-reid/configs/Base-MGN.yml\", \"/kaggle/working/fast-reid/model_final.pth\")\n",
    "\n",
    "# Transform for FastReID\n",
    "reid_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Batch Feature Extraction for ReID\n",
    "def extract_reid_features_batch(model, images):\n",
    "    if not images:\n",
    "        return {}\n",
    "    \n",
    "    imgs_tensor = torch.stack([reid_transform(img) for img in images]).cuda()\n",
    "    with torch.no_grad():\n",
    "        features = model(imgs_tensor)\n",
    "    \n",
    "    return {i: features[i].cpu().numpy().flatten() for i in range(len(images))}\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "def cosine_similarity(feat1, feat2):\n",
    "    return np.dot(feat1, feat2) / (np.linalg.norm(feat1) * np.linalg.norm(feat2))\n",
    "\n",
    "# ReID Track Matching\n",
    "def match_reid_tracks(existing_tracks, new_detections):\n",
    "    matched_tracks = {}\n",
    "    new_ids, new_feats = list(new_detections.keys()), list(new_detections.values())\n",
    "\n",
    "    if not new_feats:\n",
    "        return matched_tracks\n",
    "\n",
    "    # Batch extract new features\n",
    "    new_features = extract_reid_features_batch(reid_model, new_feats)\n",
    "\n",
    "    for track_id, track_feat in existing_tracks.items():\n",
    "        best_match, best_score = None, -1.0\n",
    "        for det_id, det_feat in new_features.items():\n",
    "            score = cosine_similarity(track_feat, det_feat)\n",
    "            if score > best_score:\n",
    "                best_match, best_score = det_id, score\n",
    "\n",
    "        if best_match and best_score > 0.7:  # Threshold to reduce mismatches\n",
    "            matched_tracks[track_id] = best_match\n",
    "\n",
    "    return matched_tracks\n",
    "\n",
    "# Initialize tracking memory\n",
    "track_features = {}\n",
    "tracking_results = []\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "    \n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Run YOLO tracking\n",
    "    results = model2.track(img, persist=True, tracker=\"/kaggle/working/bytetrack.yaml\", imgsz=(704,704))[0]\n",
    "\n",
    "    new_detections = {}\n",
    "    cropped_images = []\n",
    "    track_ids = []\n",
    "\n",
    "    for track in results.boxes:\n",
    "        x, y, w, h = map(int, track.xywh[0].cpu().numpy())\n",
    "        track_id = int(track.id[0].item()) if track.id is not None else -1\n",
    "        conf = round(track.conf[0].item(), 4)\n",
    "        cls = int(track.cls[0].item())\n",
    "\n",
    "        if cls == 0:  # Only process people\n",
    "            cropped_person = img[y:y+h, x:x+w]\n",
    "            cropped_images.append(cropped_person)\n",
    "            track_ids.append(track_id)\n",
    "\n",
    "            tracking_results.append([frame_number, track_id, x, y, w, h, conf, 1, 1])\n",
    "\n",
    "    # Batch extract new ReID features\n",
    "    if cropped_images:\n",
    "        extracted_features = extract_reid_features_batch(reid_model, cropped_images)\n",
    "        for idx, track_id in enumerate(track_ids):\n",
    "            track_features[track_id] = extracted_features[idx]\n",
    "\n",
    "    # Match lost tracks using ReID\n",
    "    matched_tracks = match_reid_tracks(track_features, new_detections)\n",
    "\n",
    "    # Update track IDs with ReID-matched IDs\n",
    "    for old_id, new_id in matched_tracks.items():\n",
    "        for track in tracking_results:\n",
    "            if track[1] == old_id:\n",
    "                track[1] = new_id\n",
    "\n",
    "# Save tracking results in MOT format\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for row in sorted(tracking_results, key=lambda x: (x[1], x[0])):  # Sort by ID first, then frame\n",
    "        f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working/fast-reid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!find . -type f -name \"*.py\" -exec sed -i 's/from collections.abc import Mapping, OrderedDict/from collections.abc import Mapping\\nfrom collections import OrderedDict/g' {} +\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!grep -r \"from collections.abc import OrderedDict\" /kaggle/working/BOT-SORT/fast_reid/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!grep -r \"from collections.abc import OrderedDict\" .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install filterpy scikit-learn\n",
    "!pip install faiss-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!find /kaggle/working/BOT-SORT/fast_reid/fastreid -type f -name \"*.py\" -exec sed -i 's/from collections import Mapping/from collections.abc import Mapping/g' {} +\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/JDAI-CV/fast-reid.git\n",
    "%cd fast-reid\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install yacs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import fastreid\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from fastreid.config import get_cfg\n",
    "from fastreid.modeling import build_model\n",
    "from fastreid.utils.checkpoint import Checkpointer\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from filterpy.kalman import KalmanFilter\n",
    "\n",
    "# Load the YOLO model\n",
    "model2 = YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\")\n",
    "\n",
    "# Load the ReID model\n",
    "def load_reid_model(config_path, weights_path):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(config_path)\n",
    "    model = build_model(cfg)\n",
    "    Checkpointer(model).load(weights_path)\n",
    "    model.eval().cuda()\n",
    "    return model\n",
    "\n",
    "reid_model = load_reid_model(\"/kaggle/working/fast-reid/configs/Base-MGN.yml\", \"/kaggle/working/fast-reid/model_final.pth\")\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "output_mot_path = \"/kaggle/working/bytetracking_ftlast_results.txt\"\n",
    "\n",
    "# Resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704\n",
    "target_height = 704\n",
    "\n",
    "# Tracking results storage\n",
    "tracking_results = []\n",
    "track_features = {}  # Store features for each track ID\n",
    "track_last_frame = {}  # Store the last frame number for each track ID\n",
    "track_counter = 0  # Counter for assigning new IDs\n",
    "max_unmatched_frames = 30  # Maximum frames to keep a track alive without matching\n",
    "\n",
    "# Kalman filter setup\n",
    "def create_kalman_filter():\n",
    "    kf = KalmanFilter(dim_x=7, dim_z=4)\n",
    "    kf.F = np.array([[1, 0, 0, 0, 1, 0, 0],\n",
    "                     [0, 1, 0, 0, 0, 1, 0],\n",
    "                     [0, 0, 1, 0, 0, 0, 1],\n",
    "                     [0, 0, 0, 1, 0, 0, 0],\n",
    "                     [0, 0, 0, 0, 1, 0, 0],\n",
    "                     [0, 0, 0, 0, 0, 1, 0],\n",
    "                     [0, 0, 0, 0, 0, 0, 1]])\n",
    "    kf.H = np.array([[1, 0, 0, 0, 0, 0, 0],\n",
    "                     [0, 1, 0, 0, 0, 0, 0],\n",
    "                     [0, 0, 1, 0, 0, 0, 0],\n",
    "                     [0, 0, 0, 1, 0, 0, 0]])\n",
    "    kf.P *= 1000  # Initial uncertainty\n",
    "    kf.R = np.eye(4) * 5  # Reduced measurement noise\n",
    "    kf.Q = np.eye(7) * 0.5  # Increased process noise\n",
    "    return kf\n",
    "\n",
    "track_kalman_filters = {}  # Store Kalman filters for each track ID\n",
    "\n",
    "# Feature aggregation using exponential moving average (EMA)\n",
    "def update_feature_ema(track_id, new_feature, alpha=0.8):  # Reduced alpha\n",
    "    if track_id in track_features:\n",
    "        track_features[track_id] = alpha * track_features[track_id] + (1 - alpha) * new_feature\n",
    "    else:\n",
    "        track_features[track_id] = new_feature\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "    results = model2.track(img_resized, persist=True, tracker=\"/kaggle/working/bytetrack.yaml\")[0]\n",
    "\n",
    "    current_frame_features = {}\n",
    "    current_frame_boxes = {}\n",
    "\n",
    "    for track in results.boxes:\n",
    "        x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "        track_id = int(track.id[0].item()) if track.id is not None else -1\n",
    "        conf = track.conf[0].item()\n",
    "\n",
    "        cls = int(track.cls[0].item())\n",
    "        if cls == 0:\n",
    "            x_center *= (original_width / target_width)\n",
    "            y_center *= (original_height / target_height)\n",
    "            w *= (original_width / target_width)\n",
    "            h *= (original_height / target_height)\n",
    "\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "            w = int(w)\n",
    "            h = int(h)\n",
    "\n",
    "            person_img = img[y:y+h, x:x+w]\n",
    "            if person_img.size == 0:\n",
    "                continue\n",
    "\n",
    "            person_img_resized = cv2.resize(person_img, (128, 256))\n",
    "            person_img_tensor = torch.from_numpy(person_img_resized).permute(2, 0, 1).float().div(255.0).unsqueeze(0).cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reid_features = reid_model(person_img_tensor).cpu().numpy()\n",
    "\n",
    "            current_frame_features[track_id] = reid_features\n",
    "            current_frame_boxes[track_id] = (x, y, w, h, conf)\n",
    "\n",
    "    # Match tracks using Hungarian algorithm\n",
    "    if track_features:\n",
    "        track_ids = list(track_features.keys())\n",
    "        current_ids = list(current_frame_features.keys())\n",
    "\n",
    "        cost_matrix = np.zeros((len(track_ids), len(current_ids)))\n",
    "        for i, tid in enumerate(track_ids):\n",
    "            for j, cid in enumerate(current_ids):\n",
    "                cost_matrix[i, j] = cdist(track_features[tid], current_frame_features[cid], metric='cosine')\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        matched_pairs = list(zip(row_ind, col_ind))\n",
    "\n",
    "        matched_tracks = set()\n",
    "        matched_detections = set()\n",
    "\n",
    "        for i, j in matched_pairs:\n",
    "            if cost_matrix[i, j] < 0.5:  # Relaxed threshold for matching\n",
    "                track_id = track_ids[i]\n",
    "                current_id = current_ids[j]\n",
    "                update_feature_ema(track_id, current_frame_features[current_id])  # Update features using EMA\n",
    "                track_last_frame[track_id] = frame_number  # Update last frame number\n",
    "                x, y, w, h, conf = current_frame_boxes[current_id]\n",
    "                tracking_results.append([frame_number, track_id, x, y, w, h, round(conf, 4), 1, 1])\n",
    "                matched_tracks.add(track_id)\n",
    "                matched_detections.add(current_id)\n",
    "\n",
    "        # Handle unmatched tracks\n",
    "        for tid in track_ids:\n",
    "            if tid not in matched_tracks:\n",
    "                # Remove tracks that are unmatched for too many frames\n",
    "                if frame_number - track_last_frame[tid] > max_unmatched_frames:\n",
    "                    del track_features[tid]\n",
    "                    del track_last_frame[tid]\n",
    "                    if tid in track_kalman_filters:\n",
    "                        del track_kalman_filters[tid]\n",
    "\n",
    "        # Handle unmatched detections (assign new IDs)\n",
    "        for cid in current_ids:\n",
    "            if cid not in matched_detections:\n",
    "                track_counter += 1\n",
    "                update_feature_ema(track_counter, current_frame_features[cid])  # Initialize features using EMA\n",
    "                track_last_frame[track_counter] = frame_number  # Set last frame number\n",
    "                x, y, w, h, conf = current_frame_boxes[cid]\n",
    "                tracking_results.append([frame_number, track_counter, x, y, w, h, round(conf, 4), 1, 1])\n",
    "    else:\n",
    "        # First frame, assign new IDs\n",
    "        for track_id, reid_features in current_frame_features.items():\n",
    "            track_counter += 1\n",
    "            update_feature_ema(track_counter, reid_features)  # Initialize features using EMA\n",
    "            track_last_frame[track_counter] = frame_number  # Set last frame number\n",
    "            x, y, w, h, conf = current_frame_boxes[track_id]\n",
    "            tracking_results.append([frame_number, track_counter, x, y, w, h, round(conf, 4), 1, 1])\n",
    "\n",
    "# Save results in MOT format\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for row in sorted(tracking_results, key=lambda x: (x[1], x[0])):\n",
    "        f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/working/fast-reid\")  # Adjust the path if necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import fastreid\n",
    "print(\"FastReID successfully imported!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install git+https://github.com/mikel-brostrom/BoT-SORT.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From ZERO TO HERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics lap thop opencv-python-headless supervision\n",
    "!pip install fastreid torchreid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = Path(\"/kaggle/input/tracking/tracking/tracking/test/01/img1\") # Update with correct path\n",
    "assert DATASET_PATH.exists(), \"Dataset not found! Upload MOT20-01.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load YOLOv8 model (can use yolov8m.pt or a fine-tuned one)\n",
    "model = YOLO(\"/kaggle/input/model-trial/pytorch/default/3/best (1).pt\")  # Change this if using your fine-tuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Test on a single image\n",
    "image_path = str(DATASET_PATH/ \"000001.jpg\")  # Ensure correct path\n",
    "results = model(image_path)\n",
    "\n",
    "# Show results using Matplotlib\n",
    "for r in results:\n",
    "    im_array = r.plot()  # YOLOv8 outputs BGR image\n",
    "\n",
    "    # Convert BGR to RGB\n",
    "    im_array = cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resize for better visualization\n",
    "    im_resized = cv2.resize(im_array, (800, 400))\n",
    "\n",
    "    # Display using Matplotlib\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(im_resized)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install strongsort supervision torchreid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from ultralytics.trackers.byte_tracker import BYTETracker\n",
    "# from ultralytics.trackers.bot_sort import BOTSORT\n",
    "# from types import SimpleNamespace\n",
    "\n",
    "# class BOTSORT(BYTETracker):\n",
    "#     def __init__(self):\n",
    "#         \"\"\"\n",
    "#         Initialize YOLOv8 object with ReID module and GMC algorithm.\n",
    "#         \"\"\"\n",
    "#         # Create a simple args object with required attributes\n",
    "#         args = SimpleNamespace(track_buffer=30)  # Add other required attributes if needed\n",
    "#         super().__init__(args=args, frame_rate=25)  # Pass the args object\n",
    "\n",
    "#         # Assign the provided parameters\n",
    "#         self.with_reid = True\n",
    "#         self.appearance_thresh = 0.25\n",
    "#         self.iou_threshold = 0.3\n",
    "#         self.max_age = 30\n",
    "#         self.min_hits = 3\n",
    "#         self.model_weights = \"/kaggle/input/reidsbs/mot20_sbs_S50.pth\"\n",
    "#         self.proximity_thresh = 0.5\n",
    "#         self.track_buffer = 30\n",
    "#         self.tracker_type = \"botsort\"\n",
    "#         self.track_high_thresh = 0.25\n",
    "#         self.track_low_thresh = 0.25\n",
    "#         self.track_buffer = 30\n",
    "#         self.fuse_score = True\n",
    "#         self.new_track_thresh = 0.25\n",
    "#         self.match_thresh = 0.8\n",
    "\n",
    "#         # Initialize additional necessary attributes\n",
    "#         self.gmc_method = \"sparseOptFlow\"  # Global Motion Compensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NirAharon/BoT-SORT.git\n",
    "%cd BoT-SORT\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-17T01:13:49.265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load full model\n",
    "ckpt = torch.load('/kaggle/input/trackingyolo/pytorch/default/5/best.pt', map_location='cpu')\n",
    "\n",
    "# Remove optimizer states\n",
    "if 'optimizer' in ckpt:\n",
    "    del ckpt['optimizer']\n",
    "\n",
    "# Save the smaller model\n",
    "torch.save(ckpt, 'best_stripped.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T18:34:26.712661Z",
     "iopub.status.busy": "2025-03-24T18:34:26.712336Z",
     "iopub.status.idle": "2025-03-24T18:34:32.408294Z",
     "shell.execute_reply": "2025-03-24T18:34:32.407327Z",
     "shell.execute_reply.started": "2025-03-24T18:34:26.712636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.95-py3-none-any.whl.metadata (35 kB)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n",
      "Downloading ultralytics-8.3.95-py3-none-any.whl (949 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: ultralytics-thop, ultralytics\n",
      "Successfully installed ultralytics-8.3.95 ultralytics-thop-2.0.14\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T21:00:58.437249Z",
     "iopub.status.busy": "2025-03-23T21:00:58.436966Z",
     "iopub.status.idle": "2025-03-23T21:01:05.041197Z",
     "shell.execute_reply": "2025-03-23T21:01:05.040304Z",
     "shell.execute_reply.started": "2025-03-23T21:00:58.437222Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting filterpy\n",
      "  Downloading filterpy-1.4.5.zip (177 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from filterpy) (1.26.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from filterpy) (1.13.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from filterpy) (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->filterpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->filterpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->filterpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->filterpy) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->filterpy) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->filterpy) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->filterpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->filterpy) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->filterpy) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->filterpy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->filterpy) (2024.2.0)\n",
      "Building wheels for collected packages: filterpy\n",
      "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110458 sha256=ac37b21b2992522f5f467cec01ebc67e94dd2b99d9adaf321979daf3b745b55f\n",
      "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
      "Successfully built filterpy\n",
      "Installing collected packages: filterpy\n",
      "Successfully installed filterpy-1.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install filterpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T18:52:15.696301Z",
     "iopub.status.busy": "2025-03-22T18:52:15.695943Z",
     "iopub.status.idle": "2025-03-22T18:52:21.095976Z",
     "shell.execute_reply": "2025-03-22T18:52:21.095060Z",
     "shell.execute_reply.started": "2025-03-22T18:52:15.696270Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchreid\n",
      "  Downloading torchreid-0.2.5.tar.gz (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: torchreid\n",
      "  Building wheel for torchreid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torchreid: filename=torchreid-0.2.5-py3-none-any.whl size=144325 sha256=6c9b87541e38800b9fabc47af7fd705c871523503da07e53474b90df0d879195\n",
      "  Stored in directory: /root/.cache/pip/wheels/bb/2d/36/816a48465cefd3e58be0317648a4c52ce39ae817f935212099\n",
      "Successfully built torchreid\n",
      "Installing collected packages: torchreid\n",
      "Successfully installed torchreid-0.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install torchreid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model without reid 67.99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T18:23:06.555515Z",
     "iopub.status.busy": "2025-03-24T18:23:06.555161Z",
     "iopub.status.idle": "2025-03-24T18:24:03.554621Z",
     "shell.execute_reply": "2025-03-24T18:24:03.553826Z",
     "shell.execute_reply.started": "2025-03-24T18:23:06.555483Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n",
      "Collecting lap>=0.5.12\n",
      "  Downloading lap-0.5.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from lap>=0.5.12) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.6->lap>=0.5.12) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.6->lap>=0.5.12) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\n",
      "Downloading lap-0.5.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 51.9 MB/s eta 0:00:00\n",
      "Installing collected packages: lap\n",
      "Successfully installed lap-0.5.12\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 3.4s, installed 1 package: ['lap>=0.5.12']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 7.3ms preprocess, 43.1ms inference, 270.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.2ms\n",
      "Speed: 3.5ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 2.3ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 40.4ms\n",
      "Speed: 2.0ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 40.4ms\n",
      "Speed: 1.5ms preprocess, 40.4ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 40.5ms\n",
      "Speed: 2.2ms preprocess, 40.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 40.4ms\n",
      "Speed: 1.9ms preprocess, 40.4ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 40.5ms\n",
      "Speed: 2.1ms preprocess, 40.5ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 2.2ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.7ms\n",
      "Speed: 1.7ms preprocess, 39.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 40.0ms\n",
      "Speed: 1.7ms preprocess, 40.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 40.4ms\n",
      "Speed: 1.7ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 40.1ms\n",
      "Speed: 1.8ms preprocess, 40.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 40.4ms\n",
      "Speed: 2.0ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 40.4ms\n",
      "Speed: 1.9ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 40.0ms\n",
      "Speed: 1.9ms preprocess, 40.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 40.4ms\n",
      "Speed: 1.7ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 40.4ms\n",
      "Speed: 1.9ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 38.8ms\n",
      "Speed: 1.8ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 38.9ms\n",
      "Speed: 2.0ms preprocess, 38.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 38.8ms\n",
      "Speed: 1.8ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 38.8ms\n",
      "Speed: 1.8ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 38.9ms\n",
      "Speed: 1.9ms preprocess, 38.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 38.8ms\n",
      "Speed: 2.0ms preprocess, 38.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 38.9ms\n",
      "Speed: 1.9ms preprocess, 38.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 38.9ms\n",
      "Speed: 1.9ms preprocess, 38.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 38.9ms\n",
      "Speed: 1.9ms preprocess, 38.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 38.9ms\n",
      "Speed: 2.0ms preprocess, 38.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 38.8ms\n",
      "Speed: 1.9ms preprocess, 38.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 38.9ms\n",
      "Speed: 1.9ms preprocess, 38.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 38.9ms\n",
      "Speed: 2.2ms preprocess, 38.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 38.9ms\n",
      "Speed: 1.7ms preprocess, 38.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 38.9ms\n",
      "Speed: 2.0ms preprocess, 38.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 38.9ms\n",
      "Speed: 2.0ms preprocess, 38.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 38.9ms\n",
      "Speed: 2.0ms preprocess, 38.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 38.9ms\n",
      "Speed: 2.3ms preprocess, 38.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 38.8ms\n",
      "Speed: 1.9ms preprocess, 38.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 38.8ms\n",
      "Speed: 1.9ms preprocess, 38.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 38.8ms\n",
      "Speed: 1.7ms preprocess, 38.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.2ms\n",
      "Speed: 1.9ms preprocess, 39.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.7ms\n",
      "Speed: 2.1ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 40.5ms\n",
      "Speed: 2.1ms preprocess, 40.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 40.4ms\n",
      "Speed: 2.4ms preprocess, 40.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.7ms\n",
      "Speed: 2.1ms preprocess, 39.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.7ms\n",
      "Speed: 2.2ms preprocess, 39.7ms inference, 2.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.7ms\n",
      "Speed: 1.5ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.7ms\n",
      "Speed: 1.9ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.7ms\n",
      "Speed: 1.9ms preprocess, 39.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.7ms\n",
      "Speed: 2.4ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.7ms\n",
      "Speed: 2.1ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 41.2ms\n",
      "Speed: 1.7ms preprocess, 41.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 41.2ms\n",
      "Speed: 1.9ms preprocess, 41.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 40.4ms\n",
      "Speed: 1.7ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.7ms\n",
      "Speed: 1.9ms preprocess, 39.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 40.1ms\n",
      "Speed: 1.9ms preprocess, 40.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 39.6ms\n",
      "Speed: 2.2ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 41.2ms\n",
      "Speed: 1.8ms preprocess, 41.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 41.2ms\n",
      "Speed: 1.8ms preprocess, 41.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.8ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.7ms\n",
      "Speed: 2.0ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 41.6ms\n",
      "Speed: 2.0ms preprocess, 41.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 41.3ms\n",
      "Speed: 1.7ms preprocess, 41.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 38.1ms\n",
      "Speed: 2.1ms preprocess, 38.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 38.1ms\n",
      "Speed: 2.0ms preprocess, 38.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 38.1ms\n",
      "Speed: 1.8ms preprocess, 38.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 38.2ms\n",
      "Speed: 2.0ms preprocess, 38.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 38.1ms\n",
      "Speed: 2.1ms preprocess, 38.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 38.2ms\n",
      "Speed: 2.2ms preprocess, 38.2ms inference, 2.0ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 38.2ms\n",
      "Speed: 3.0ms preprocess, 38.2ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 38.2ms\n",
      "Speed: 2.2ms preprocess, 38.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 38.2ms\n",
      "Speed: 2.1ms preprocess, 38.2ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 38.2ms\n",
      "Speed: 2.1ms preprocess, 38.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 38.2ms\n",
      "Speed: 2.1ms preprocess, 38.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 38.2ms\n",
      "Speed: 2.3ms preprocess, 38.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 38.1ms\n",
      "Speed: 2.3ms preprocess, 38.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 38.1ms\n",
      "Speed: 2.1ms preprocess, 38.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 38.1ms\n",
      "Speed: 2.0ms preprocess, 38.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 38.8ms\n",
      "Speed: 1.6ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 38.8ms\n",
      "Speed: 2.1ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 38.9ms\n",
      "Speed: 1.7ms preprocess, 38.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 38.9ms\n",
      "Speed: 2.0ms preprocess, 38.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 40.3ms\n",
      "Speed: 1.8ms preprocess, 40.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 40.4ms\n",
      "Speed: 1.7ms preprocess, 40.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 40.1ms\n",
      "Speed: 1.8ms preprocess, 40.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 40.4ms\n",
      "Speed: 1.9ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 40.4ms\n",
      "Speed: 1.9ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 40.4ms\n",
      "Speed: 2.1ms preprocess, 40.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.5ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.5ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 40.3ms\n",
      "Speed: 1.9ms preprocess, 40.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 40.4ms\n",
      "Speed: 1.9ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 40.4ms\n",
      "Speed: 1.9ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 40.4ms\n",
      "Speed: 1.7ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.7ms\n",
      "Speed: 2.3ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.5ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.7ms\n",
      "Speed: 1.9ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 40.2ms\n",
      "Speed: 1.9ms preprocess, 40.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 40.4ms\n",
      "Speed: 2.0ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 40.4ms\n",
      "Speed: 1.9ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 40.4ms\n",
      "Speed: 1.5ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 40.1ms\n",
      "Speed: 1.9ms preprocess, 40.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 40.4ms\n",
      "Speed: 1.5ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.5ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.7ms\n",
      "Speed: 2.0ms preprocess, 39.7ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.7ms\n",
      "Speed: 2.0ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.9ms\n",
      "Speed: 1.8ms preprocess, 39.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 41.2ms\n",
      "Speed: 1.8ms preprocess, 41.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 41.3ms\n",
      "Speed: 1.8ms preprocess, 41.3ms inference, 1.8ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.5ms preprocess, 39.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.7ms\n",
      "Speed: 2.2ms preprocess, 39.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.7ms\n",
      "Speed: 2.0ms preprocess, 39.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.7ms\n",
      "Speed: 1.9ms preprocess, 39.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.7ms\n",
      "Speed: 2.1ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.7ms\n",
      "Speed: 2.0ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.7ms\n",
      "Speed: 2.0ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.7ms\n",
      "Speed: 1.9ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "Tracking results saved to /kaggle/working/modeltrackingresults.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model2 = YOLO('/kaggle/input/trackingyolo/pytorch/default/6/best.pt')\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "gt_file_path = \"/kaggle/input/testgt/gt.txt\"\n",
    "output_mot_path = \"/kaggle/working/modeltrackingresults.txt\"\n",
    "\n",
    "# Resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704  # YOLOv8 input size\n",
    "target_height = 704\n",
    "\n",
    "# Tracking results storage\n",
    "tracking_results = []\n",
    "track_history = {}  # Store previous frames' tracking data for smoother tracking\n",
    "\n",
    "# Optimized confidence & IOU thresholds\n",
    "iou_threshold = 0.6  # Slightly increased for better tracking continuity\n",
    "conf_threshold = 0.22  # Lowered to retain potential detections\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "    # Load and resize image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "    # Run YOLOv8 tracking with optimized parameters\n",
    "    results = model2.track(img_resized, persist=True, iou=iou_threshold, conf=conf_threshold)[0]  \n",
    "\n",
    "    for track in results.boxes:\n",
    "        x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "        track_id = int(track.id[0].item()) if track.id is not None and len(track.id) > 0 else -1  # Ensure track_id persists\n",
    "        conf = track.conf[0].item()\n",
    "\n",
    "        # Keep only person class\n",
    "        cls = int(track.cls[0].item())\n",
    "        if cls == 0:\n",
    "            # Scale back to original image size\n",
    "            scale_x = original_width / target_width\n",
    "            scale_y = original_height / target_height\n",
    "\n",
    "            x_center *= scale_x\n",
    "            y_center *= scale_y\n",
    "            w *= scale_x\n",
    "            h *= scale_y\n",
    "\n",
    "            # Convert to top-left format\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "            w = int(w)\n",
    "            h = int(h)\n",
    "\n",
    "            # Smooth ID handling: prevent flickering IDs by checking history\n",
    "            if track_id in track_history and abs(track_history[track_id][0] - x_center) < 50:\n",
    "                track_id = track_history[track_id][1]  # Keep previous ID\n",
    "\n",
    "            track_history[track_id] = (x_center, track_id)\n",
    "\n",
    "            # Save tracking result in MOT format\n",
    "            tracking_results.append([frame_number, track_id, x, y, w, h, round(conf, 4), 1, 1])\n",
    "\n",
    "            # Draw bounding boxes on image\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\"ID {track_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Keep only valid track IDs (removes short-lived detections)\n",
    "track_id_data = {}\n",
    "\n",
    "for row in tracking_results:\n",
    "    frame_number, track_id = row[0], row[1]\n",
    "    if track_id not in track_id_data:\n",
    "        track_id_data[track_id] = []\n",
    "    track_id_data[track_id].append(row)\n",
    "\n",
    "# Remove short-lived tracks (less than 5 frames for more stable tracking)\n",
    "min_track_length = 20\n",
    "track_id_data = {k: v for k, v in track_id_data.items() if len(v) >= min_track_length}\n",
    "\n",
    "# Save results in MOT format using your original sorting\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for track_id in sorted(track_id_data.keys()):  # Sort by track_id first\n",
    "        for row in sorted(track_id_data[track_id], key=lambda x: x[0]):  # Sort frames within each ID\n",
    "            f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL WITH REID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T20:04:51.653883Z",
     "iopub.status.busy": "2025-03-22T20:04:51.653573Z",
     "iopub.status.idle": "2025-03-22T20:23:54.188686Z",
     "shell.execute_reply": "2025-03-22T20:23:54.187837Z",
     "shell.execute_reply.started": "2025-03-22T20:04:51.653860Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded imagenet pretrained weights from \"/root/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchreid/reid/utils/torchtools.py:297: UserWarning: The pretrained weights \"/kaggle/input/reidsbs/mot20_sbs_S50.pth\" cannot be loaded, please check the key names manually (** ignored and continue **)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.8ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 2.2ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 2.9ms preprocess, 43.1ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 2.3ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 1.6ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 2.3ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.2ms\n",
      "Speed: 2.3ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.9ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.6ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.2ms\n",
      "Speed: 1.6ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.2ms\n",
      "Speed: 2.9ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 1.6ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 43.2ms\n",
      "Speed: 1.6ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 1.6ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.2ms\n",
      "Speed: 2.2ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 2.3ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.2ms\n",
      "Speed: 2.2ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.2ms\n",
      "Speed: 2.2ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.2ms\n",
      "Speed: 1.7ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.2ms\n",
      "Speed: 2.2ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 2.2ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 43.1ms\n",
      "Speed: 1.6ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 43.2ms\n",
      "Speed: 2.3ms preprocess, 43.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 43.1ms\n",
      "Speed: 2.3ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 43.2ms\n",
      "Speed: 1.7ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.9ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 43.2ms\n",
      "Speed: 2.3ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 43.2ms\n",
      "Speed: 2.2ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 1.8ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.3ms\n",
      "Speed: 1.9ms preprocess, 43.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.2ms\n",
      "Speed: 2.2ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.6ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 2.4ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 1.6ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 2.2ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 2.3ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 2.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.9ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.2ms\n",
      "Speed: 1.7ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.3ms\n",
      "Speed: 1.7ms preprocess, 43.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.2ms\n",
      "Speed: 1.7ms preprocess, 43.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 43.1ms\n",
      "Speed: 2.5ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.2ms\n",
      "Speed: 2.3ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 43.1ms\n",
      "Speed: 1.9ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 43.2ms\n",
      "Speed: 2.1ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "Tracking results saved to /kaggle/working/modeltrackingresults.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from torchreid import models, utils\n",
    "\n",
    "# Load the YOLO model\n",
    "model2 = YOLO('/kaggle/input/trackingyolo/pytorch/default/6/best.pt')\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "gt_file_path = \"/kaggle/input/testgt/gt.txt\"\n",
    "output_mot_path = \"/kaggle/working/modeltrackingresults.txt\"\n",
    "reid_model_path = \"/kaggle/input/reidsbs/mot20_sbs_S50.pth\"\n",
    "\n",
    "# Resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704  # YOLOv8 input size\n",
    "target_height = 704\n",
    "\n",
    "# Tracking results storage\n",
    "tracking_results = []\n",
    "track_history = {}  # Store previous frames' tracking data for smoother tracking\n",
    "track_features = {}  # Store appearance features for each track ID\n",
    "\n",
    "# Optimized confidence & IOU thresholds\n",
    "iou_threshold = 0.6  # Slightly increased for better tracking continuity\n",
    "conf_threshold = 0.22  # Lowered to retain potential detections\n",
    "\n",
    "# Load the ReID model\n",
    "reid_model = models.build_model(name=\"osnet_x1_0\", num_classes=1000)\n",
    "utils.load_pretrained_weights(reid_model, reid_model_path)\n",
    "reid_model.eval()\n",
    "\n",
    "# Function to extract features using the ReID model\n",
    "def extract_features(img, bbox):\n",
    "    x, y, w, h = bbox\n",
    "    crop = img[y:y+h, x:x+w]\n",
    "    crop = cv2.resize(crop, (128, 256))  # Resize to ReID input size\n",
    "    crop = torch.from_numpy(crop).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "    with torch.no_grad():\n",
    "        features = reid_model(crop)\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(feat1, feat2):\n",
    "    feat1 = feat1.flatten()  # Flatten to 1D array\n",
    "    feat2 = feat2.flatten()  # Flatten to 1D array\n",
    "    return np.dot(feat1, feat2) / (np.linalg.norm(feat1) * np.linalg.norm(feat2))\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "    # Load and resize image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "    # Run YOLOv8 tracking with optimized parameters\n",
    "    results = model2.track(img_resized, persist=True, iou=iou_threshold, conf=conf_threshold)[0]  \n",
    "\n",
    "    for track in results.boxes:\n",
    "        x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "        track_id = int(track.id[0].item()) if track.id is not None and len(track.id) > 0 else -1  # Ensure track_id persists\n",
    "        conf = track.conf[0].item()\n",
    "\n",
    "        # Keep only person class\n",
    "        cls = int(track.cls[0].item())\n",
    "        if cls == 0:\n",
    "            # Scale back to original image size\n",
    "            scale_x = original_width / target_width\n",
    "            scale_y = original_height / target_height\n",
    "\n",
    "            x_center *= scale_x\n",
    "            y_center *= scale_y\n",
    "            w *= scale_x\n",
    "            h *= scale_y\n",
    "\n",
    "            # Convert to top-left format\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "            w = int(w)\n",
    "            h = int(h)\n",
    "\n",
    "            # Extract appearance features using ReID model\n",
    "            bbox = (x, y, w, h)\n",
    "            features = extract_features(img, bbox)\n",
    "\n",
    "            # Match features with existing tracks\n",
    "            best_match_id = None\n",
    "            best_similarity = -1\n",
    "\n",
    "            for existing_id, existing_features in track_features.items():\n",
    "                similarity = cosine_similarity(features, existing_features)\n",
    "                if similarity > best_similarity:\n",
    "                    best_similarity = similarity\n",
    "                    best_match_id = existing_id\n",
    "\n",
    "            # Re-assign ID if similarity is above a threshold\n",
    "            if best_similarity > 0.95:  # Threshold for matching\n",
    "                track_id = best_match_id\n",
    "\n",
    "            # Update track history and features\n",
    "            track_history[track_id] = (x_center, track_id)\n",
    "            track_features[track_id] = features\n",
    "\n",
    "            # Save tracking result in MOT format\n",
    "            tracking_results.append([frame_number, track_id, x, y, w, h, round(conf, 4), 1, 1])\n",
    "\n",
    "            # Draw bounding boxes on image\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\"ID {track_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Keep only valid track IDs (removes short-lived detections)\n",
    "track_id_data = {}\n",
    "\n",
    "for row in tracking_results:\n",
    "    frame_number, track_id = row[0], row[1]\n",
    "    if track_id not in track_id_data:\n",
    "        track_id_data[track_id] = []\n",
    "    track_id_data[track_id].append(row)\n",
    "\n",
    "# Remove short-lived tracks (less than 5 frames for more stable tracking)\n",
    "min_track_length = 20\n",
    "track_id_data = {k: v for k, v in track_id_data.items() if len(v) >= min_track_length}\n",
    "\n",
    "# Save results in MOT format using your original sorting\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for track_id in sorted(track_id_data.keys()):  # Sort by track_id first\n",
    "        for row in sorted(track_id_data[track_id], key=lambda x: x[0]):  # Sort frames within each ID\n",
    "            f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T17:57:02.605414Z",
     "iopub.status.busy": "2025-03-22T17:57:02.605079Z",
     "iopub.status.idle": "2025-03-22T17:57:02.611415Z",
     "shell.execute_reply": "2025-03-22T17:57:02.610597Z",
     "shell.execute_reply.started": "2025-03-22T17:57:02.605380Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped visualization folder saved at: /kaggle/working/visualized_modeltrack_output.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define folder paths\n",
    "visualization_output = \"/kaggle/working/modeltracking_output\"\n",
    "zip_path = \"/kaggle/working/visualized_modeltrack_output.zip\"\n",
    "\n",
    "# Zip the folder\n",
    "shutil.make_archive(zip_path.replace(\".zip\", \"\"), 'zip', visualization_output)\n",
    "\n",
    "print(f\"Zipped visualization folder saved at: {zip_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T21:03:46.278526Z",
     "iopub.status.busy": "2025-03-23T21:03:46.278145Z",
     "iopub.status.idle": "2025-03-23T21:03:50.622554Z",
     "shell.execute_reply": "2025-03-23T21:03:50.621334Z",
     "shell.execute_reply.started": "2025-03-23T21:03:46.278495Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting motmetrics\n",
      "  Downloading motmetrics-1.4.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from motmetrics) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from motmetrics) (2.2.3)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from motmetrics) (1.13.1)\n",
      "Collecting xmltodict>=0.12.0 (from motmetrics)\n",
      "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.1->motmetrics) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.1->motmetrics) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.1->motmetrics) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.1->motmetrics) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.1->motmetrics) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.12.1->motmetrics) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->motmetrics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->motmetrics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.1->motmetrics) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.23.1->motmetrics) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.12.1->motmetrics) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.12.1->motmetrics) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.12.1->motmetrics) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.12.1->motmetrics) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.12.1->motmetrics) (2024.2.0)\n",
      "Downloading motmetrics-1.4.0-py3-none-any.whl (161 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: xmltodict, motmetrics\n",
      "Successfully installed motmetrics-1.4.0 xmltodict-0.14.2\n"
     ]
    }
   ],
   "source": [
    "!pip install motmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T21:05:43.313023Z",
     "iopub.status.busy": "2025-03-23T21:05:43.312697Z",
     "iopub.status.idle": "2025-03-23T21:05:44.154798Z",
     "shell.execute_reply": "2025-03-23T21:05:44.153736Z",
     "shell.execute_reply.started": "2025-03-23T21:05:43.313000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         mota      motp      idf1  precision    recall\n",
      "acc  0.679914  0.325606  0.694606   0.911306  0.759936\n"
     ]
    }
   ],
   "source": [
    "import motmetrics as mm\n",
    "import numpy as np\n",
    "\n",
    "# Load ground truth and predicted tracks\n",
    "def load_tracks(file_path):\n",
    "    data = np.loadtxt(file_path, delimiter=',')\n",
    "    return data\n",
    "\n",
    "gt_file = '/kaggle/input/filteredgt/filtered_gt.txt'\n",
    "pred_file = '/kaggle/working/modeltrackingresults.txt'\n",
    "\n",
    "gt_data = load_tracks(gt_file)\n",
    "pred_data = load_tracks(pred_file)\n",
    "\n",
    "# Create an accumulator to store results\n",
    "acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "# Iterate through each frame\n",
    "for frame in np.unique(gt_data[:, 0]):\n",
    "    # Extract ground truth and predictions for the current frame\n",
    "    gt_objects = gt_data[gt_data[:, 0] == frame]\n",
    "    pred_objects = pred_data[pred_data[:, 0] == frame]\n",
    "\n",
    "    # Extract bounding boxes and IDs\n",
    "    gt_boxes = gt_objects[:, 2:6]  # x, y, width, height\n",
    "    gt_ids = gt_objects[:, 1]\n",
    "\n",
    "    pred_boxes = pred_objects[:, 2:6]\n",
    "    pred_ids = pred_objects[:, 1]\n",
    "\n",
    "    # Compute pairwise distances (e.g., IoU)\n",
    "    dist_matrix = mm.distances.iou_matrix(gt_boxes, pred_boxes, max_iou=0.5)\n",
    "\n",
    "    # Update the accumulator\n",
    "    acc.update(gt_ids, pred_ids, dist_matrix)\n",
    "\n",
    "# Compute metrics\n",
    "mh = mm.metrics.create()\n",
    "summary = mh.compute(acc, metrics=['mota', 'motp', 'idf1', 'precision', 'recall'], name='acc')\n",
    "\n",
    "# Print the results\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T18:12:59.856208Z",
     "iopub.status.busy": "2025-03-22T18:12:59.855874Z",
     "iopub.status.idle": "2025-03-22T18:12:59.865913Z",
     "shell.execute_reply": "2025-03-22T18:12:59.864988Z",
     "shell.execute_reply.started": "2025-03-22T18:12:59.856184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytetrack.yaml has been updated successfully with optimized settings!\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the path to your bytetrack.yaml file\n",
    "file_path = \"/kaggle/working/bytetrack.yaml\"\n",
    "\n",
    "# Load the existing YAML file\n",
    "with open(file_path, \"r\") as file:\n",
    "    bytetrack_config = yaml.safe_load(file)\n",
    "# tracking_0.6_0.1_0.3_0.85_30_0.2_True.txt\n",
    "\n",
    "# \"tracker_type\": \"bytetrack\",\n",
    "#         \"track_high_thresh\": params[0],\n",
    "#         \"track_low_thresh\": params[1],\n",
    "#         \"iou_thresh\": params[2],\n",
    "#         \"match_thresh\": params[3],\n",
    "#         \"track_buffer\": params[4],\n",
    "#         \"new_track_thresh\": params[5],\n",
    "#         \"fuse_score\": params[6],\n",
    "\n",
    "\n",
    "# Modify the values with optimized settings\n",
    "bytetrack_config[\"tracker_type\"] = \"bytetrack\"\n",
    "bytetrack_config[\"track_high_thresh\"] = 0.42  # Higher confidence filtering for stable tracks ##effect\n",
    "bytetrack_config[\"track_low_thresh\"] = 0.1  # Keep lower-confidence detections to maintain recall ##no\n",
    "bytetrack_config[\"new_track_thresh\"] = 0.2  # Prevent frequent new ID assignments #no\n",
    "bytetrack_config[\"iou_thresh\"] = 0.58  # Higher IoU helps in tracking objects with slight movement #no\n",
    "bytetrack_config[\"match_thresh\"] = 0.78  # Reduce ID switches by making matching stricter #bigeffect\n",
    "bytetrack_config[\"track_buffer\"] = 30  # Increase buffer to maintain identity over more frames #small \n",
    "bytetrack_config[\"fuse_score\"] = True  # Enable score fusion for better tracking #true better\n",
    "bytetrack_config[\"frame_rate\"] = 25  # Ensure tracking is adjusted for 25 FPS videos\n",
    "bytetrack_config[\"det_thresh\"] = 0.5  # Detection confidence threshold #no effect\n",
    "bytetrack_config[\"lost_track_thresh\"] = 10  # Use confidence fusion for better tracking #no effect\n",
    "# Save the updated YAML file\n",
    "with open(file_path, \"w\") as file:\n",
    "    yaml.dump(bytetrack_config, file, default_flow_style=False)\n",
    "\n",
    "print(\"bytetrack.yaml has been updated successfully with optimized settings!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T18:13:02.942856Z",
     "iopub.status.busy": "2025-03-22T18:13:02.942484Z",
     "iopub.status.idle": "2025-03-22T18:13:30.725698Z",
     "shell.execute_reply": "2025-03-22T18:13:30.724835Z",
     "shell.execute_reply.started": "2025-03-22T18:13:02.942824Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 2.1ms preprocess, 43.1ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 42.2ms\n",
      "Speed: 2.1ms preprocess, 42.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 30.1ms\n",
      "Speed: 2.1ms preprocess, 30.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 30.1ms\n",
      "Speed: 2.0ms preprocess, 30.1ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 30.0ms\n",
      "Speed: 1.9ms preprocess, 30.0ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 29.7ms\n",
      "Speed: 2.0ms preprocess, 29.7ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 29.7ms\n",
      "Speed: 2.1ms preprocess, 29.7ms inference, 1.8ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 28.8ms\n",
      "Speed: 2.3ms preprocess, 28.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.0ms\n",
      "Speed: 1.9ms preprocess, 27.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.3ms\n",
      "Speed: 2.0ms preprocess, 26.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.3ms\n",
      "Speed: 2.1ms preprocess, 26.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.0ms\n",
      "Speed: 2.0ms preprocess, 26.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 25.1ms\n",
      "Speed: 2.0ms preprocess, 25.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 25.2ms\n",
      "Speed: 2.0ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 25.0ms\n",
      "Speed: 2.0ms preprocess, 25.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 25.0ms\n",
      "Speed: 1.9ms preprocess, 25.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.1ms\n",
      "Speed: 1.9ms preprocess, 24.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.2ms\n",
      "Speed: 2.2ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 25.0ms\n",
      "Speed: 2.1ms preprocess, 25.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 25.0ms\n",
      "Speed: 1.7ms preprocess, 25.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 25.0ms\n",
      "Speed: 2.1ms preprocess, 25.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 24.2ms\n",
      "Speed: 2.2ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 25.0ms\n",
      "Speed: 2.4ms preprocess, 25.0ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 23.7ms\n",
      "Speed: 2.0ms preprocess, 23.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 23.7ms\n",
      "Speed: 1.9ms preprocess, 23.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 23.8ms\n",
      "Speed: 2.1ms preprocess, 23.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 23.8ms\n",
      "Speed: 2.0ms preprocess, 23.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 23.7ms\n",
      "Speed: 1.9ms preprocess, 23.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 23.8ms\n",
      "Speed: 2.3ms preprocess, 23.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 23.8ms\n",
      "Speed: 2.1ms preprocess, 23.8ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 23.7ms\n",
      "Speed: 1.8ms preprocess, 23.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 23.7ms\n",
      "Speed: 2.0ms preprocess, 23.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 23.6ms\n",
      "Speed: 2.2ms preprocess, 23.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 23.1ms\n",
      "Speed: 2.1ms preprocess, 23.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 23.1ms\n",
      "Speed: 2.2ms preprocess, 23.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 23.1ms\n",
      "Speed: 2.2ms preprocess, 23.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 22.2ms\n",
      "Speed: 2.0ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 22.2ms\n",
      "Speed: 2.7ms preprocess, 22.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 22.2ms\n",
      "Speed: 2.3ms preprocess, 22.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 22.1ms\n",
      "Speed: 1.9ms preprocess, 22.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 22.2ms\n",
      "Speed: 1.9ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 22.2ms\n",
      "Speed: 2.2ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 22.2ms\n",
      "Speed: 1.9ms preprocess, 22.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 22.2ms\n",
      "Speed: 2.1ms preprocess, 22.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 22.2ms\n",
      "Speed: 2.1ms preprocess, 22.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 22.3ms\n",
      "Speed: 2.1ms preprocess, 22.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 22.2ms\n",
      "Speed: 2.0ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 22.2ms\n",
      "Speed: 2.3ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 22.0ms\n",
      "Speed: 2.3ms preprocess, 22.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 21.8ms\n",
      "Speed: 2.2ms preprocess, 21.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 22.2ms\n",
      "Speed: 2.2ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 22.2ms\n",
      "Speed: 2.0ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 22.2ms\n",
      "Speed: 2.1ms preprocess, 22.2ms inference, 2.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 22.3ms\n",
      "Speed: 2.1ms preprocess, 22.3ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 22.1ms\n",
      "Speed: 2.0ms preprocess, 22.1ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 22.2ms\n",
      "Speed: 2.3ms preprocess, 22.2ms inference, 2.0ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 22.2ms\n",
      "Speed: 3.0ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 22.1ms\n",
      "Speed: 2.1ms preprocess, 22.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 22.1ms\n",
      "Speed: 1.8ms preprocess, 22.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 21.8ms\n",
      "Speed: 2.1ms preprocess, 21.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 22.1ms\n",
      "Speed: 1.8ms preprocess, 22.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 22.1ms\n",
      "Speed: 1.9ms preprocess, 22.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 22.2ms\n",
      "Speed: 2.1ms preprocess, 22.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 22.1ms\n",
      "Speed: 1.9ms preprocess, 22.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 21.8ms\n",
      "Speed: 2.4ms preprocess, 21.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.6ms\n",
      "Speed: 2.1ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.6ms\n",
      "Speed: 2.1ms preprocess, 21.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 21.7ms\n",
      "Speed: 2.0ms preprocess, 21.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 21.7ms\n",
      "Speed: 1.9ms preprocess, 21.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.6ms\n",
      "Speed: 1.9ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.6ms\n",
      "Speed: 1.9ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.6ms\n",
      "Speed: 2.1ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.8ms\n",
      "Speed: 1.9ms preprocess, 21.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 21.8ms\n",
      "Speed: 2.2ms preprocess, 21.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 21.6ms\n",
      "Speed: 2.0ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.7ms\n",
      "Speed: 1.9ms preprocess, 21.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 21.7ms\n",
      "Speed: 2.3ms preprocess, 21.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 21.5ms\n",
      "Speed: 2.1ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 21.5ms\n",
      "Speed: 2.0ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 21.7ms\n",
      "Speed: 1.9ms preprocess, 21.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.8ms\n",
      "Speed: 2.1ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.7ms\n",
      "Speed: 2.1ms preprocess, 21.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 21.5ms\n",
      "Speed: 2.2ms preprocess, 21.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 21.4ms\n",
      "Speed: 1.8ms preprocess, 21.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.5ms\n",
      "Speed: 1.9ms preprocess, 21.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 21.5ms\n",
      "Speed: 1.8ms preprocess, 21.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 21.6ms\n",
      "Speed: 2.3ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 21.5ms\n",
      "Speed: 2.2ms preprocess, 21.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 21.6ms\n",
      "Speed: 1.9ms preprocess, 21.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 21.5ms\n",
      "Speed: 1.9ms preprocess, 21.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 21.5ms\n",
      "Speed: 2.1ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 21.5ms\n",
      "Speed: 1.9ms preprocess, 21.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 21.4ms\n",
      "Speed: 2.2ms preprocess, 21.4ms inference, 2.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.5ms\n",
      "Speed: 2.2ms preprocess, 21.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.4ms\n",
      "Speed: 2.1ms preprocess, 21.4ms inference, 2.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 21.4ms\n",
      "Speed: 1.9ms preprocess, 21.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 21.5ms\n",
      "Speed: 1.9ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 21.7ms\n",
      "Speed: 1.9ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 21.7ms\n",
      "Speed: 2.2ms preprocess, 21.7ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 21.5ms\n",
      "Speed: 2.1ms preprocess, 21.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 21.6ms\n",
      "Speed: 1.9ms preprocess, 21.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 21.5ms\n",
      "Speed: 1.8ms preprocess, 21.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 21.5ms\n",
      "Speed: 2.0ms preprocess, 21.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.7ms\n",
      "Speed: 1.9ms preprocess, 21.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 21.8ms\n",
      "Speed: 2.2ms preprocess, 21.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 21.3ms\n",
      "Speed: 2.1ms preprocess, 21.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 21.3ms\n",
      "Speed: 2.0ms preprocess, 21.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.3ms\n",
      "Speed: 2.0ms preprocess, 21.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.4ms\n",
      "Speed: 2.0ms preprocess, 21.4ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.7ms\n",
      "Speed: 2.1ms preprocess, 21.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.4ms\n",
      "Speed: 2.0ms preprocess, 21.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 21.3ms\n",
      "Speed: 1.9ms preprocess, 21.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 21.3ms\n",
      "Speed: 1.9ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 21.3ms\n",
      "Speed: 2.1ms preprocess, 21.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.3ms\n",
      "Speed: 1.9ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 21.3ms\n",
      "Speed: 1.8ms preprocess, 21.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 21.4ms\n",
      "Speed: 2.0ms preprocess, 21.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 21.3ms\n",
      "Speed: 1.9ms preprocess, 21.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 21.2ms\n",
      "Speed: 1.8ms preprocess, 21.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 21.6ms\n",
      "Speed: 2.0ms preprocess, 21.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 21.4ms\n",
      "Speed: 2.0ms preprocess, 21.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 21.3ms\n",
      "Speed: 2.0ms preprocess, 21.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 21.6ms\n",
      "Speed: 2.1ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 21.6ms\n",
      "Speed: 1.9ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 21.6ms\n",
      "Speed: 1.9ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 21.6ms\n",
      "Speed: 2.0ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 21.7ms\n",
      "Speed: 2.3ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.6ms\n",
      "Speed: 2.1ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.9ms\n",
      "Speed: 2.1ms preprocess, 21.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.9ms\n",
      "Speed: 2.0ms preprocess, 21.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.8ms\n",
      "Speed: 2.0ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.7ms\n",
      "Speed: 2.2ms preprocess, 21.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.6ms\n",
      "Speed: 1.9ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.6ms\n",
      "Speed: 2.1ms preprocess, 21.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.7ms\n",
      "Speed: 1.8ms preprocess, 21.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.7ms\n",
      "Speed: 1.9ms preprocess, 21.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 21.6ms\n",
      "Speed: 2.0ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 21.7ms\n",
      "Speed: 2.0ms preprocess, 21.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 21.8ms\n",
      "Speed: 1.9ms preprocess, 21.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 21.6ms\n",
      "Speed: 1.9ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 21.7ms\n",
      "Speed: 1.9ms preprocess, 21.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.9ms\n",
      "Speed: 2.0ms preprocess, 21.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 20.9ms\n",
      "Speed: 2.0ms preprocess, 20.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 21.1ms\n",
      "Speed: 2.1ms preprocess, 21.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 20.6ms\n",
      "Speed: 2.1ms preprocess, 20.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 20.6ms\n",
      "Speed: 1.8ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 21.2ms\n",
      "Speed: 2.2ms preprocess, 21.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 21.1ms\n",
      "Speed: 2.0ms preprocess, 21.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 21.0ms\n",
      "Speed: 2.1ms preprocess, 21.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 20.8ms\n",
      "Speed: 2.2ms preprocess, 20.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 20.6ms\n",
      "Speed: 2.2ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 20.6ms\n",
      "Speed: 1.9ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 20.7ms\n",
      "Speed: 1.9ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 20.6ms\n",
      "Speed: 2.3ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 20.6ms\n",
      "Speed: 2.0ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 20.7ms\n",
      "Speed: 2.1ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 21.1ms\n",
      "Speed: 2.2ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 20.6ms\n",
      "Speed: 2.2ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 20.8ms\n",
      "Speed: 2.1ms preprocess, 20.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 21.1ms\n",
      "Speed: 2.0ms preprocess, 21.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 20.7ms\n",
      "Speed: 2.0ms preprocess, 20.7ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 20.9ms\n",
      "Speed: 2.1ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 21.1ms\n",
      "Speed: 2.1ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 20.8ms\n",
      "Speed: 2.0ms preprocess, 20.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 21.1ms\n",
      "Speed: 1.8ms preprocess, 21.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 20.7ms\n",
      "Speed: 1.9ms preprocess, 20.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 21.0ms\n",
      "Speed: 1.8ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 21.2ms\n",
      "Speed: 2.1ms preprocess, 21.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 20.7ms\n",
      "Speed: 2.0ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 21.2ms\n",
      "Speed: 2.2ms preprocess, 21.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 21.2ms\n",
      "Speed: 1.7ms preprocess, 21.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 23.4ms\n",
      "Speed: 1.9ms preprocess, 23.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 23.4ms\n",
      "Speed: 1.9ms preprocess, 23.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 23.3ms\n",
      "Speed: 1.9ms preprocess, 23.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 23.4ms\n",
      "Speed: 2.0ms preprocess, 23.4ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 23.5ms\n",
      "Speed: 2.2ms preprocess, 23.5ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 23.4ms\n",
      "Speed: 2.1ms preprocess, 23.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 23.3ms\n",
      "Speed: 2.0ms preprocess, 23.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 22.8ms\n",
      "Speed: 2.1ms preprocess, 22.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 22.3ms\n",
      "Speed: 1.8ms preprocess, 22.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 22.3ms\n",
      "Speed: 2.3ms preprocess, 22.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 22.8ms\n",
      "Speed: 2.1ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 22.8ms\n",
      "Speed: 2.2ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 22.7ms\n",
      "Speed: 2.0ms preprocess, 22.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 22.8ms\n",
      "Speed: 2.2ms preprocess, 22.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 22.8ms\n",
      "Speed: 1.9ms preprocess, 22.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 22.7ms\n",
      "Speed: 1.9ms preprocess, 22.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 22.3ms\n",
      "Speed: 2.2ms preprocess, 22.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 22.8ms\n",
      "Speed: 2.1ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 22.8ms\n",
      "Speed: 1.9ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 22.7ms\n",
      "Speed: 1.8ms preprocess, 22.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 22.8ms\n",
      "Speed: 2.1ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 22.8ms\n",
      "Speed: 2.0ms preprocess, 22.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 22.8ms\n",
      "Speed: 1.9ms preprocess, 22.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 22.8ms\n",
      "Speed: 2.0ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 22.8ms\n",
      "Speed: 1.9ms preprocess, 22.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 22.8ms\n",
      "Speed: 1.9ms preprocess, 22.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 22.8ms\n",
      "Speed: 1.9ms preprocess, 22.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 22.8ms\n",
      "Speed: 2.0ms preprocess, 22.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 22.8ms\n",
      "Speed: 1.7ms preprocess, 22.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 22.9ms\n",
      "Speed: 2.1ms preprocess, 22.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 22.8ms\n",
      "Speed: 2.1ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 22.6ms\n",
      "Speed: 2.1ms preprocess, 22.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 22.9ms\n",
      "Speed: 2.3ms preprocess, 22.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 22.9ms\n",
      "Speed: 2.2ms preprocess, 22.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 22.8ms\n",
      "Speed: 2.1ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 22.6ms\n",
      "Speed: 2.1ms preprocess, 22.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 22.8ms\n",
      "Speed: 2.1ms preprocess, 22.8ms inference, 1.8ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 22.9ms\n",
      "Speed: 2.4ms preprocess, 22.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 22.8ms\n",
      "Speed: 2.1ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 22.7ms\n",
      "Speed: 2.0ms preprocess, 22.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 22.3ms\n",
      "Speed: 1.9ms preprocess, 22.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 22.8ms\n",
      "Speed: 2.2ms preprocess, 22.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 22.8ms\n",
      "Speed: 1.9ms preprocess, 22.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 22.9ms\n",
      "Speed: 2.1ms preprocess, 22.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 22.8ms\n",
      "Speed: 1.9ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 22.8ms\n",
      "Speed: 2.0ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 22.8ms\n",
      "Speed: 1.9ms preprocess, 22.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 23.3ms\n",
      "Speed: 2.9ms preprocess, 23.3ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 23.9ms\n",
      "Speed: 2.0ms preprocess, 23.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 23.8ms\n",
      "Speed: 1.9ms preprocess, 23.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 23.7ms\n",
      "Speed: 1.9ms preprocess, 23.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 24.1ms\n",
      "Speed: 2.1ms preprocess, 24.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.1ms\n",
      "Speed: 1.9ms preprocess, 24.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 23.9ms\n",
      "Speed: 2.2ms preprocess, 23.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.2ms\n",
      "Speed: 2.2ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 24.2ms\n",
      "Speed: 2.1ms preprocess, 24.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.2ms\n",
      "Speed: 2.1ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.6ms\n",
      "Speed: 2.2ms preprocess, 24.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.6ms\n",
      "Speed: 2.2ms preprocess, 24.6ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 24.6ms\n",
      "Speed: 1.9ms preprocess, 24.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.6ms\n",
      "Speed: 1.9ms preprocess, 24.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 24.6ms\n",
      "Speed: 2.1ms preprocess, 24.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.0ms\n",
      "Speed: 1.9ms preprocess, 25.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.3ms\n",
      "Speed: 2.0ms preprocess, 24.3ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.0ms\n",
      "Speed: 2.2ms preprocess, 25.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.0ms\n",
      "Speed: 2.2ms preprocess, 25.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 25.0ms\n",
      "Speed: 2.0ms preprocess, 25.0ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.8ms\n",
      "Speed: 2.0ms preprocess, 24.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 25.0ms\n",
      "Speed: 1.9ms preprocess, 25.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 25.1ms\n",
      "Speed: 2.0ms preprocess, 25.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 25.0ms\n",
      "Speed: 2.1ms preprocess, 25.0ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 25.5ms\n",
      "Speed: 2.0ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 25.5ms\n",
      "Speed: 2.2ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 25.5ms\n",
      "Speed: 2.3ms preprocess, 25.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 25.6ms\n",
      "Speed: 2.0ms preprocess, 25.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 25.4ms\n",
      "Speed: 1.8ms preprocess, 25.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 25.6ms\n",
      "Speed: 2.0ms preprocess, 25.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 25.6ms\n",
      "Speed: 2.0ms preprocess, 25.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 25.3ms\n",
      "Speed: 1.8ms preprocess, 25.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 25.6ms\n",
      "Speed: 1.9ms preprocess, 25.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 25.4ms\n",
      "Speed: 2.2ms preprocess, 25.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 25.4ms\n",
      "Speed: 2.0ms preprocess, 25.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 25.6ms\n",
      "Speed: 1.9ms preprocess, 25.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 25.6ms\n",
      "Speed: 2.1ms preprocess, 25.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 25.6ms\n",
      "Speed: 2.3ms preprocess, 25.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 25.0ms\n",
      "Speed: 2.0ms preprocess, 25.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.6ms\n",
      "Speed: 2.0ms preprocess, 24.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.3ms\n",
      "Speed: 2.0ms preprocess, 24.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.1ms\n",
      "Speed: 2.0ms preprocess, 24.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.2ms\n",
      "Speed: 2.1ms preprocess, 24.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.2ms\n",
      "Speed: 2.2ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.6ms\n",
      "Speed: 2.0ms preprocess, 24.6ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.1ms\n",
      "Speed: 2.2ms preprocess, 24.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.5ms\n",
      "Speed: 1.9ms preprocess, 24.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.6ms\n",
      "Speed: 2.0ms preprocess, 24.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 24.6ms\n",
      "Speed: 2.3ms preprocess, 24.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 24.5ms\n",
      "Speed: 2.0ms preprocess, 24.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 24.4ms\n",
      "Speed: 2.3ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.0ms\n",
      "Speed: 2.0ms preprocess, 24.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.6ms\n",
      "Speed: 2.0ms preprocess, 24.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.5ms\n",
      "Speed: 1.9ms preprocess, 24.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.6ms\n",
      "Speed: 2.1ms preprocess, 24.6ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.5ms\n",
      "Speed: 2.1ms preprocess, 24.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.4ms\n",
      "Speed: 2.1ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.0ms\n",
      "Speed: 1.8ms preprocess, 24.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.5ms\n",
      "Speed: 2.1ms preprocess, 24.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.2ms\n",
      "Speed: 2.3ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 23.7ms\n",
      "Speed: 1.9ms preprocess, 23.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 23.7ms\n",
      "Speed: 1.8ms preprocess, 23.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 23.8ms\n",
      "Speed: 2.0ms preprocess, 23.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 23.5ms\n",
      "Speed: 2.3ms preprocess, 23.5ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 23.2ms\n",
      "Speed: 2.0ms preprocess, 23.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 23.0ms\n",
      "Speed: 2.3ms preprocess, 23.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 23.4ms\n",
      "Speed: 1.8ms preprocess, 23.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 23.4ms\n",
      "Speed: 2.0ms preprocess, 23.4ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 23.5ms\n",
      "Speed: 2.0ms preprocess, 23.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 23.5ms\n",
      "Speed: 2.0ms preprocess, 23.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.4ms\n",
      "Speed: 1.9ms preprocess, 23.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 23.4ms\n",
      "Speed: 2.1ms preprocess, 23.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 23.2ms\n",
      "Speed: 2.0ms preprocess, 23.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.4ms\n",
      "Speed: 1.9ms preprocess, 23.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 23.4ms\n",
      "Speed: 2.1ms preprocess, 23.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 23.4ms\n",
      "Speed: 2.0ms preprocess, 23.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 23.2ms\n",
      "Speed: 2.2ms preprocess, 23.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.5ms\n",
      "Speed: 2.0ms preprocess, 23.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 23.4ms\n",
      "Speed: 2.0ms preprocess, 23.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.1ms\n",
      "Speed: 2.4ms preprocess, 23.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.4ms\n",
      "Speed: 2.0ms preprocess, 23.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 23.4ms\n",
      "Speed: 2.1ms preprocess, 23.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 23.5ms\n",
      "Speed: 2.0ms preprocess, 23.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 23.4ms\n",
      "Speed: 2.2ms preprocess, 23.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 23.3ms\n",
      "Speed: 2.0ms preprocess, 23.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 23.0ms\n",
      "Speed: 2.1ms preprocess, 23.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.4ms\n",
      "Speed: 1.9ms preprocess, 23.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.4ms\n",
      "Speed: 1.9ms preprocess, 23.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.4ms\n",
      "Speed: 2.1ms preprocess, 23.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.4ms\n",
      "Speed: 1.9ms preprocess, 23.4ms inference, 1.8ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 23.4ms\n",
      "Speed: 2.0ms preprocess, 23.4ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.2ms\n",
      "Speed: 2.2ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.1ms\n",
      "Speed: 2.0ms preprocess, 24.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 23.5ms\n",
      "Speed: 2.1ms preprocess, 23.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.1ms\n",
      "Speed: 1.9ms preprocess, 24.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.5ms\n",
      "Speed: 1.9ms preprocess, 24.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.5ms\n",
      "Speed: 2.0ms preprocess, 24.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.0ms\n",
      "Speed: 1.9ms preprocess, 24.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.6ms\n",
      "Speed: 2.0ms preprocess, 24.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 24.5ms\n",
      "Speed: 1.9ms preprocess, 24.5ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.1ms\n",
      "Speed: 2.3ms preprocess, 24.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.5ms\n",
      "Speed: 1.9ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 24.5ms\n",
      "Speed: 1.8ms preprocess, 24.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 24.6ms\n",
      "Speed: 2.0ms preprocess, 24.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.4ms\n",
      "Speed: 1.9ms preprocess, 24.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 24.2ms\n",
      "Speed: 2.2ms preprocess, 24.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 23.9ms\n",
      "Speed: 2.0ms preprocess, 23.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 23.5ms\n",
      "Speed: 1.8ms preprocess, 23.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 23.5ms\n",
      "Speed: 2.0ms preprocess, 23.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.2ms\n",
      "Speed: 2.2ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.2ms\n",
      "Speed: 2.1ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.1ms\n",
      "Speed: 1.9ms preprocess, 24.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 23.9ms\n",
      "Speed: 2.1ms preprocess, 23.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 23.5ms\n",
      "Speed: 2.1ms preprocess, 23.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 24.2ms\n",
      "Speed: 2.1ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.1ms\n",
      "Speed: 2.3ms preprocess, 24.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 23.8ms\n",
      "Speed: 2.2ms preprocess, 23.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 23.8ms\n",
      "Speed: 2.0ms preprocess, 23.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 23.8ms\n",
      "Speed: 2.2ms preprocess, 23.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.2ms\n",
      "Speed: 2.2ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 23.7ms\n",
      "Speed: 2.0ms preprocess, 23.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.1ms\n",
      "Speed: 2.2ms preprocess, 24.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 23.7ms\n",
      "Speed: 2.3ms preprocess, 23.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.3ms\n",
      "Speed: 2.0ms preprocess, 24.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.3ms\n",
      "Speed: 2.2ms preprocess, 24.3ms inference, 2.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.2ms\n",
      "Speed: 2.2ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 23.7ms\n",
      "Speed: 2.2ms preprocess, 23.7ms inference, 2.0ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.2ms\n",
      "Speed: 2.1ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.0ms\n",
      "Speed: 1.9ms preprocess, 24.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 23.8ms\n",
      "Speed: 2.2ms preprocess, 23.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 24.2ms\n",
      "Speed: 1.8ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 24.1ms\n",
      "Speed: 2.1ms preprocess, 24.1ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.3ms\n",
      "Speed: 2.0ms preprocess, 24.3ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 24.3ms\n",
      "Speed: 2.2ms preprocess, 24.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.0ms\n",
      "Speed: 2.1ms preprocess, 24.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.0ms\n",
      "Speed: 1.9ms preprocess, 24.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 23.8ms\n",
      "Speed: 2.0ms preprocess, 23.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 23.8ms\n",
      "Speed: 2.0ms preprocess, 23.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 23.8ms\n",
      "Speed: 2.0ms preprocess, 23.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 23.8ms\n",
      "Speed: 1.9ms preprocess, 23.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 23.7ms\n",
      "Speed: 1.8ms preprocess, 23.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 23.8ms\n",
      "Speed: 1.9ms preprocess, 23.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 23.3ms\n",
      "Speed: 1.9ms preprocess, 23.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.4ms\n",
      "Speed: 2.0ms preprocess, 23.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 23.9ms\n",
      "Speed: 2.1ms preprocess, 23.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 23.8ms\n",
      "Speed: 2.0ms preprocess, 23.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 23.8ms\n",
      "Speed: 1.6ms preprocess, 23.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 23.8ms\n",
      "Speed: 1.8ms preprocess, 23.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 23.8ms\n",
      "Speed: 2.1ms preprocess, 23.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 23.9ms\n",
      "Speed: 2.1ms preprocess, 23.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 23.8ms\n",
      "Speed: 2.1ms preprocess, 23.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 23.5ms\n",
      "Speed: 1.9ms preprocess, 23.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 23.8ms\n",
      "Speed: 1.9ms preprocess, 23.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 23.7ms\n",
      "Speed: 1.9ms preprocess, 23.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.8ms\n",
      "Speed: 2.2ms preprocess, 23.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.3ms\n",
      "Speed: 2.1ms preprocess, 24.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.1ms\n",
      "Speed: 1.7ms preprocess, 24.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.0ms\n",
      "Speed: 1.9ms preprocess, 24.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 23.7ms\n",
      "Speed: 2.0ms preprocess, 23.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.3ms\n",
      "Speed: 2.1ms preprocess, 24.3ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.2ms\n",
      "Speed: 2.2ms preprocess, 24.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.3ms\n",
      "Speed: 2.1ms preprocess, 24.3ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.1ms\n",
      "Speed: 2.1ms preprocess, 24.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 23.8ms\n",
      "Speed: 2.3ms preprocess, 23.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "Tracking results saved to /kaggle/working/bytetracking_results.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 fine-tuned model\n",
    "model2 = YOLO('/kaggle/input/trackingyolo/pytorch/default/6/best.pt')\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "output_mot_path = \"/kaggle/working/bytetracking_results.txt\"\n",
    "\n",
    "# Resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704  # YOLOv8 input size\n",
    "target_height = 704\n",
    "\n",
    "# Tracking results storage\n",
    "tracking_results = []\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "    # Load and resize image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "    # Run YOLOv8 tracking with ByteTrack\n",
    "    results = model2.track(img_resized, persist=True, iou=0.58, conf=0.25,tracker=\"/kaggle/working/bytetrack.yaml\")[0]  \n",
    "\n",
    "    for track in results.boxes:\n",
    "        x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "        track_id = int(track.id[0].item()) if track.id is not None else -1\n",
    "        conf = track.conf[0].item()\n",
    "        cls = int(track.cls[0].item())\n",
    "\n",
    "        # Process only persons (class 0)\n",
    "        if cls == 0:\n",
    "            # Convert coordinates to original image size\n",
    "            x_center *= (original_width / target_width)\n",
    "            y_center *= (original_height / target_height)\n",
    "            w *= (original_width / target_width)\n",
    "            h *= (original_height / target_height)\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "            w, h = int(w), int(h)\n",
    "\n",
    "            # Save tracking result in MOT format\n",
    "            tracking_results.append([frame_number, track_id, x, y, w, h, round(conf, 4), 1, 1])\n",
    "\n",
    "            # Draw bounding boxes on image\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\"ID {track_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Sort and save results in MOT format\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for row in sorted(tracking_results, key=lambda x: (x[1], x[0])):\n",
    "        f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T18:13:30.726925Z",
     "iopub.status.busy": "2025-03-22T18:13:30.726694Z",
     "iopub.status.idle": "2025-03-22T18:13:31.484730Z",
     "shell.execute_reply": "2025-03-22T18:13:31.483731Z",
     "shell.execute_reply.started": "2025-03-22T18:13:30.726904Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        mota      motp      idf1  precision    recall\n",
      "acc  0.66989  0.327519  0.626394   0.918092  0.743629\n"
     ]
    }
   ],
   "source": [
    "import motmetrics as mm\n",
    "import numpy as np\n",
    "\n",
    "# Load ground truth and predicted tracks\n",
    "def load_tracks(file_path):\n",
    "    data = np.loadtxt(file_path, delimiter=',')\n",
    "    return data\n",
    "\n",
    "gt_file = '/kaggle/input/filteredgt/filtered_gt.txt'\n",
    "pred_file = '/kaggle/working/bytetracking_results.txt'\n",
    "\n",
    "gt_data = load_tracks(gt_file)\n",
    "pred_data = load_tracks(pred_file)\n",
    "\n",
    "# Create an accumulator to store results\n",
    "acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "# Iterate through each frame\n",
    "for frame in np.unique(gt_data[:, 0]):\n",
    "    # Extract ground truth and predictions for the current frame\n",
    "    gt_objects = gt_data[gt_data[:, 0] == frame]\n",
    "    pred_objects = pred_data[pred_data[:, 0] == frame]\n",
    "\n",
    "    # Extract bounding boxes and IDs\n",
    "    gt_boxes = gt_objects[:, 2:6]  # x, y, width, height\n",
    "    gt_ids = gt_objects[:, 1]\n",
    "\n",
    "    pred_boxes = pred_objects[:, 2:6]\n",
    "    pred_ids = pred_objects[:, 1]\n",
    "\n",
    "    # Compute pairwise distances (e.g., IoU)\n",
    "    dist_matrix = mm.distances.iou_matrix(gt_boxes, pred_boxes, max_iou=0.5)\n",
    "\n",
    "    # Update the accumulator\n",
    "    acc.update(gt_ids, pred_ids, dist_matrix)\n",
    "\n",
    "# Compute metrics\n",
    "mh = mm.metrics.create()\n",
    "summary = mh.compute(acc, metrics=['mota', 'motp', 'idf1', 'precision', 'recall'], name='acc')\n",
    "\n",
    "# Print the results\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T02:02:58.877092Z",
     "iopub.status.busy": "2025-03-17T02:02:58.876724Z",
     "iopub.status.idle": "2025-03-17T02:03:01.601622Z",
     "shell.execute_reply": "2025-03-17T02:03:01.600841Z",
     "shell.execute_reply.started": "2025-03-17T02:02:58.877063Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as: /kaggle/working/mv2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load MOT tracking results\n",
    "def load_mot_results(mot_file):\n",
    "    tracking_data = {}\n",
    "    with open(mot_file, 'r') as f:\n",
    "        for line in f:\n",
    "            frame_id, track_id, x, y, w, h, conf, *_ = map(float, line.strip().split(','))\n",
    "            if int(frame_id) not in tracking_data:\n",
    "                tracking_data[int(frame_id)] = []\n",
    "            tracking_data[int(frame_id)].append({\n",
    "                \"tracked_id\": track_id, \"x\": int(x), \"y\": int(y),\n",
    "                \"w\": int(w), \"h\": int(h), \"confidence\": conf\n",
    "            })\n",
    "    return tracking_data\n",
    "\n",
    "# Load submission template\n",
    "def load_submission_template(submission_file):\n",
    "    df = pd.read_csv(submission_file)\n",
    "    return df\n",
    "\n",
    "# Process and fill submission file\n",
    "def fill_submission_file(mot_file, submission_file, output_file):\n",
    "    tracking_data = load_mot_results(mot_file)\n",
    "    df = load_submission_template(submission_file)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i, \"objective\"] == \"tracking\":\n",
    "            frame_id = int(df.loc[i, \"frame\"])  # Get frame from submission template\n",
    "            if frame_id in tracking_data:\n",
    "                df.loc[i, \"objects\"] = str(tracking_data[frame_id])  # Fill tracking results\n",
    "        elif df.loc[i, \"objective\"] == \"face_reid\":\n",
    "            df.loc[i, \"frame\"] = -1  # Ensure face_reid has frame -1\n",
    "            df.loc[i, \"objects\"] = str(ast.literal_eval(df.loc[i, \"objects\"]))  # Keep original\n",
    "\n",
    "    # Save final submission file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Submission file saved as: {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "mot_file = \"/kaggle/working/bytetracking_results.txt\"\n",
    "submission_file = \"/kaggle/input/fawrysub1/sub1.csv\"\n",
    "output_file = \"/kaggle/working/mv2.csv\"\n",
    "\n",
    "fill_submission_file(mot_file, submission_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Botsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T13:34:02.618852Z",
     "iopub.status.busy": "2025-03-17T13:34:02.618492Z",
     "iopub.status.idle": "2025-03-17T13:34:02.624561Z",
     "shell.execute_reply": "2025-03-17T13:34:02.623830Z",
     "shell.execute_reply.started": "2025-03-17T13:34:02.618824Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML files created successfully in /kaggle/working!\n"
     ]
    }
   ],
   "source": [
    "# # BoT-SORT Configuration\n",
    "botsort_config = \"\"\"\n",
    "tracker_type: botsort\n",
    "track_high_thresh: 0.6\n",
    "track_low_thresh: 0.1\n",
    "new_track_thresh: 0.2\n",
    "iou_thresh: 0.3\n",
    "match_thresh: 0.85\n",
    "track_buffer: 30\n",
    "fuse_score: True\n",
    "frame_rate: 25\n",
    "det_thresh: 0.5\n",
    "lost_track_thresh: 50\n",
    "proximity_thresh: 0.5\n",
    "appearance_thresh: 0.25\n",
    "with_reid: False\n",
    "gmc_method: sparseOptFlow\n",
    "\"\"\"\n",
    "\n",
    "# OC-SORT Configuration\n",
    "ocsort_config = \"\"\"\n",
    "tracker_type: ocsort\n",
    "track_high_thresh: 0.6\n",
    "track_low_thresh: 0.1\n",
    "new_track_thresh: 0.2\n",
    "iou_thresh: 0.3\n",
    "match_thresh: 0.85\n",
    "track_buffer: 30\n",
    "fuse_score: True\n",
    "frame_rate: 25\n",
    "det_thresh: 0.5\n",
    "lost_track_thresh: 50\n",
    "\"\"\"\n",
    "\n",
    "# Save BoT-SORT configuration to botsort.yaml\n",
    "with open(\"/kaggle/working/botsort.yaml\", \"w\") as file:\n",
    "    file.write(botsort_config)\n",
    "\n",
    "# Save OC-SORT configuration to ocsort.yaml\n",
    "with open(\"/kaggle/working/ocsort.yaml\", \"w\") as file:\n",
    "    file.write(ocsort_config)\n",
    "\n",
    "print(\"YAML files created successfully in /kaggle/working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-17T13:33:08.649110Z",
     "iopub.status.busy": "2025-03-17T13:33:08.648833Z",
     "iopub.status.idle": "2025-03-17T13:33:43.299772Z",
     "shell.execute_reply": "2025-03-17T13:33:43.298940Z",
     "shell.execute_reply.started": "2025-03-17T13:33:08.649090Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 704x704 18 persons, 43.1ms\n",
      "Speed: 1.8ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 18 persons, 43.1ms\n",
      "Speed: 1.7ms preprocess, 43.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 19 persons, 29.5ms\n",
      "Speed: 1.9ms preprocess, 29.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 20 persons, 29.4ms\n",
      "Speed: 1.7ms preprocess, 29.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 21 persons, 29.3ms\n",
      "Speed: 1.8ms preprocess, 29.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 21 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 21 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 23 persons, 27.4ms\n",
      "Speed: 1.6ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 25 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 25 persons, 26.5ms\n",
      "Speed: 1.9ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 27 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 28 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 28 persons, 26.6ms\n",
      "Speed: 1.7ms preprocess, 26.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 29 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 31 persons, 26.5ms\n",
      "Speed: 1.9ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 31 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 31 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 30 persons, 26.5ms\n",
      "Speed: 2.1ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 30 persons, 26.5ms\n",
      "Speed: 1.5ms preprocess, 26.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 31 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 27.3ms\n",
      "Speed: 1.6ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 31 persons, 26.2ms\n",
      "Speed: 2.1ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 26.3ms\n",
      "Speed: 1.9ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 31 persons, 26.2ms\n",
      "Speed: 1.5ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 30 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 30 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 30 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 30 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 30 persons, 27.2ms\n",
      "Speed: 1.5ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 26.3ms\n",
      "Speed: 1.9ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.3ms\n",
      "Speed: 1.7ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.5ms\n",
      "Speed: 1.9ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.6ms\n",
      "Speed: 1.7ms preprocess, 26.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.3ms\n",
      "Speed: 1.6ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.3ms\n",
      "Speed: 1.7ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.3ms\n",
      "Speed: 1.9ms preprocess, 26.3ms inference, 2.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.4ms\n",
      "Speed: 2.2ms preprocess, 27.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.6ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.2ms\n",
      "Speed: 1.9ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.3ms\n",
      "Speed: 1.7ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 28.0ms\n",
      "Speed: 1.9ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.1ms\n",
      "Speed: 1.8ms preprocess, 27.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 28.6ms\n",
      "Speed: 1.8ms preprocess, 28.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.9ms\n",
      "Speed: 1.7ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.5ms\n",
      "Speed: 1.7ms preprocess, 27.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.2ms\n",
      "Speed: 1.9ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.3ms\n",
      "Speed: 1.7ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.6ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 27.3ms\n",
      "Speed: 1.6ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 26.3ms\n",
      "Speed: 1.7ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.7ms\n",
      "Speed: 1.6ms preprocess, 26.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.9ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 2.1ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.9ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.4ms\n",
      "Speed: 1.9ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.6ms\n",
      "Speed: 1.7ms preprocess, 26.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.6ms\n",
      "Speed: 1.8ms preprocess, 26.6ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.6ms\n",
      "Speed: 1.8ms preprocess, 26.6ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.5ms\n",
      "Speed: 2.0ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.9ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.3ms\n",
      "Speed: 1.9ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 2.0ms preprocess, 27.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.9ms preprocess, 26.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.5ms\n",
      "Speed: 2.0ms preprocess, 27.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.6ms\n",
      "Speed: 2.1ms preprocess, 26.6ms inference, 1.8ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.6ms\n",
      "Speed: 2.2ms preprocess, 26.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.6ms\n",
      "Speed: 2.1ms preprocess, 26.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 2.1ms preprocess, 27.4ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 2.3ms preprocess, 27.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 2.3ms preprocess, 27.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 2.0ms preprocess, 27.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.8ms\n",
      "Speed: 2.4ms preprocess, 26.8ms inference, 1.9ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.5ms\n",
      "Speed: 2.7ms preprocess, 27.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 2.0ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 28.4ms\n",
      "Speed: 2.2ms preprocess, 28.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 30.2ms\n",
      "Speed: 2.0ms preprocess, 30.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 30.1ms\n",
      "Speed: 2.1ms preprocess, 30.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.6ms\n",
      "Speed: 2.6ms preprocess, 29.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.4ms\n",
      "Speed: 2.4ms preprocess, 29.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.1ms\n",
      "Speed: 1.9ms preprocess, 29.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.1ms\n",
      "Speed: 1.8ms preprocess, 29.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.2ms\n",
      "Speed: 1.8ms preprocess, 29.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.1ms\n",
      "Speed: 1.9ms preprocess, 29.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.1ms\n",
      "Speed: 1.9ms preprocess, 29.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.1ms\n",
      "Speed: 2.1ms preprocess, 29.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.1ms\n",
      "Speed: 1.8ms preprocess, 29.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.1ms\n",
      "Speed: 1.8ms preprocess, 29.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.1ms\n",
      "Speed: 1.9ms preprocess, 29.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.1ms\n",
      "Speed: 1.8ms preprocess, 29.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.6ms\n",
      "Speed: 1.8ms preprocess, 28.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.7ms\n",
      "Speed: 1.8ms preprocess, 28.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 28.7ms\n",
      "Speed: 1.8ms preprocess, 28.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.5ms\n",
      "Speed: 1.8ms preprocess, 27.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.7ms\n",
      "Speed: 1.9ms preprocess, 28.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.5ms\n",
      "Speed: 1.8ms preprocess, 27.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.6ms\n",
      "Speed: 2.1ms preprocess, 27.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.5ms\n",
      "Speed: 1.8ms preprocess, 27.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.6ms\n",
      "Speed: 1.8ms preprocess, 27.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.6ms\n",
      "Speed: 2.0ms preprocess, 27.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.5ms\n",
      "Speed: 2.0ms preprocess, 27.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.5ms\n",
      "Speed: 1.9ms preprocess, 27.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 2.1ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.3ms\n",
      "Speed: 1.9ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 2.0ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.2ms\n",
      "Speed: 2.0ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.3ms\n",
      "Speed: 2.0ms preprocess, 26.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.3ms\n",
      "Speed: 1.9ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 1.9ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.9ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.6ms\n",
      "Speed: 1.7ms preprocess, 26.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.9ms\n",
      "Speed: 1.8ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.9ms\n",
      "Speed: 1.8ms preprocess, 26.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.8ms\n",
      "Speed: 1.8ms preprocess, 26.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 28.0ms\n",
      "Speed: 1.7ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.9ms\n",
      "Speed: 1.8ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.9ms\n",
      "Speed: 1.7ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.9ms\n",
      "Speed: 1.8ms preprocess, 26.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.9ms\n",
      "Speed: 1.7ms preprocess, 26.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.9ms\n",
      "Speed: 1.7ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.9ms\n",
      "Speed: 1.7ms preprocess, 26.9ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.8ms\n",
      "Speed: 1.7ms preprocess, 26.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.9ms\n",
      "Speed: 1.7ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.8ms\n",
      "Speed: 1.9ms preprocess, 26.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.5ms\n",
      "Speed: 1.6ms preprocess, 26.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.6ms\n",
      "Speed: 1.8ms preprocess, 26.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.4ms\n",
      "Speed: 2.0ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 26.6ms\n",
      "Speed: 1.8ms preprocess, 26.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 26.6ms\n",
      "Speed: 1.8ms preprocess, 26.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 27.6ms\n",
      "Speed: 1.8ms preprocess, 27.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 29.2ms\n",
      "Speed: 1.7ms preprocess, 29.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 29.3ms\n",
      "Speed: 1.8ms preprocess, 29.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 29.3ms\n",
      "Speed: 1.8ms preprocess, 29.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 28.9ms\n",
      "Speed: 1.7ms preprocess, 28.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 29.1ms\n",
      "Speed: 1.7ms preprocess, 29.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 29.5ms\n",
      "Speed: 1.7ms preprocess, 29.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 29.3ms\n",
      "Speed: 1.7ms preprocess, 29.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 29.1ms\n",
      "Speed: 1.7ms preprocess, 29.1ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 29.1ms\n",
      "Speed: 1.7ms preprocess, 29.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 29.0ms\n",
      "Speed: 1.9ms preprocess, 29.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 29.1ms\n",
      "Speed: 1.8ms preprocess, 29.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 29.1ms\n",
      "Speed: 1.9ms preprocess, 29.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 29.7ms\n",
      "Speed: 1.7ms preprocess, 29.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 29.7ms\n",
      "Speed: 1.7ms preprocess, 29.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 30.0ms\n",
      "Speed: 1.8ms preprocess, 30.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 30.1ms\n",
      "Speed: 1.7ms preprocess, 30.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 30.1ms\n",
      "Speed: 1.7ms preprocess, 30.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.9ms\n",
      "Speed: 1.8ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 28.0ms\n",
      "Speed: 1.8ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.9ms\n",
      "Speed: 1.8ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.9ms\n",
      "Speed: 1.8ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.9ms\n",
      "Speed: 1.9ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.6ms\n",
      "Speed: 1.7ms preprocess, 26.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.6ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.6ms\n",
      "Speed: 1.7ms preprocess, 26.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.3ms\n",
      "Speed: 1.9ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.3ms\n",
      "Speed: 1.6ms preprocess, 26.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 2.0ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 2.0ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 2.2ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.3ms\n",
      "Speed: 1.9ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 28.7ms\n",
      "Speed: 1.8ms preprocess, 28.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.5ms\n",
      "Speed: 1.9ms preprocess, 27.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 28.7ms\n",
      "Speed: 1.9ms preprocess, 28.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 28.2ms\n",
      "Speed: 1.8ms preprocess, 28.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.8ms\n",
      "Speed: 1.9ms preprocess, 27.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.9ms\n",
      "Speed: 1.7ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.9ms\n",
      "Speed: 1.8ms preprocess, 26.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.9ms\n",
      "Speed: 1.8ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.9ms\n",
      "Speed: 1.7ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.9ms\n",
      "Speed: 1.8ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.2ms\n",
      "Speed: 1.9ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.3ms\n",
      "Speed: 1.7ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.3ms\n",
      "Speed: 1.6ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "Tracking results saved to /kaggle/working/bottracking_results.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 fine-tuned model\n",
    "model2 = YOLO('/kaggle/input/trackingyolo/pytorch/default/6/best.pt')\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "output_mot_path = \"/kaggle/working/bottracking_results.txt\"\n",
    "\n",
    "# Resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704  # YOLOv8 input size\n",
    "target_height = 704\n",
    "\n",
    "# Tracking results storage\n",
    "tracking_results = []\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "    # Load and resize image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "    # Run YOLOv8 tracking with ByteTrack\n",
    "    results = model2.track(img_resized, persist=True, tracker=\"/kaggle/working/botsort.yaml\")[0]  \n",
    "\n",
    "    for track in results.boxes:\n",
    "        x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "        track_id = int(track.id[0].item()) if track.id is not None else -1\n",
    "        conf = track.conf[0].item()\n",
    "        cls = int(track.cls[0].item())\n",
    "\n",
    "        # Process only persons (class 0)\n",
    "        if cls == 0:\n",
    "            # Convert coordinates to original image size\n",
    "            x_center *= (original_width / target_width)\n",
    "            y_center *= (original_height / target_height)\n",
    "            w *= (original_width / target_width)\n",
    "            h *= (original_height / target_height)\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "            w, h = int(w), int(h)\n",
    "\n",
    "            # Save tracking result in MOT format\n",
    "            tracking_results.append([frame_number, track_id, x, y, w, h, round(conf, 4), 1, 1])\n",
    "\n",
    "            # Draw bounding boxes on image\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\"ID {track_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Sort and save results in MOT format\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for row in sorted(tracking_results, key=lambda x: (x[1], x[0])):\n",
    "        f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T13:33:43.301415Z",
     "iopub.status.busy": "2025-03-17T13:33:43.301113Z",
     "iopub.status.idle": "2025-03-17T13:33:43.944187Z",
     "shell.execute_reply": "2025-03-17T13:33:43.943369Z",
     "shell.execute_reply.started": "2025-03-17T13:33:43.301391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         mota      motp      idf1  precision    recall\n",
      "acc  0.593299  0.320313  0.576551   0.931008  0.646944\n"
     ]
    }
   ],
   "source": [
    "import motmetrics as mm\n",
    "import numpy as np\n",
    "\n",
    "# Load ground truth and predicted tracks\n",
    "def load_tracks(file_path):\n",
    "    data = np.loadtxt(file_path, delimiter=',')\n",
    "    return data\n",
    "\n",
    "gt_file = '/kaggle/input/filteredgt/filtered_gt.txt'\n",
    "pred_file = '/kaggle/working/bottracking_results.txt'\n",
    "\n",
    "gt_data = load_tracks(gt_file)\n",
    "pred_data = load_tracks(pred_file)\n",
    "\n",
    "# Create an accumulator to store results\n",
    "acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "# Iterate through each frame\n",
    "for frame in np.unique(gt_data[:, 0]):\n",
    "    # Extract ground truth and predictions for the current frame\n",
    "    gt_objects = gt_data[gt_data[:, 0] == frame]\n",
    "    pred_objects = pred_data[pred_data[:, 0] == frame]\n",
    "\n",
    "    # Extract bounding boxes and IDs\n",
    "    gt_boxes = gt_objects[:, 2:6]  # x, y, width, height\n",
    "    gt_ids = gt_objects[:, 1]\n",
    "\n",
    "    pred_boxes = pred_objects[:, 2:6]\n",
    "    pred_ids = pred_objects[:, 1]\n",
    "\n",
    "    # Compute pairwise distances (e.g., IoU)\n",
    "    dist_matrix = mm.distances.iou_matrix(gt_boxes, pred_boxes, max_iou=0.5)\n",
    "\n",
    "    # Update the accumulator\n",
    "    acc.update(gt_ids, pred_ids, dist_matrix)\n",
    "\n",
    "# Compute metrics\n",
    "mh = mm.metrics.create()\n",
    "summary = mh.compute(acc, metrics=['mota', 'motp', 'idf1', 'precision', 'recall'], name='acc')\n",
    "\n",
    "# Print the results\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-17T13:27:56.219846Z",
     "iopub.status.busy": "2025-03-17T13:27:56.219514Z",
     "iopub.status.idle": "2025-03-17T13:28:30.749156Z",
     "shell.execute_reply": "2025-03-17T13:28:30.748316Z",
     "shell.execute_reply.started": "2025-03-17T13:27:56.219817Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 704x704 18 persons, 43.0ms\n",
      "Speed: 1.8ms preprocess, 43.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 18 persons, 43.0ms\n",
      "Speed: 1.7ms preprocess, 43.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 19 persons, 28.8ms\n",
      "Speed: 2.0ms preprocess, 28.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 20 persons, 27.9ms\n",
      "Speed: 1.6ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 21 persons, 27.8ms\n",
      "Speed: 1.8ms preprocess, 27.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 21 persons, 27.8ms\n",
      "Speed: 1.8ms preprocess, 27.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 21 persons, 28.7ms\n",
      "Speed: 1.8ms preprocess, 28.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 23 persons, 27.8ms\n",
      "Speed: 1.9ms preprocess, 27.8ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 25 persons, 27.8ms\n",
      "Speed: 1.8ms preprocess, 27.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 25 persons, 27.8ms\n",
      "Speed: 1.6ms preprocess, 27.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 27 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 28 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 28 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 29 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 31 persons, 27.3ms\n",
      "Speed: 1.5ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 31 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 31 persons, 26.3ms\n",
      "Speed: 1.6ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 30 persons, 26.2ms\n",
      "Speed: 2.0ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 30 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 31 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 31 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 31 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 30 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 30 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 30 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 31 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 31 persons, 26.3ms\n",
      "Speed: 1.7ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 26.3ms\n",
      "Speed: 2.0ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.3ms\n",
      "Speed: 1.6ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.3ms\n",
      "Speed: 1.7ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.3ms\n",
      "Speed: 1.9ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.6ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.3ms\n",
      "Speed: 1.7ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.2ms\n",
      "Speed: 1.6ms preprocess, 27.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.3ms\n",
      "Speed: 1.7ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.2ms\n",
      "Speed: 1.5ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 2.2ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.3ms\n",
      "Speed: 1.9ms preprocess, 26.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.3ms\n",
      "Speed: 2.0ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 2.0ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 28.7ms\n",
      "Speed: 2.0ms preprocess, 28.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.5ms\n",
      "Speed: 2.0ms preprocess, 27.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 28.8ms\n",
      "Speed: 2.0ms preprocess, 28.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 28.1ms\n",
      "Speed: 1.9ms preprocess, 28.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.9ms\n",
      "Speed: 2.2ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 28.1ms\n",
      "Speed: 2.0ms preprocess, 28.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.2ms\n",
      "Speed: 2.0ms preprocess, 27.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.2ms\n",
      "Speed: 1.9ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.2ms\n",
      "Speed: 1.9ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 2.0ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 28.0ms\n",
      "Speed: 1.5ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 28.0ms\n",
      "Speed: 1.9ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 28.0ms\n",
      "Speed: 1.8ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 28.0ms\n",
      "Speed: 1.6ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 29.5ms\n",
      "Speed: 1.8ms preprocess, 29.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 29.4ms\n",
      "Speed: 1.8ms preprocess, 29.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 29.3ms\n",
      "Speed: 1.7ms preprocess, 29.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 26.5ms\n",
      "Speed: 1.9ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 1.9ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.4ms\n",
      "Speed: 2.0ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.4ms\n",
      "Speed: 1.9ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.5ms\n",
      "Speed: 1.9ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.6ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.4ms\n",
      "Speed: 1.7ms preprocess, 26.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.3ms\n",
      "Speed: 1.8ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.2ms\n",
      "Speed: 1.9ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.5ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 2.2ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 2.0ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.3ms\n",
      "Speed: 1.9ms preprocess, 26.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 29.1ms\n",
      "Speed: 1.9ms preprocess, 29.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 29.2ms\n",
      "Speed: 2.1ms preprocess, 29.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.9ms\n",
      "Speed: 1.8ms preprocess, 28.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.6ms\n",
      "Speed: 2.1ms preprocess, 28.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.5ms\n",
      "Speed: 1.8ms preprocess, 28.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.5ms\n",
      "Speed: 1.8ms preprocess, 29.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.3ms\n",
      "Speed: 1.9ms preprocess, 29.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.8ms\n",
      "Speed: 1.8ms preprocess, 29.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.7ms\n",
      "Speed: 2.0ms preprocess, 29.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.9ms\n",
      "Speed: 1.9ms preprocess, 29.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 30.1ms\n",
      "Speed: 1.6ms preprocess, 30.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 30.1ms\n",
      "Speed: 1.9ms preprocess, 30.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 30.1ms\n",
      "Speed: 1.8ms preprocess, 30.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 30.1ms\n",
      "Speed: 1.9ms preprocess, 30.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 30.5ms\n",
      "Speed: 2.1ms preprocess, 30.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 30.5ms\n",
      "Speed: 1.7ms preprocess, 30.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 30.5ms\n",
      "Speed: 2.0ms preprocess, 30.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.7ms\n",
      "Speed: 1.9ms preprocess, 28.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.8ms\n",
      "Speed: 1.9ms preprocess, 27.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.8ms\n",
      "Speed: 1.8ms preprocess, 27.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.8ms\n",
      "Speed: 1.5ms preprocess, 27.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.8ms\n",
      "Speed: 1.9ms preprocess, 28.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 28.0ms\n",
      "Speed: 1.7ms preprocess, 28.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 28.0ms\n",
      "Speed: 1.8ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 28.0ms\n",
      "Speed: 1.7ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.2ms\n",
      "Speed: 1.6ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.0ms\n",
      "Speed: 1.7ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.0ms\n",
      "Speed: 1.7ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.1ms\n",
      "Speed: 1.7ms preprocess, 28.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.0ms\n",
      "Speed: 1.9ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.0ms\n",
      "Speed: 1.8ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.2ms\n",
      "Speed: 1.9ms preprocess, 27.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 2.6ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 28.0ms\n",
      "Speed: 1.8ms preprocess, 28.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.6ms\n",
      "Speed: 1.7ms preprocess, 28.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 28.6ms\n",
      "Speed: 1.7ms preprocess, 28.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 29.1ms\n",
      "Speed: 1.7ms preprocess, 29.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 29.1ms\n",
      "Speed: 1.7ms preprocess, 29.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 28.5ms\n",
      "Speed: 1.7ms preprocess, 28.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.6ms preprocess, 26.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.6ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 26.5ms\n",
      "Speed: 1.9ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.9ms\n",
      "Speed: 1.7ms preprocess, 27.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.8ms\n",
      "Speed: 1.7ms preprocess, 26.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.8ms\n",
      "Speed: 1.7ms preprocess, 26.8ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.8ms\n",
      "Speed: 1.9ms preprocess, 26.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.9ms\n",
      "Speed: 1.9ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.6ms\n",
      "Speed: 1.7ms preprocess, 26.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 2.1ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.9ms\n",
      "Speed: 1.9ms preprocess, 27.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.8ms\n",
      "Speed: 1.7ms preprocess, 26.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.9ms\n",
      "Speed: 1.8ms preprocess, 26.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.8ms\n",
      "Speed: 1.7ms preprocess, 26.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.9ms\n",
      "Speed: 1.8ms preprocess, 26.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.6ms\n",
      "Speed: 1.7ms preprocess, 26.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.4ms\n",
      "Speed: 1.9ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 2.1ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.9ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.3ms\n",
      "Speed: 1.7ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.4ms\n",
      "Speed: 1.6ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.6ms\n",
      "Speed: 1.7ms preprocess, 26.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.4ms\n",
      "Speed: 1.9ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 26.5ms\n",
      "Speed: 1.6ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 27.6ms\n",
      "Speed: 1.8ms preprocess, 27.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 28.6ms\n",
      "Speed: 1.8ms preprocess, 28.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 28.6ms\n",
      "Speed: 1.7ms preprocess, 28.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 29.2ms\n",
      "Speed: 1.8ms preprocess, 29.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 29.2ms\n",
      "Speed: 1.7ms preprocess, 29.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 29.3ms\n",
      "Speed: 1.8ms preprocess, 29.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 28.5ms\n",
      "Speed: 1.7ms preprocess, 28.5ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 28.3ms\n",
      "Speed: 1.9ms preprocess, 28.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 29.3ms\n",
      "Speed: 2.0ms preprocess, 29.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 28.9ms\n",
      "Speed: 1.7ms preprocess, 28.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 29.5ms\n",
      "Speed: 1.9ms preprocess, 29.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 29.4ms\n",
      "Speed: 1.8ms preprocess, 29.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 29.3ms\n",
      "Speed: 1.8ms preprocess, 29.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.6ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.7ms\n",
      "Speed: 1.8ms preprocess, 26.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.6ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.4ms\n",
      "Speed: 2.0ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.4ms\n",
      "Speed: 1.7ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.5ms\n",
      "Speed: 1.7ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 26.6ms\n",
      "Speed: 1.6ms preprocess, 26.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 27.2ms\n",
      "Speed: 1.7ms preprocess, 27.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.2ms\n",
      "Speed: 1.7ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.3ms\n",
      "Speed: 1.7ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.3ms\n",
      "Speed: 1.6ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 27.2ms\n",
      "Speed: 1.6ms preprocess, 27.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.3ms\n",
      "Speed: 1.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 1.8ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 27.4ms\n",
      "Speed: 2.1ms preprocess, 27.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 26.5ms\n",
      "Speed: 1.8ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "Tracking results saved to /kaggle/working/ocsorttracking_results.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 fine-tuned model\n",
    "model2 = YOLO('/kaggle/input/trackingyolo/pytorch/default/6/best.pt')\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "output_mot_path = \"/kaggle/working/ocsorttracking_results.txt\"\n",
    "\n",
    "# Resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704  # YOLOv8 input size\n",
    "target_height = 704\n",
    "\n",
    "# Tracking results storage\n",
    "tracking_results = []\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "    # Load and resize image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "    # Run YOLOv8 tracking with ByteTrack\n",
    "    results = model2.track(img_resized, persist=True, tracker=\"/kaggle/working/botsort.yaml\")[0]  \n",
    "\n",
    "    for track in results.boxes:\n",
    "        x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "        track_id = int(track.id[0].item()) if track.id is not None else -1\n",
    "        conf = track.conf[0].item()\n",
    "        cls = int(track.cls[0].item())\n",
    "\n",
    "        # Process only persons (class 0)\n",
    "        if cls == 0:\n",
    "            # Convert coordinates to original image size\n",
    "            x_center *= (original_width / target_width)\n",
    "            y_center *= (original_height / target_height)\n",
    "            w *= (original_width / target_width)\n",
    "            h *= (original_height / target_height)\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "            w, h = int(w), int(h)\n",
    "\n",
    "            # Save tracking result in MOT format\n",
    "            tracking_results.append([frame_number, track_id, x, y, w, h, round(conf, 4), 1, 1])\n",
    "\n",
    "            # Draw bounding boxes on image\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\"ID {track_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Sort and save results in MOT format\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for row in sorted(tracking_results, key=lambda x: (x[1], x[0])):\n",
    "        f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T13:28:30.750748Z",
     "iopub.status.busy": "2025-03-17T13:28:30.750421Z",
     "iopub.status.idle": "2025-03-17T13:28:31.390330Z",
     "shell.execute_reply": "2025-03-17T13:28:31.389416Z",
     "shell.execute_reply.started": "2025-03-17T13:28:30.750700Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         mota      motp      idf1  precision   recall\n",
      "acc  0.593878  0.320855  0.581676   0.929228  0.64877\n"
     ]
    }
   ],
   "source": [
    "import motmetrics as mm\n",
    "import numpy as np\n",
    "\n",
    "# Load ground truth and predicted tracks\n",
    "def load_tracks(file_path):\n",
    "    data = np.loadtxt(file_path, delimiter=',')\n",
    "    return data\n",
    "\n",
    "gt_file = '/kaggle/input/filteredgt/filtered_gt.txt'\n",
    "pred_file = '/kaggle/working/ocsorttracking_results.txt'\n",
    "\n",
    "gt_data = load_tracks(gt_file)\n",
    "pred_data = load_tracks(pred_file)\n",
    "\n",
    "# Create an accumulator to store results\n",
    "acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "# Iterate through each frame\n",
    "for frame in np.unique(gt_data[:, 0]):\n",
    "    # Extract ground truth and predictions for the current frame\n",
    "    gt_objects = gt_data[gt_data[:, 0] == frame]\n",
    "    pred_objects = pred_data[pred_data[:, 0] == frame]\n",
    "\n",
    "    # Extract bounding boxes and IDs\n",
    "    gt_boxes = gt_objects[:, 2:6]  # x, y, width, height\n",
    "    gt_ids = gt_objects[:, 1]\n",
    "\n",
    "    pred_boxes = pred_objects[:, 2:6]\n",
    "    pred_ids = pred_objects[:, 1]\n",
    "\n",
    "    # Compute pairwise distances (e.g., IoU)\n",
    "    dist_matrix = mm.distances.iou_matrix(gt_boxes, pred_boxes, max_iou=0.5)\n",
    "\n",
    "    # Update the accumulator\n",
    "    acc.update(gt_ids, pred_ids, dist_matrix)\n",
    "\n",
    "# Compute metrics\n",
    "mh = mm.metrics.create()\n",
    "summary = mh.compute(acc, metrics=['mota', 'motp', 'idf1', 'precision', 'recall'], name='acc')\n",
    "\n",
    "# Print the results\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoostTrack algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-21T14:16:12.055761Z",
     "iopub.status.busy": "2025-03-21T14:16:12.055450Z",
     "iopub.status.idle": "2025-03-21T14:16:18.384926Z",
     "shell.execute_reply": "2025-03-21T14:16:18.383973Z",
     "shell.execute_reply.started": "2025-03-21T14:16:12.055737Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting filterpy\n",
      "  Downloading filterpy-1.4.5.zip (177 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from filterpy) (1.26.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from filterpy) (1.13.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from filterpy) (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->filterpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->filterpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->filterpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->filterpy) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->filterpy) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->filterpy) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->filterpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->filterpy) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->filterpy) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->filterpy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->filterpy) (2024.2.0)\n",
      "Building wheels for collected packages: filterpy\n",
      "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110458 sha256=c124f77d52eab70dfddbd9ab50da7f3abc98932cc2484a7a80e479c7866b89b3\n",
      "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
      "Successfully built filterpy\n",
      "Installing collected packages: filterpy\n",
      "Successfully installed filterpy-1.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install filterpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T14:16:25.556297Z",
     "iopub.status.busy": "2025-03-21T14:16:25.555843Z",
     "iopub.status.idle": "2025-03-21T14:16:59.851218Z",
     "shell.execute_reply": "2025-03-21T14:16:59.850411Z",
     "shell.execute_reply.started": "2025-03-21T14:16:25.556261Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 704x704 42 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 43.1ms\n",
      "Speed: 2.2ms preprocess, 43.1ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.7ms\n",
      "Speed: 2.3ms preprocess, 39.7ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.8ms\n",
      "Speed: 2.2ms preprocess, 39.8ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 38.2ms\n",
      "Speed: 2.1ms preprocess, 38.2ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 38.2ms\n",
      "Speed: 2.0ms preprocess, 38.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 38.1ms\n",
      "Speed: 2.0ms preprocess, 38.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 38.2ms\n",
      "Speed: 2.3ms preprocess, 38.2ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 36.2ms\n",
      "Speed: 2.1ms preprocess, 36.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 35.6ms\n",
      "Speed: 2.0ms preprocess, 35.6ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 34.7ms\n",
      "Speed: 2.0ms preprocess, 34.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 33.8ms\n",
      "Speed: 2.4ms preprocess, 33.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 33.7ms\n",
      "Speed: 2.0ms preprocess, 33.7ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 32.8ms\n",
      "Speed: 2.1ms preprocess, 32.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 32.8ms\n",
      "Speed: 2.0ms preprocess, 32.8ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 32.3ms\n",
      "Speed: 2.2ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 32.3ms\n",
      "Speed: 1.9ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 32.3ms\n",
      "Speed: 2.0ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 32.3ms\n",
      "Speed: 2.0ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 31.8ms\n",
      "Speed: 1.9ms preprocess, 31.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 30.5ms\n",
      "Speed: 2.2ms preprocess, 30.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 30.1ms\n",
      "Speed: 1.8ms preprocess, 30.1ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 29.9ms\n",
      "Speed: 2.0ms preprocess, 29.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 30.0ms\n",
      "Speed: 2.0ms preprocess, 30.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 29.9ms\n",
      "Speed: 2.2ms preprocess, 29.9ms inference, 1.8ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 30.0ms\n",
      "Speed: 2.2ms preprocess, 30.0ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 30.0ms\n",
      "Speed: 1.9ms preprocess, 30.0ms inference, 2.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 26.8ms\n",
      "Speed: 2.3ms preprocess, 26.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 26.1ms\n",
      "Speed: 2.1ms preprocess, 26.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 26.0ms\n",
      "Speed: 2.1ms preprocess, 26.0ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 26.1ms\n",
      "Speed: 2.2ms preprocess, 26.1ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 26.0ms\n",
      "Speed: 2.2ms preprocess, 26.0ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 26.2ms\n",
      "Speed: 2.3ms preprocess, 26.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 25.6ms\n",
      "Speed: 2.1ms preprocess, 25.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 25.6ms\n",
      "Speed: 2.0ms preprocess, 25.6ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 25.5ms\n",
      "Speed: 2.2ms preprocess, 25.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 25.5ms\n",
      "Speed: 2.1ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.5ms\n",
      "Speed: 2.2ms preprocess, 25.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 25.5ms\n",
      "Speed: 2.1ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 25.5ms\n",
      "Speed: 2.1ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 25.7ms\n",
      "Speed: 2.1ms preprocess, 25.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 25.1ms\n",
      "Speed: 2.3ms preprocess, 25.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 25.1ms\n",
      "Speed: 2.4ms preprocess, 25.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 25.0ms\n",
      "Speed: 2.2ms preprocess, 25.0ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.9ms\n",
      "Speed: 2.4ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 2.0ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.9ms\n",
      "Speed: 2.3ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.9ms\n",
      "Speed: 2.3ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 25.3ms\n",
      "Speed: 2.1ms preprocess, 25.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.7ms\n",
      "Speed: 2.1ms preprocess, 24.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 25.8ms\n",
      "Speed: 2.2ms preprocess, 25.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 25.7ms\n",
      "Speed: 2.0ms preprocess, 25.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 25.2ms\n",
      "Speed: 2.2ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.7ms\n",
      "Speed: 1.9ms preprocess, 25.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 25.2ms\n",
      "Speed: 2.0ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 25.0ms\n",
      "Speed: 2.5ms preprocess, 25.0ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 25.0ms\n",
      "Speed: 2.1ms preprocess, 25.0ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.9ms\n",
      "Speed: 2.3ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 2.3ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 24.9ms\n",
      "Speed: 2.3ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.9ms\n",
      "Speed: 2.3ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 25.1ms\n",
      "Speed: 2.2ms preprocess, 25.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 25.2ms\n",
      "Speed: 2.1ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 25.9ms\n",
      "Speed: 2.4ms preprocess, 25.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 25.2ms\n",
      "Speed: 2.1ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 25.1ms\n",
      "Speed: 2.0ms preprocess, 25.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 25.2ms\n",
      "Speed: 2.1ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 25.2ms\n",
      "Speed: 1.9ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 25.0ms\n",
      "Speed: 1.8ms preprocess, 25.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.2ms\n",
      "Speed: 1.9ms preprocess, 24.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.3ms\n",
      "Speed: 2.3ms preprocess, 24.3ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 24.2ms\n",
      "Speed: 2.1ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.4ms\n",
      "Speed: 2.0ms preprocess, 24.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 24.3ms\n",
      "Speed: 1.8ms preprocess, 24.3ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.4ms\n",
      "Speed: 2.1ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 23.7ms\n",
      "Speed: 1.9ms preprocess, 23.7ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 23.7ms\n",
      "Speed: 2.4ms preprocess, 23.7ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.4ms\n",
      "Speed: 2.1ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.1ms\n",
      "Speed: 2.1ms preprocess, 24.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 23.7ms\n",
      "Speed: 2.1ms preprocess, 23.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.7ms\n",
      "Speed: 1.8ms preprocess, 23.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 23.7ms\n",
      "Speed: 2.3ms preprocess, 23.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 23.6ms\n",
      "Speed: 2.0ms preprocess, 23.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.7ms\n",
      "Speed: 2.2ms preprocess, 23.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 23.7ms\n",
      "Speed: 2.0ms preprocess, 23.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 23.7ms\n",
      "Speed: 2.1ms preprocess, 23.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 23.7ms\n",
      "Speed: 2.0ms preprocess, 23.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 23.7ms\n",
      "Speed: 2.0ms preprocess, 23.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.7ms\n",
      "Speed: 2.1ms preprocess, 23.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 23.6ms\n",
      "Speed: 2.0ms preprocess, 23.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 23.7ms\n",
      "Speed: 2.2ms preprocess, 23.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.7ms\n",
      "Speed: 2.2ms preprocess, 23.7ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 23.7ms\n",
      "Speed: 2.2ms preprocess, 23.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 23.7ms\n",
      "Speed: 2.1ms preprocess, 23.7ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 23.7ms\n",
      "Speed: 2.1ms preprocess, 23.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 23.8ms\n",
      "Speed: 2.2ms preprocess, 23.8ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.5ms\n",
      "Speed: 2.3ms preprocess, 24.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 23.8ms\n",
      "Speed: 2.2ms preprocess, 23.8ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.1ms\n",
      "Speed: 2.2ms preprocess, 24.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 25.3ms\n",
      "Speed: 2.4ms preprocess, 25.3ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.7ms\n",
      "Speed: 2.2ms preprocess, 24.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 25.6ms\n",
      "Speed: 2.4ms preprocess, 25.6ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 25.7ms\n",
      "Speed: 2.0ms preprocess, 25.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 25.0ms\n",
      "Speed: 2.2ms preprocess, 25.0ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 25.0ms\n",
      "Speed: 2.2ms preprocess, 25.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.9ms\n",
      "Speed: 2.3ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 26.2ms\n",
      "Speed: 2.1ms preprocess, 26.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 25.8ms\n",
      "Speed: 2.1ms preprocess, 25.8ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 25.5ms\n",
      "Speed: 2.0ms preprocess, 25.5ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 25.8ms\n",
      "Speed: 2.0ms preprocess, 25.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 25.2ms\n",
      "Speed: 2.2ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 25.2ms\n",
      "Speed: 1.9ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 25.2ms\n",
      "Speed: 1.9ms preprocess, 25.2ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.2ms\n",
      "Speed: 2.0ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.1ms\n",
      "Speed: 2.0ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 25.4ms\n",
      "Speed: 2.4ms preprocess, 25.4ms inference, 2.0ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.2ms\n",
      "Speed: 2.2ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 25.2ms\n",
      "Speed: 2.1ms preprocess, 25.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.2ms\n",
      "Speed: 2.0ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 25.2ms\n",
      "Speed: 1.9ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.2ms\n",
      "Speed: 1.9ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 25.2ms\n",
      "Speed: 2.1ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 25.2ms\n",
      "Speed: 2.2ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 25.2ms\n",
      "Speed: 1.9ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.2ms\n",
      "Speed: 2.0ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.2ms\n",
      "Speed: 2.0ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 25.2ms\n",
      "Speed: 2.3ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 26.5ms\n",
      "Speed: 1.9ms preprocess, 26.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.2ms\n",
      "Speed: 1.9ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 25.2ms\n",
      "Speed: 2.0ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 25.2ms\n",
      "Speed: 2.1ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 25.7ms\n",
      "Speed: 1.9ms preprocess, 25.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 25.1ms\n",
      "Speed: 2.0ms preprocess, 25.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 25.0ms\n",
      "Speed: 2.2ms preprocess, 25.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 25.0ms\n",
      "Speed: 1.9ms preprocess, 25.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.8ms\n",
      "Speed: 1.9ms preprocess, 24.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.0ms\n",
      "Speed: 1.9ms preprocess, 25.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 2.3ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 25.3ms\n",
      "Speed: 2.0ms preprocess, 25.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.6ms\n",
      "Speed: 1.8ms preprocess, 24.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 25.4ms\n",
      "Speed: 1.9ms preprocess, 25.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 25.8ms\n",
      "Speed: 2.0ms preprocess, 25.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 25.2ms\n",
      "Speed: 2.2ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 25.2ms\n",
      "Speed: 2.2ms preprocess, 25.2ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 25.4ms\n",
      "Speed: 2.2ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 26.8ms\n",
      "Speed: 2.1ms preprocess, 26.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 25.8ms\n",
      "Speed: 2.3ms preprocess, 25.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 55 persons, 27.4ms\n",
      "Speed: 2.4ms preprocess, 27.4ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 58 persons, 26.4ms\n",
      "Speed: 2.1ms preprocess, 26.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 26.8ms\n",
      "Speed: 2.0ms preprocess, 26.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 59 persons, 27.4ms\n",
      "Speed: 1.9ms preprocess, 27.4ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 60 persons, 26.6ms\n",
      "Speed: 2.1ms preprocess, 26.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 26.6ms\n",
      "Speed: 2.1ms preprocess, 26.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 57 persons, 26.6ms\n",
      "Speed: 2.1ms preprocess, 26.6ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 28.1ms\n",
      "Speed: 1.9ms preprocess, 28.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 28.7ms\n",
      "Speed: 1.8ms preprocess, 28.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 28.2ms\n",
      "Speed: 2.4ms preprocess, 28.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 27.5ms\n",
      "Speed: 2.0ms preprocess, 27.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 26.2ms\n",
      "Speed: 2.1ms preprocess, 26.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 25.5ms\n",
      "Speed: 1.8ms preprocess, 25.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 58 persons, 25.6ms\n",
      "Speed: 2.0ms preprocess, 25.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 61 persons, 25.7ms\n",
      "Speed: 2.0ms preprocess, 25.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 61 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 24.9ms\n",
      "Speed: 2.3ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 57 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 58 persons, 25.0ms\n",
      "Speed: 2.0ms preprocess, 25.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 62 persons, 24.9ms\n",
      "Speed: 2.3ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 57 persons, 25.0ms\n",
      "Speed: 2.1ms preprocess, 25.0ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 55 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 25.0ms\n",
      "Speed: 2.3ms preprocess, 25.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 55 persons, 24.8ms\n",
      "Speed: 1.9ms preprocess, 24.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 55 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 58 persons, 24.9ms\n",
      "Speed: 2.3ms preprocess, 24.9ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 57 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 24.9ms\n",
      "Speed: 2.4ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 25.0ms\n",
      "Speed: 2.3ms preprocess, 25.0ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 55 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.7ms\n",
      "Speed: 1.9ms preprocess, 25.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 25.1ms\n",
      "Speed: 2.2ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 2.3ms preprocess, 24.9ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.3ms\n",
      "Speed: 2.0ms preprocess, 25.3ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.8ms\n",
      "Speed: 1.9ms preprocess, 24.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.7ms\n",
      "Speed: 1.9ms preprocess, 24.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 24.7ms\n",
      "Speed: 2.0ms preprocess, 24.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 24.7ms\n",
      "Speed: 2.1ms preprocess, 24.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.1ms\n",
      "Speed: 2.0ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 25.8ms\n",
      "Speed: 2.0ms preprocess, 25.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 55 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 25.8ms\n",
      "Speed: 2.0ms preprocess, 25.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 25.6ms\n",
      "Speed: 2.4ms preprocess, 25.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 25.2ms\n",
      "Speed: 2.0ms preprocess, 25.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.8ms\n",
      "Speed: 2.2ms preprocess, 25.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 24.9ms\n",
      "Speed: 1.7ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.9ms\n",
      "Speed: 1.7ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 24.9ms\n",
      "Speed: 1.7ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 25.0ms\n",
      "Speed: 2.0ms preprocess, 25.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.0ms\n",
      "Speed: 2.5ms preprocess, 25.0ms inference, 1.9ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.0ms\n",
      "Speed: 2.4ms preprocess, 25.0ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.8ms\n",
      "Speed: 2.1ms preprocess, 25.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 25.8ms\n",
      "Speed: 2.0ms preprocess, 25.8ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.2ms\n",
      "Speed: 2.1ms preprocess, 25.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 25.7ms\n",
      "Speed: 2.1ms preprocess, 25.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 24.9ms\n",
      "Speed: 2.3ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 26.2ms\n",
      "Speed: 1.9ms preprocess, 26.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 25.6ms\n",
      "Speed: 1.9ms preprocess, 25.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 55 persons, 25.5ms\n",
      "Speed: 2.0ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 25.5ms\n",
      "Speed: 1.7ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 26.7ms\n",
      "Speed: 2.0ms preprocess, 26.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 26.0ms\n",
      "Speed: 1.8ms preprocess, 26.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 27.4ms\n",
      "Speed: 1.9ms preprocess, 27.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 27.0ms\n",
      "Speed: 2.0ms preprocess, 27.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 26.6ms\n",
      "Speed: 2.0ms preprocess, 26.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 26.2ms\n",
      "Speed: 1.8ms preprocess, 26.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 25.5ms\n",
      "Speed: 2.0ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.5ms\n",
      "Speed: 1.8ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.6ms\n",
      "Speed: 2.3ms preprocess, 25.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 25.5ms\n",
      "Speed: 2.0ms preprocess, 25.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 25.6ms\n",
      "Speed: 2.3ms preprocess, 25.6ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 25.5ms\n",
      "Speed: 2.3ms preprocess, 25.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 25.6ms\n",
      "Speed: 2.3ms preprocess, 25.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 25.5ms\n",
      "Speed: 1.8ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.5ms\n",
      "Speed: 2.0ms preprocess, 25.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.5ms\n",
      "Speed: 2.1ms preprocess, 25.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 25.7ms\n",
      "Speed: 2.1ms preprocess, 25.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 24.9ms\n",
      "Speed: 1.6ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 24.8ms\n",
      "Speed: 1.8ms preprocess, 24.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.8ms\n",
      "Speed: 1.8ms preprocess, 24.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.8ms\n",
      "Speed: 1.8ms preprocess, 24.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 24.8ms\n",
      "Speed: 1.9ms preprocess, 24.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 24.9ms\n",
      "Speed: 1.7ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.8ms\n",
      "Speed: 1.9ms preprocess, 24.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.8ms\n",
      "Speed: 1.9ms preprocess, 24.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 24.8ms\n",
      "Speed: 2.0ms preprocess, 24.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 2.1ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 1.8ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 25.0ms\n",
      "Speed: 1.8ms preprocess, 25.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 2.3ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 24.9ms\n",
      "Speed: 2.0ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 24.9ms\n",
      "Speed: 1.9ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 25.4ms\n",
      "Speed: 1.7ms preprocess, 25.4ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.7ms\n",
      "Speed: 1.8ms preprocess, 25.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.1ms\n",
      "Speed: 1.8ms preprocess, 25.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 25.2ms\n",
      "Speed: 2.1ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 25.2ms\n",
      "Speed: 2.1ms preprocess, 25.2ms inference, 1.9ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 25.2ms\n",
      "Speed: 2.4ms preprocess, 25.2ms inference, 2.0ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.8ms\n",
      "Speed: 2.3ms preprocess, 25.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.0ms\n",
      "Speed: 2.3ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 26.2ms\n",
      "Speed: 2.4ms preprocess, 26.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.6ms\n",
      "Speed: 2.0ms preprocess, 25.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.6ms\n",
      "Speed: 2.0ms preprocess, 25.6ms inference, 2.0ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 25.8ms\n",
      "Speed: 2.3ms preprocess, 25.8ms inference, 2.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.2ms\n",
      "Speed: 1.9ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 25.2ms\n",
      "Speed: 2.1ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 25.2ms\n",
      "Speed: 1.9ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 25.2ms\n",
      "Speed: 1.8ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.2ms\n",
      "Speed: 1.8ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.1ms\n",
      "Speed: 2.1ms preprocess, 25.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 25.2ms\n",
      "Speed: 1.9ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.2ms\n",
      "Speed: 2.1ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 25.1ms\n",
      "Speed: 1.7ms preprocess, 25.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 25.2ms\n",
      "Speed: 2.2ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 25.2ms\n",
      "Speed: 2.2ms preprocess, 25.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 25.2ms\n",
      "Speed: 2.0ms preprocess, 25.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 55 persons, 25.2ms\n",
      "Speed: 2.2ms preprocess, 25.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 25.2ms\n",
      "Speed: 2.0ms preprocess, 25.2ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 25.3ms\n",
      "Speed: 2.7ms preprocess, 25.3ms inference, 2.0ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 25.2ms\n",
      "Speed: 2.1ms preprocess, 25.2ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 25.2ms\n",
      "Speed: 1.9ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 55 persons, 25.1ms\n",
      "Speed: 2.0ms preprocess, 25.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 25.3ms\n",
      "Speed: 1.9ms preprocess, 25.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 55 persons, 25.1ms\n",
      "Speed: 2.0ms preprocess, 25.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 55 persons, 25.1ms\n",
      "Speed: 1.9ms preprocess, 25.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 25.2ms\n",
      "Speed: 2.1ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 26.2ms\n",
      "Speed: 2.0ms preprocess, 26.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 25.6ms\n",
      "Speed: 2.2ms preprocess, 25.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 25.6ms\n",
      "Speed: 2.0ms preprocess, 25.6ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 57 persons, 25.5ms\n",
      "Speed: 2.5ms preprocess, 25.5ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 25.6ms\n",
      "Speed: 2.2ms preprocess, 25.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 26.9ms\n",
      "Speed: 2.4ms preprocess, 26.9ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 53 persons, 26.4ms\n",
      "Speed: 1.9ms preprocess, 26.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 26.0ms\n",
      "Speed: 2.3ms preprocess, 26.0ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 26.2ms\n",
      "Speed: 2.3ms preprocess, 26.2ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 25.5ms\n",
      "Speed: 2.3ms preprocess, 25.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 25.5ms\n",
      "Speed: 2.0ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 25.6ms\n",
      "Speed: 2.3ms preprocess, 25.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 25.6ms\n",
      "Speed: 2.3ms preprocess, 25.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 25.5ms\n",
      "Speed: 2.3ms preprocess, 25.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 56 persons, 25.6ms\n",
      "Speed: 2.0ms preprocess, 25.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 52 persons, 25.5ms\n",
      "Speed: 2.1ms preprocess, 25.5ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 54 persons, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 25.5ms\n",
      "Speed: 2.1ms preprocess, 25.5ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 25.6ms\n",
      "Speed: 2.3ms preprocess, 25.6ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 25.6ms\n",
      "Speed: 2.6ms preprocess, 25.6ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "Tracking results saved to /kaggle/working/boosttrackingresults.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Load the YOLO model\n",
    "model2 = YOLO('/kaggle/input/trackingyolo/pytorch/default/6/best.pt')\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "gt_file_path = \"/kaggle/input/testgt/gt.txt\"\n",
    "output_mot_path = \"/kaggle/working/modeltrackingresults.txt\"\n",
    "\n",
    "# Resize parameters\n",
    "original_width = 1920\n",
    "original_height = 1080\n",
    "target_width = 704  # YOLOv8 input size\n",
    "target_height = 704\n",
    "\n",
    "# Tracking results storage\n",
    "tracking_results = []\n",
    "previous_tracks = {}\n",
    "next_track_id = 0\n",
    "\n",
    "# IoU Calculation\n",
    "def iou(box1, box2):\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "    xi1 = max(x1, x2)\n",
    "    yi1 = max(y1, y2)\n",
    "    xi2 = min(x1 + w1, x2 + w2)\n",
    "    yi2 = min(y1 + h1, y2 + h2)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "    return inter_area / (box1_area + box2_area - inter_area)\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])  # Extract frame number\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "    # Load and resize image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "    # Run YOLOv8 detection (without built-in tracking)\n",
    "    results = model2(img_resized)[0]\n",
    "\n",
    "    detections = []\n",
    "    for track in results.boxes:\n",
    "        x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "        conf = track.conf[0].item()\n",
    "        cls = int(track.cls[0].item())\n",
    "        if cls == 0:\n",
    "            x_center *= (original_width / target_width)\n",
    "            y_center *= (original_height / target_height)\n",
    "            w *= (original_width / target_width)\n",
    "            h *= (original_height / target_height)\n",
    "            x = int(x_center - w / 2)\n",
    "            y = int(y_center - h / 2)\n",
    "            detections.append([x, y, w, h, conf])\n",
    "\n",
    "    # Match detections with previous tracks using Hungarian Algorithm\n",
    "    if previous_tracks:\n",
    "        track_ids = list(previous_tracks.keys())\n",
    "        track_boxes = np.array([previous_tracks[t] for t in track_ids])\n",
    "        detection_boxes = np.array([d[:4] for d in detections])\n",
    "        cost_matrix = np.zeros((len(track_boxes), len(detection_boxes)))\n",
    "        for i, tbox in enumerate(track_boxes):\n",
    "            for j, dbox in enumerate(detection_boxes):\n",
    "                cost_matrix[i, j] = 1 - iou(tbox, dbox)\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        assigned_tracks = {}\n",
    "        assigned_detections = set()\n",
    "        for r, c in zip(row_ind, col_ind):\n",
    "            if cost_matrix[r, c] < 0.5:  # IoU threshold\n",
    "                assigned_tracks[track_ids[r]] = detections[c][:4]\n",
    "                assigned_detections.add(c)\n",
    "\n",
    "        # Assign new IDs to unassigned detections\n",
    "        for i, det in enumerate(detections):\n",
    "            if i not in assigned_detections:\n",
    "                assigned_tracks[next_track_id] = det[:4]\n",
    "                next_track_id += 1\n",
    "\n",
    "        previous_tracks = assigned_tracks\n",
    "    else:\n",
    "        for det in detections:\n",
    "            previous_tracks[next_track_id] = det[:4]\n",
    "            next_track_id += 1\n",
    "\n",
    "    # Store tracking results\n",
    "    for track_id, box in previous_tracks.items():\n",
    "        x, y, w, h = box\n",
    "        conf = next((d[4] for d in detections if d[:4] == box), 1.0)  # Get confidence from detections\n",
    "        tracking_results.append([frame_number, track_id, x, y, w, h, round(conf, 4), 1, 1])\n",
    "\n",
    "# Sort tracking results by track_id first, then by frame number\n",
    "track_id_data = {}\n",
    "for row in tracking_results:\n",
    "    frame_number, track_id = row[0], row[1]\n",
    "    if track_id not in track_id_data:\n",
    "        track_id_data[track_id] = []\n",
    "    track_id_data[track_id].append(row)\n",
    "\n",
    "# Save results in MOT format\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for track_id in sorted(track_id_data.keys()):  # Sort by track_id first\n",
    "        for row in sorted(track_id_data[track_id], key=lambda x: x[0]):  # Sort frames within each ID\n",
    "            f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T19:08:50.819230Z",
     "iopub.status.busy": "2025-03-21T19:08:50.818851Z",
     "iopub.status.idle": "2025-03-21T19:08:51.580862Z",
     "shell.execute_reply": "2025-03-21T19:08:51.580082Z",
     "shell.execute_reply.started": "2025-03-21T19:08:50.819192Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         mota      motp      idf1  precision    recall\n",
      "acc  0.556363  0.330231  0.606879   0.776893  0.790946\n"
     ]
    }
   ],
   "source": [
    "import motmetrics as mm\n",
    "import numpy as np\n",
    "\n",
    "# Load ground truth and predicted tracks\n",
    "def load_tracks(file_path):\n",
    "    data = np.loadtxt(file_path, delimiter=',')\n",
    "    return data\n",
    "\n",
    "gt_file = '/kaggle/input/filteredgt/filtered_gt.txt'\n",
    "pred_file = '/kaggle/input/gttrials/tracking_results (3).txt'\n",
    "\n",
    "gt_data = load_tracks(gt_file)\n",
    "pred_data = load_tracks(pred_file)\n",
    "\n",
    "# Create an accumulator to store results\n",
    "acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "# Iterate through each frame\n",
    "for frame in np.unique(gt_data[:, 0]):\n",
    "    # Extract ground truth and predictions for the current frame\n",
    "    gt_objects = gt_data[gt_data[:, 0] == frame]\n",
    "    pred_objects = pred_data[pred_data[:, 0] == frame]\n",
    "\n",
    "    # Extract bounding boxes and IDs\n",
    "    gt_boxes = gt_objects[:, 2:6]  # x, y, width, height\n",
    "    gt_ids = gt_objects[:, 1]\n",
    "\n",
    "    pred_boxes = pred_objects[:, 2:6]\n",
    "    pred_ids = pred_objects[:, 1]\n",
    "\n",
    "    # Compute pairwise distances (e.g., IoU)\n",
    "    dist_matrix = mm.distances.iou_matrix(gt_boxes, pred_boxes, max_iou=0.5)\n",
    "\n",
    "    # Update the accumulator\n",
    "    acc.update(gt_ids, pred_ids, dist_matrix)\n",
    "\n",
    "# Compute metrics\n",
    "mh = mm.metrics.create()\n",
    "summary = mh.compute(acc, metrics=['mota', 'motp', 'idf1', 'precision', 'recall'], name='acc')\n",
    "\n",
    "# Print the results\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T14:25:13.625425Z",
     "iopub.status.busy": "2025-03-21T14:25:13.625067Z",
     "iopub.status.idle": "2025-03-21T14:25:23.040518Z",
     "shell.execute_reply": "2025-03-21T14:25:23.039282Z",
     "shell.execute_reply.started": "2025-03-21T14:25:13.625399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'BoostTrack'...\n",
      "remote: Enumerating objects: 1643, done.\u001b[K\n",
      "remote: Counting objects: 100% (192/192), done.\u001b[K\n",
      "remote: Compressing objects: 100% (133/133), done.\u001b[K\n",
      "remote: Total 1643 (delta 90), reused 59 (delta 59), pack-reused 1451 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1643/1643), 129.81 MiB | 24.74 MiB/s, done.\n",
      "Resolving deltas: 100% (339/339), done.\n",
      "/kaggle/working/BoostTrack\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/vukasin-stanojevic/BoostTrack.git\n",
    "%cd BoostTrack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T13:34:38.852536Z",
     "iopub.status.busy": "2025-03-24T13:34:38.852243Z",
     "iopub.status.idle": "2025-03-24T13:35:01.724410Z",
     "shell.execute_reply": "2025-03-24T13:35:01.723535Z",
     "shell.execute_reply.started": "2025-03-24T13:34:38.852514Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yolox\n",
      "  Downloading yolox-0.3.0.tar.gz (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.0/80.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from yolox) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from yolox) (2.5.1+cu121)\n",
      "Requirement already satisfied: opencv_python in /usr/local/lib/python3.10/dist-packages (from yolox) (4.10.0.84)\n",
      "Collecting loguru (from yolox)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from yolox) (0.25.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from yolox) (4.67.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from yolox) (0.20.1+cu121)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from yolox) (11.0.0)\n",
      "Collecting thop (from yolox)\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from yolox) (1.11.1.3)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yolox) (0.9.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from yolox) (2.17.1)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from yolox) (2.0.8)\n",
      "Collecting onnx==1.8.1 (from yolox)\n",
      "  Downloading onnx-1.8.1.tar.gz (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "INFO: pip is looking at multiple versions of yolox to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting yolox\n",
      "  Downloading yolox-0.2.0.tar.gz (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install yolox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T13:37:15.677141Z",
     "iopub.status.busy": "2025-03-24T13:37:15.676836Z",
     "iopub.status.idle": "2025-03-24T13:37:42.760887Z",
     "shell.execute_reply": "2025-03-24T13:37:42.759654Z",
     "shell.execute_reply.started": "2025-03-24T13:37:15.677118Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Megvii-BaseDetection/YOLOX.git\n",
      "  Cloning https://github.com/Megvii-BaseDetection/YOLOX.git to /tmp/pip-req-build-tz8qw2wp\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Megvii-BaseDetection/YOLOX.git /tmp/pip-req-build-tz8qw2wp\n",
      "  Resolved https://github.com/Megvii-BaseDetection/YOLOX.git to commit d872c71bf63e1906ef7b7bb5a9d7a529c7a59e6a\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: opencv_python in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (4.10.0.84)\n",
      "Collecting loguru (from yolox==0.3.0)\n",
      "  Using cached loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (4.67.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (0.20.1+cu121)\n",
      "Collecting thop (from yolox==0.3.0)\n",
      "  Using cached thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (1.11.1.3)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (0.9.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (5.9.5)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (2.17.1)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (2.0.8)\n",
      "Requirement already satisfied: onnx>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (1.17.0)\n",
      "Collecting onnx-simplifier==0.4.10 (from yolox==0.3.0)\n",
      "  Downloading onnx_simplifier-0.4.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from onnx-simplifier==0.4.10->yolox==0.3.0) (13.9.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.13.0->yolox==0.3.0) (3.20.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (2.4.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->yolox==0.3.0) (3.7.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7->yolox==0.3.0) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (1.68.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (3.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (24.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (3.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->yolox==0.3.0) (11.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (2.9.0.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->yolox==0.3.0) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->yolox==0.3.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->yolox==0.3.0) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->yolox==0.3.0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->yolox==0.3.0) (2024.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnx-simplifier==0.4.10->yolox==0.3.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnx-simplifier==0.4.10->yolox==0.3.0) (2.19.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->yolox==0.3.0) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->onnx-simplifier==0.4.10->yolox==0.3.0) (0.1.2)\n",
      "Downloading onnx_simplifier-0.4.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Building wheels for collected packages: yolox\n",
      "  Building wheel for yolox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for yolox: filename=yolox-0.3.0-cp310-cp310-linux_x86_64.whl size=1538296 sha256=318cf9dd73f21a6d83a3686abdab66783bb0bfb6f5765e1f226b8e2e2f6aefac\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8cfj3_d6/wheels/34/59/cd/6e6affe0835b4ebf5b53f47f913fcff04e2a03042616955c9e\n",
      "Successfully built yolox\n",
      "Installing collected packages: loguru, thop, onnx-simplifier, yolox\n",
      "Successfully installed loguru-0.7.3 onnx-simplifier-0.4.10 thop-0.1.1.post2209072238 yolox-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U git+https://github.com/Megvii-BaseDetection/YOLOX.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T13:50:34.751168Z",
     "iopub.status.busy": "2025-03-24T13:50:34.750840Z",
     "iopub.status.idle": "2025-03-24T13:50:34.875792Z",
     "shell.execute_reply": "2025-03-24T13:50:34.874045Z",
     "shell.execute_reply.started": "2025-03-24T13:50:34.751146Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yolox.tracker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ae3df608fc95>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myolox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_augment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreproc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myolox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfuse_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mYOLOX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_tracker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBYTETracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myolox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/YOLOX/yolox/tracker/byte_tracker.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mkalman_filter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKalmanFilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myolox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbasetrack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseTrack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrackState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yolox.tracker'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from yolox.exp import get_exp\n",
    "from yolox.data.data_augment import preproc\n",
    "from yolox.utils import fuse_model, postprocess\n",
    "from YOLOX.yolox.tracker.byte_tracker import BYTETracker\n",
    "from yolox.utils.visualize import vis\n",
    "\n",
    "# Load YOLOX model\n",
    "exp = get_exp(None, \"yolox_s\")  # Change \"yolox_s\" to your model size\n",
    "model = exp.get_model()\n",
    "ckpt = torch.load(\"/kaggle/input/yoloxs/yolox_s.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "fuse_model(model)\n",
    "tracker = BYTETracker()\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"/kaggle/input/tracking/tracking/tracking/test/01/img1\"\n",
    "output_mot_path = \"/kaggle/working/modeltrackingresults.txt\"\n",
    "\n",
    "# Tracking results storage\n",
    "tracking_results = []\n",
    "\n",
    "# Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "    \n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img, ratio = preproc(img, exp.test_size, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    img_tensor = torch.from_numpy(img).unsqueeze(0).float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "        detections = postprocess(outputs[0], num_classes=1, conf_thre=0.25, nms_thre=0.45)\n",
    "    \n",
    "    if detections is not None:\n",
    "        tracked_objects = tracker.update(detections.cpu().numpy(), exp.test_size, ratio)\n",
    "        \n",
    "        for track in tracked_objects:\n",
    "            track_id = int(track.track_id)\n",
    "            x, y, w, h = map(int, track.tlwh)\n",
    "            conf = round(track.score, 4)\n",
    "            \n",
    "            tracking_results.append([frame_number, track_id, x, y, w, h, conf, 1, 1])\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\"ID {track_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Sort tracking results: first by Frame Number, then by Track ID\n",
    "tracking_results.sort(key=lambda x: (x[1], x[0]))\n",
    "\n",
    "# Save results in MOT format\n",
    "with open(output_mot_path, \"w\") as f:\n",
    "    for row in tracking_results:\n",
    "        f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"Tracking results saved to {output_mot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T13:40:34.429702Z",
     "iopub.status.busy": "2025-03-24T13:40:34.429371Z",
     "iopub.status.idle": "2025-03-24T13:40:58.024151Z",
     "shell.execute_reply": "2025-03-24T13:40:58.022696Z",
     "shell.execute_reply.started": "2025-03-24T13:40:34.429678Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'YOLOX'...\n",
      "remote: Enumerating objects: 1928, done.\u001b[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
      "remote: Total 1928 (delta 2), reused 0 (delta 0), pack-reused 1916 (from 2)\u001b[K\n",
      "Receiving objects: 100% (1928/1928), 7.55 MiB | 27.13 MiB/s, done.\n",
      "Resolving deltas: 100% (1148/1148), done.\n",
      "/kaggle/working/YOLOX\n",
      "Obtaining file:///kaggle/working/YOLOX\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: opencv_python in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (4.10.0.84)\n",
      "Requirement already satisfied: loguru in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (0.7.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (4.67.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (0.20.1+cu121)\n",
      "Requirement already satisfied: thop in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (1.11.1.3)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (0.9.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (5.9.5)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (2.17.1)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (2.0.8)\n",
      "Requirement already satisfied: onnx>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (1.17.0)\n",
      "Requirement already satisfied: onnx-simplifier==0.4.10 in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (0.4.10)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from onnx-simplifier==0.4.10->yolox==0.3.0) (13.9.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.13.0->yolox==0.3.0) (3.20.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (2.4.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->yolox==0.3.0) (3.7.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7->yolox==0.3.0) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (1.68.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (3.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (24.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (3.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->yolox==0.3.0) (11.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (2.9.0.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->yolox==0.3.0) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->yolox==0.3.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->yolox==0.3.0) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->yolox==0.3.0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->yolox==0.3.0) (2024.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnx-simplifier==0.4.10->yolox==0.3.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnx-simplifier==0.4.10->yolox==0.3.0) (2.19.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->yolox==0.3.0) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->onnx-simplifier==0.4.10->yolox==0.3.0) (0.1.2)\n",
      "Installing collected packages: yolox\n",
      "  Attempting uninstall: yolox\n",
      "    Found existing installation: yolox 0.3.0\n",
      "    Uninstalling yolox-0.3.0:\n",
      "      Successfully uninstalled yolox-0.3.0\n",
      "  Running setup.py develop for yolox\n",
      "Successfully installed yolox-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Megvii-BaseDetection/YOLOX.git\n",
    "%cd YOLOX\n",
    "!pip install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T13:42:54.720096Z",
     "iopub.status.busy": "2025-03-24T13:42:54.719721Z",
     "iopub.status.idle": "2025-03-24T13:43:39.415751Z",
     "shell.execute_reply": "2025-03-24T13:43:39.414916Z",
     "shell.execute_reply.started": "2025-03-24T13:42:54.720068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///kaggle/working/YOLOX\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: opencv_python in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (4.10.0.84)\n",
      "Requirement already satisfied: loguru in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (0.7.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (4.67.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (0.20.1+cu121)\n",
      "Requirement already satisfied: thop in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (1.11.1.3)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (0.9.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (5.9.5)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (2.17.1)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (2.0.8)\n",
      "Requirement already satisfied: onnx>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (1.17.0)\n",
      "Requirement already satisfied: onnx-simplifier==0.4.10 in /usr/local/lib/python3.10/dist-packages (from yolox==0.3.0) (0.4.10)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from onnx-simplifier==0.4.10->yolox==0.3.0) (13.9.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.13.0->yolox==0.3.0) (3.20.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->yolox==0.3.0) (2.4.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->yolox==0.3.0) (3.7.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->yolox==0.3.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7->yolox==0.3.0) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (1.68.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (3.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (24.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->yolox==0.3.0) (3.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->yolox==0.3.0) (11.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->yolox==0.3.0) (2.9.0.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->yolox==0.3.0) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->yolox==0.3.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->yolox==0.3.0) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->yolox==0.3.0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->yolox==0.3.0) (2024.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnx-simplifier==0.4.10->yolox==0.3.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnx-simplifier==0.4.10->yolox==0.3.0) (2.19.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->yolox==0.3.0) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->onnx-simplifier==0.4.10->yolox==0.3.0) (0.1.2)\n",
      "Installing collected packages: yolox\n",
      "  Attempting uninstall: yolox\n",
      "    Found existing installation: yolox 0.3.0\n",
      "    Uninstalling yolox-0.3.0:\n",
      "      Successfully uninstalled yolox-0.3.0\n",
      "  Running setup.py develop for yolox\n",
      "Successfully installed yolox-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -e /kaggle/working/YOLOX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T13:43:39.417481Z",
     "iopub.status.busy": "2025-03-24T13:43:39.417245Z",
     "iopub.status.idle": "2025-03-24T13:43:39.559899Z",
     "shell.execute_reply": "2025-03-24T13:43:39.558973Z",
     "shell.execute_reply.started": "2025-03-24T13:43:39.417460Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core  evaluators  __init__.py  models\t    tools\n",
      "data  exp\t  layers       __pycache__  utils\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/working/YOLOX/yolox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T13:44:35.184328Z",
     "iopub.status.busy": "2025-03-24T13:44:35.183957Z",
     "iopub.status.idle": "2025-03-24T13:45:01.256045Z",
     "shell.execute_reply": "2025-03-24T13:45:01.255162Z",
     "shell.execute_reply.started": "2025-03-24T13:44:35.184301Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ByteTrack'...\n",
      "remote: Enumerating objects: 2007, done.\u001b[K\n",
      "remote: Total 2007 (delta 0), reused 0 (delta 0), pack-reused 2007 (from 1)\u001b[K\n",
      "Receiving objects: 100% (2007/2007), 79.60 MiB | 47.61 MiB/s, done.\n",
      "Resolving deltas: 100% (1141/1141), done.\n",
      "/kaggle/working/YOLOX/ByteTrack\n",
      "Obtaining file:///kaggle/working/YOLOX/ByteTrack\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Installing collected packages: yolox\n",
      "  Attempting uninstall: yolox\n",
      "    Found existing installation: yolox 0.3.0\n",
      "    Uninstalling yolox-0.3.0:\n",
      "      Successfully uninstalled yolox-0.3.0\n",
      "  Running setup.py develop for yolox\n",
      "Successfully installed yolox-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ifzhang/ByteTrack.git\n",
    "%cd ByteTrack\n",
    "!pip install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T13:46:21.181370Z",
     "iopub.status.busy": "2025-03-24T13:46:21.180998Z",
     "iopub.status.idle": "2025-03-24T13:46:21.309323Z",
     "shell.execute_reply": "2025-03-24T13:46:21.308306Z",
     "shell.execute_reply.started": "2025-03-24T13:46:21.181339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/working/YOLOX/ByteTrack/yolox/tracker /kaggle/working/YOLOX/yolox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:22:58.073698Z",
     "iopub.status.busy": "2025-03-24T14:22:58.073376Z",
     "iopub.status.idle": "2025-03-24T14:23:29.884567Z",
     "shell.execute_reply": "2025-03-24T14:23:29.883699Z",
     "shell.execute_reply.started": "2025-03-24T14:22:58.073674Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ByteTrack'...\n",
      "remote: Enumerating objects: 2007, done.\u001b[K\n",
      "remote: Total 2007 (delta 0), reused 0 (delta 0), pack-reused 2007 (from 1)\u001b[K\n",
      "Receiving objects: 100% (2007/2007), 79.60 MiB | 42.70 MiB/s, done.\n",
      "Resolving deltas: 100% (1141/1141), done.\n",
      "/kaggle/working/ByteTrack\n",
      "Obtaining file:///kaggle/working/ByteTrack\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Installing collected packages: yolox\n",
      "  Running setup.py develop for yolox\n",
      "Successfully installed yolox-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ifzhang/ByteTrack.git\n",
    "%cd ByteTrack\n",
    "!pip install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:41:26.476803Z",
     "iopub.status.busy": "2025-03-24T14:41:26.476499Z",
     "iopub.status.idle": "2025-03-24T14:41:26.484651Z",
     "shell.execute_reply": "2025-03-24T14:41:26.483716Z",
     "shell.execute_reply.started": "2025-03-24T14:41:26.476782Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOX(\n",
      "  (backbone): YOLOPAFPN(\n",
      "    (backbone): CSPDarknet(\n",
      "      (stem): Focus(\n",
      "        (conv): BaseConv(\n",
      "          (conv): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (dark2): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): CSPLayer(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv3): BaseConv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dark3): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): CSPLayer(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv3): BaseConv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dark4): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): CSPLayer(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv3): BaseConv(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dark5): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): SPPBottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): ModuleList(\n",
      "            (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "            (1): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)\n",
      "            (2): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): CSPLayer(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv3): BaseConv(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (lateral_conv0): BaseConv(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (C3_p4): CSPLayer(\n",
      "      (conv1): BaseConv(\n",
      "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv2): BaseConv(\n",
      "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv3): BaseConv(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (reduce_conv1): BaseConv(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (C3_p3): CSPLayer(\n",
      "      (conv1): BaseConv(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv2): BaseConv(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv3): BaseConv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bu_conv2): BaseConv(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (C3_n3): CSPLayer(\n",
      "      (conv1): BaseConv(\n",
      "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv2): BaseConv(\n",
      "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv3): BaseConv(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bu_conv1): BaseConv(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (C3_n4): CSPLayer(\n",
      "      (conv1): BaseConv(\n",
      "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv2): BaseConv(\n",
      "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv3): BaseConv(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): YOLOXHead(\n",
      "    (cls_convs): ModuleList(\n",
      "      (0-2): 3 x Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): BaseConv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (reg_convs): ModuleList(\n",
      "      (0-2): 3 x Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): BaseConv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls_preds): ModuleList(\n",
      "      (0-2): 3 x Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (reg_preds): ModuleList(\n",
      "      (0-2): 3 x Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (obj_preds): ModuleList(\n",
      "      (0-2): 3 x Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (stems): ModuleList(\n",
      "      (0): BaseConv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv(\n",
      "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): BaseConv(\n",
      "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (l1_loss): L1Loss()\n",
      "    (bcewithlog_loss): BCEWithLogitsLoss()\n",
      "    (iou_loss): IOUloss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:47:13.305067Z",
     "iopub.status.busy": "2025-03-24T14:47:13.304746Z",
     "iopub.status.idle": "2025-03-24T14:49:08.229899Z",
     "shell.execute_reply": "2025-03-24T14:49:08.228890Z",
     "shell.execute_reply.started": "2025-03-24T14:47:13.305041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-4f6d847ffe9a>:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"/kaggle/input/yoloxs/yolox_s.pth\", map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MOT format tracking results saved to /kaggle/working/tracking_results.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from yolox.exp import get_exp\n",
    "#from yolox.utils.boxes import postprocess\n",
    "from yolox.tracker.byte_tracker import BYTETracker\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def postprocess(prediction, num_classes=1, conf_thre=0.25, nms_thre=0.45):\n",
    "    \"\"\"Applies confidence thresholding and Non-Maximum Suppression (NMS).\"\"\"\n",
    "    \n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
    "    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
    "    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
    "    prediction[:, :, :4] = box_corner[:, :, :4]\n",
    "\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    \n",
    "    for i, image_pred in enumerate(prediction):  \n",
    "        class_conf, class_pred = torch.max(image_pred[:, 5: 5 + num_classes], 1, keepdim=True)\n",
    "\n",
    "        conf_mask = (image_pred[:, 4] * class_conf.squeeze() >= conf_thre).squeeze()\n",
    "        detections = torch.cat((image_pred[:, :5], class_conf, class_pred.float()), 1)\n",
    "        detections = detections[conf_mask]\n",
    "\n",
    "        if not detections.size(0):\n",
    "            continue\n",
    "        \n",
    "        nms_out_index = torchvision.ops.nms(detections[:, :4], detections[:, 4] * detections[:, 5], nms_thre)\n",
    "        output[i] = detections[nms_out_index]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# ✅ Load YOLOX model\n",
    "exp = get_exp(\"exps/default/yolox_s.py\", \"yolox_s\")\n",
    "model = exp.get_model()\n",
    "ckpt = torch.load(\"/kaggle/input/yoloxs/yolox_s.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "# ✅ Initialize ByteTrack tracker\n",
    "class TrackerArgs:\n",
    "    track_thresh = 0.5\n",
    "    track_buffer = 30\n",
    "    match_thresh = 0.8\n",
    "    aspect_ratio_thresh = 1.6\n",
    "    min_box_area = 10\n",
    "\n",
    "tracker = BYTETracker(TrackerArgs())\n",
    "\n",
    "# ✅ Paths\n",
    "test_images_path = \"/kaggle/input/surveillance-for-retail-stores/tracking/test/01/img1\"\n",
    "mot_output_path = \"/kaggle/working/tracking_results.txt\"\n",
    "\n",
    "# ✅ Tracking results list\n",
    "tracking_results = []\n",
    "\n",
    "# ✅ Function to preprocess image manually\n",
    "def preprocess_image(img, target_size=(640, 640)):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    img = cv2.resize(img, target_size)  # Resize\n",
    "    img = img / 255.0  # Normalize\n",
    "    img = np.transpose(img, (2, 0, 1))  # HWC → CHW\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dim\n",
    "    return torch.tensor(img, dtype=torch.float32)\n",
    "\n",
    "# ✅ Process test frames\n",
    "for img_name in sorted(os.listdir(test_images_path)):\n",
    "    frame_number = int(img_name.split(\".\")[0])\n",
    "    img_path = os.path.join(test_images_path, img_name)\n",
    "    \n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    img_tensor = preprocess_image(img)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "        #print(f\"Model Output Shape: {outputs[0].shape if len(outputs) > 0 else 'No output'}\")\n",
    "\n",
    "        if len(outputs) == 0 or outputs[0].ndim != 3:\n",
    "            continue\n",
    "\n",
    "        detections = postprocess(outputs[0], num_classes=1, conf_thre=0.25, nms_thre=0.45)\n",
    "        print(f\"Detections Output: {detections}\")  # Debugging\n",
    "\n",
    "\n",
    "    if detections is not None:\n",
    "        tracked_objects = tracker.update(detections.cpu().numpy(), target_size, 1.0)\n",
    "        \n",
    "        for track in tracked_objects:\n",
    "            track_id = int(track.track_id)\n",
    "            x, y, w, h = map(int, track.tlwh)\n",
    "            conf = round(track.score, 4)\n",
    "\n",
    "            tracking_results.append([frame_number, track_id, x, y, w, h, conf, 1, 1])\n",
    "\n",
    "# ✅ Sort by (Track ID, Frame Number)\n",
    "tracking_results.sort(key=lambda x: (x[1], x[0]))\n",
    "\n",
    "# ✅ Save results in MOT format\n",
    "with open(mot_output_path, \"w\") as f:\n",
    "    for result in tracking_results:\n",
    "        f.write(\",\".join(map(str, result)) + \"\\n\")\n",
    "\n",
    "print(f\"✅ MOT format tracking results saved to {mot_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:24:54.976196Z",
     "iopub.status.busy": "2025-03-24T14:24:54.975835Z",
     "iopub.status.idle": "2025-03-24T14:25:06.114001Z",
     "shell.execute_reply": "2025-03-24T14:25:06.112896Z",
     "shell.execute_reply.started": "2025-03-24T14:24:54.976165Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cython_bbox\n",
      "  Downloading cython_bbox-0.1.5.tar.gz (4.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from cython_bbox) (3.0.11)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cython_bbox) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->cython_bbox) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->cython_bbox) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->cython_bbox) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->cython_bbox) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->cython_bbox) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->cython_bbox) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->cython_bbox) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->cython_bbox) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->cython_bbox) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->cython_bbox) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->cython_bbox) (2024.2.0)\n",
      "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: cython_bbox\n",
      "  Building wheel for cython_bbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for cython_bbox: filename=cython_bbox-0.1.5-cp310-cp310-linux_x86_64.whl size=99108 sha256=e7671f02dbfaa610cc44e89db855d4d8aa2c216b55ee960182719196a5694ca2\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/b7/68/bab98b7180cda501101a57fb7d36884218ad45ec60c27cd679\n",
      "Successfully built cython_bbox\n",
      "Installing collected packages: faiss-gpu, cython_bbox\n",
      "Successfully installed cython_bbox-0.1.5 faiss-gpu-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install cython_bbox faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:24:39.882594Z",
     "iopub.status.busy": "2025-03-24T14:24:39.882264Z",
     "iopub.status.idle": "2025-03-24T14:24:43.721551Z",
     "shell.execute_reply": "2025-03-24T14:24:43.720385Z",
     "shell.execute_reply.started": "2025-03-24T14:24:39.882571Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lap\n",
      "  Downloading lap-0.5.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from lap) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.6->lap) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.6->lap) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.6->lap) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.6->lap) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.6->lap) (2024.2.0)\n",
      "Downloading lap-0.5.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lap\n",
      "Successfully installed lap-0.5.12\n"
     ]
    }
   ],
   "source": [
    "!pip install lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:24:01.074560Z",
     "iopub.status.busy": "2025-03-24T14:24:01.074213Z",
     "iopub.status.idle": "2025-03-24T14:24:04.878903Z",
     "shell.execute_reply": "2025-03-24T14:24:04.877829Z",
     "shell.execute_reply.started": "2025-03-24T14:24:01.074531Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:24:17.263539Z",
     "iopub.status.busy": "2025-03-24T14:24:17.263240Z",
     "iopub.status.idle": "2025-03-24T14:24:21.065384Z",
     "shell.execute_reply": "2025-03-24T14:24:21.064450Z",
     "shell.execute_reply.started": "2025-03-24T14:24:17.263516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thop\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (3.0.2)\n",
      "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: thop\n",
      "Successfully installed thop-0.1.1.post2209072238\n"
     ]
    }
   ],
   "source": [
    "!pip install thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T18:34:43.157765Z",
     "iopub.status.busy": "2025-03-24T18:34:43.157448Z",
     "iopub.status.idle": "2025-03-24T18:35:37.166061Z",
     "shell.execute_reply": "2025-03-24T18:35:37.165184Z",
     "shell.execute_reply.started": "2025-03-24T18:34:43.157740Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n",
      "Collecting lap>=0.5.12\n",
      "  Downloading lap-0.5.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from lap>=0.5.12) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.6->lap>=0.5.12) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.6->lap>=0.5.12) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\n",
      "Downloading lap-0.5.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 31.3 MB/s eta 0:00:00\n",
      "Installing collected packages: lap\n",
      "Successfully installed lap-0.5.12\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 3.5s, installed 1 package: ['lap>=0.5.12']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "0: 704x704 39 persons, 43.2ms\n",
      "Speed: 8.2ms preprocess, 43.2ms inference, 298.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 43.1ms\n",
      "Speed: 3.1ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 43.1ms\n",
      "Speed: 1.6ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 33 persons, 39.6ms\n",
      "Speed: 2.2ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.5ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 2.2ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 40.2ms\n",
      "Speed: 1.8ms preprocess, 40.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 40.4ms\n",
      "Speed: 1.9ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 39.8ms\n",
      "Speed: 2.0ms preprocess, 39.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 37.7ms\n",
      "Speed: 1.9ms preprocess, 37.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 36.1ms\n",
      "Speed: 2.0ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 36.0ms\n",
      "Speed: 1.8ms preprocess, 36.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 33.8ms\n",
      "Speed: 1.9ms preprocess, 33.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 33.8ms\n",
      "Speed: 2.1ms preprocess, 33.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 33.8ms\n",
      "Speed: 1.8ms preprocess, 33.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 33.8ms\n",
      "Speed: 1.9ms preprocess, 33.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 33.8ms\n",
      "Speed: 1.8ms preprocess, 33.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 33.8ms\n",
      "Speed: 1.7ms preprocess, 33.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 33.7ms\n",
      "Speed: 1.8ms preprocess, 33.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 33.8ms\n",
      "Speed: 1.8ms preprocess, 33.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 33.8ms\n",
      "Speed: 1.7ms preprocess, 33.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 33.8ms\n",
      "Speed: 1.9ms preprocess, 33.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 33.8ms\n",
      "Speed: 1.7ms preprocess, 33.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 33.8ms\n",
      "Speed: 1.9ms preprocess, 33.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 33.3ms\n",
      "Speed: 1.9ms preprocess, 33.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 32.2ms\n",
      "Speed: 1.9ms preprocess, 32.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 32.2ms\n",
      "Speed: 1.8ms preprocess, 32.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 32.2ms\n",
      "Speed: 1.9ms preprocess, 32.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 32.3ms\n",
      "Speed: 2.1ms preprocess, 32.3ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 32.3ms\n",
      "Speed: 2.0ms preprocess, 32.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 32.3ms\n",
      "Speed: 2.0ms preprocess, 32.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 32.3ms\n",
      "Speed: 1.9ms preprocess, 32.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 32.3ms\n",
      "Speed: 1.6ms preprocess, 32.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 32.3ms\n",
      "Speed: 1.9ms preprocess, 32.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 32.3ms\n",
      "Speed: 1.8ms preprocess, 32.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 32.2ms\n",
      "Speed: 1.8ms preprocess, 32.2ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 32.2ms\n",
      "Speed: 1.8ms preprocess, 32.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 32.0ms\n",
      "Speed: 2.0ms preprocess, 32.0ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 31.3ms\n",
      "Speed: 2.0ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 31.4ms\n",
      "Speed: 2.0ms preprocess, 31.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 31.3ms\n",
      "Speed: 2.0ms preprocess, 31.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 31.3ms\n",
      "Speed: 1.9ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 31.3ms\n",
      "Speed: 1.9ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 31.3ms\n",
      "Speed: 1.9ms preprocess, 31.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 31.3ms\n",
      "Speed: 1.7ms preprocess, 31.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 31.3ms\n",
      "Speed: 1.7ms preprocess, 31.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 31.4ms\n",
      "Speed: 1.7ms preprocess, 31.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 31.3ms\n",
      "Speed: 1.8ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 31.3ms\n",
      "Speed: 1.8ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 31.3ms\n",
      "Speed: 1.8ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 31.3ms\n",
      "Speed: 1.8ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 31.3ms\n",
      "Speed: 1.8ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 31.3ms\n",
      "Speed: 1.8ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 31.3ms\n",
      "Speed: 1.8ms preprocess, 31.3ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 31.3ms\n",
      "Speed: 1.8ms preprocess, 31.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 31.3ms\n",
      "Speed: 1.9ms preprocess, 31.3ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 31.4ms\n",
      "Speed: 2.1ms preprocess, 31.4ms inference, 1.7ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 32.3ms\n",
      "Speed: 2.0ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 32.3ms\n",
      "Speed: 2.5ms preprocess, 32.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 32.8ms\n",
      "Speed: 2.2ms preprocess, 32.8ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 32.8ms\n",
      "Speed: 2.0ms preprocess, 32.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 33.3ms\n",
      "Speed: 2.0ms preprocess, 33.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 33.3ms\n",
      "Speed: 2.1ms preprocess, 33.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 33.8ms\n",
      "Speed: 2.1ms preprocess, 33.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 33.8ms\n",
      "Speed: 2.1ms preprocess, 33.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 33.8ms\n",
      "Speed: 2.0ms preprocess, 33.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 33.8ms\n",
      "Speed: 1.8ms preprocess, 33.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 34.3ms\n",
      "Speed: 2.2ms preprocess, 34.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 34.3ms\n",
      "Speed: 2.0ms preprocess, 34.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 34.8ms\n",
      "Speed: 1.9ms preprocess, 34.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 34.9ms\n",
      "Speed: 1.9ms preprocess, 34.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 34.9ms\n",
      "Speed: 2.2ms preprocess, 34.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 34.9ms\n",
      "Speed: 1.9ms preprocess, 34.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 35.5ms\n",
      "Speed: 2.0ms preprocess, 35.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 35.5ms\n",
      "Speed: 1.8ms preprocess, 35.5ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 35.5ms\n",
      "Speed: 1.8ms preprocess, 35.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 32 persons, 35.5ms\n",
      "Speed: 2.2ms preprocess, 35.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 35.5ms\n",
      "Speed: 1.8ms preprocess, 35.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 35.5ms\n",
      "Speed: 1.7ms preprocess, 35.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 36.1ms\n",
      "Speed: 1.9ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 36.1ms\n",
      "Speed: 1.9ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 36.1ms\n",
      "Speed: 1.7ms preprocess, 36.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 36.1ms\n",
      "Speed: 1.9ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 36.1ms\n",
      "Speed: 1.9ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 34 persons, 34.7ms\n",
      "Speed: 1.6ms preprocess, 34.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 32.3ms\n",
      "Speed: 1.8ms preprocess, 32.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 32.2ms\n",
      "Speed: 1.7ms preprocess, 32.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 32.3ms\n",
      "Speed: 1.6ms preprocess, 32.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 32.3ms\n",
      "Speed: 1.8ms preprocess, 32.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 32.3ms\n",
      "Speed: 1.8ms preprocess, 32.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 32.2ms\n",
      "Speed: 1.9ms preprocess, 32.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 32.3ms\n",
      "Speed: 1.7ms preprocess, 32.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 32.3ms\n",
      "Speed: 2.0ms preprocess, 32.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 32.2ms\n",
      "Speed: 1.8ms preprocess, 32.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 32.3ms\n",
      "Speed: 1.7ms preprocess, 32.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 32.2ms\n",
      "Speed: 1.8ms preprocess, 32.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 29.2ms\n",
      "Speed: 1.7ms preprocess, 29.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 29.2ms\n",
      "Speed: 1.8ms preprocess, 29.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 29.2ms\n",
      "Speed: 1.9ms preprocess, 29.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.2ms\n",
      "Speed: 1.8ms preprocess, 29.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.2ms\n",
      "Speed: 1.8ms preprocess, 29.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.2ms\n",
      "Speed: 1.8ms preprocess, 29.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 29.2ms\n",
      "Speed: 2.0ms preprocess, 29.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 29.3ms\n",
      "Speed: 1.9ms preprocess, 29.3ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 29.2ms\n",
      "Speed: 1.8ms preprocess, 29.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 29.2ms\n",
      "Speed: 2.0ms preprocess, 29.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 29.2ms\n",
      "Speed: 2.1ms preprocess, 29.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 29.2ms\n",
      "Speed: 1.8ms preprocess, 29.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 29.2ms\n",
      "Speed: 2.0ms preprocess, 29.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.2ms\n",
      "Speed: 1.9ms preprocess, 29.2ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 29.2ms\n",
      "Speed: 1.8ms preprocess, 29.2ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 29.2ms\n",
      "Speed: 1.8ms preprocess, 29.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 29.2ms\n",
      "Speed: 1.8ms preprocess, 29.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 28.6ms\n",
      "Speed: 2.1ms preprocess, 28.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 28.5ms\n",
      "Speed: 1.8ms preprocess, 28.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 28.5ms\n",
      "Speed: 1.9ms preprocess, 28.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 28.5ms\n",
      "Speed: 1.8ms preprocess, 28.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 28.5ms\n",
      "Speed: 2.0ms preprocess, 28.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 28.5ms\n",
      "Speed: 2.0ms preprocess, 28.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 35 persons, 28.5ms\n",
      "Speed: 1.8ms preprocess, 28.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 36 persons, 28.5ms\n",
      "Speed: 1.8ms preprocess, 28.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 28.5ms\n",
      "Speed: 1.6ms preprocess, 28.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 28.5ms\n",
      "Speed: 1.6ms preprocess, 28.5ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 28.5ms\n",
      "Speed: 1.9ms preprocess, 28.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 28.5ms\n",
      "Speed: 2.2ms preprocess, 28.5ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 28.5ms\n",
      "Speed: 1.8ms preprocess, 28.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 28.4ms\n",
      "Speed: 1.8ms preprocess, 28.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 28.4ms\n",
      "Speed: 1.9ms preprocess, 28.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 28.5ms\n",
      "Speed: 1.8ms preprocess, 28.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 28.4ms\n",
      "Speed: 1.8ms preprocess, 28.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 28.5ms\n",
      "Speed: 1.9ms preprocess, 28.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 28.5ms\n",
      "Speed: 1.7ms preprocess, 28.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 32.2ms\n",
      "Speed: 1.8ms preprocess, 32.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 32.2ms\n",
      "Speed: 1.7ms preprocess, 32.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 32.7ms\n",
      "Speed: 1.7ms preprocess, 32.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 32.7ms\n",
      "Speed: 1.8ms preprocess, 32.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 33.2ms\n",
      "Speed: 2.1ms preprocess, 33.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 33.2ms\n",
      "Speed: 1.8ms preprocess, 33.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 33.3ms\n",
      "Speed: 1.9ms preprocess, 33.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 33.2ms\n",
      "Speed: 1.8ms preprocess, 33.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 33.6ms\n",
      "Speed: 1.7ms preprocess, 33.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 33.8ms\n",
      "Speed: 1.8ms preprocess, 33.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 33.7ms\n",
      "Speed: 1.7ms preprocess, 33.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 31.4ms\n",
      "Speed: 1.7ms preprocess, 31.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 31.3ms\n",
      "Speed: 1.8ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 31.4ms\n",
      "Speed: 1.9ms preprocess, 31.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 31.3ms\n",
      "Speed: 1.8ms preprocess, 31.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 31.3ms\n",
      "Speed: 1.9ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 31.3ms\n",
      "Speed: 1.8ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 31.3ms\n",
      "Speed: 1.9ms preprocess, 31.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 31.4ms\n",
      "Speed: 2.1ms preprocess, 31.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 31.3ms\n",
      "Speed: 1.9ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 31.3ms\n",
      "Speed: 1.9ms preprocess, 31.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 31.3ms\n",
      "Speed: 2.0ms preprocess, 31.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 31.3ms\n",
      "Speed: 1.8ms preprocess, 31.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 31.3ms\n",
      "Speed: 1.9ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 31.3ms\n",
      "Speed: 1.9ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 31.3ms\n",
      "Speed: 2.0ms preprocess, 31.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 31.3ms\n",
      "Speed: 1.9ms preprocess, 31.3ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 36.1ms\n",
      "Speed: 2.3ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 36.1ms\n",
      "Speed: 2.0ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 36.7ms\n",
      "Speed: 2.0ms preprocess, 36.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 36.8ms\n",
      "Speed: 1.9ms preprocess, 36.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 36.7ms\n",
      "Speed: 1.5ms preprocess, 36.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 36.8ms\n",
      "Speed: 2.0ms preprocess, 36.8ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 36.8ms\n",
      "Speed: 2.2ms preprocess, 36.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 36.7ms\n",
      "Speed: 1.8ms preprocess, 36.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 36.7ms\n",
      "Speed: 1.7ms preprocess, 36.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 36.7ms\n",
      "Speed: 1.8ms preprocess, 36.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 37.4ms\n",
      "Speed: 1.5ms preprocess, 37.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 38.5ms\n",
      "Speed: 1.6ms preprocess, 38.5ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 38.9ms\n",
      "Speed: 1.6ms preprocess, 38.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 38.9ms\n",
      "Speed: 1.8ms preprocess, 38.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.0ms\n",
      "Speed: 1.9ms preprocess, 39.0ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 38.9ms\n",
      "Speed: 1.9ms preprocess, 38.9ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 38.8ms\n",
      "Speed: 1.8ms preprocess, 38.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 38.8ms\n",
      "Speed: 1.8ms preprocess, 38.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 38.8ms\n",
      "Speed: 1.8ms preprocess, 38.8ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 38.8ms\n",
      "Speed: 1.6ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 40.2ms\n",
      "Speed: 1.8ms preprocess, 40.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 39.9ms\n",
      "Speed: 1.7ms preprocess, 39.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 40.4ms\n",
      "Speed: 1.7ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 49 persons, 40.4ms\n",
      "Speed: 1.7ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 51 persons, 40.4ms\n",
      "Speed: 2.0ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 40.4ms\n",
      "Speed: 1.7ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 40.4ms\n",
      "Speed: 1.9ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 40.4ms\n",
      "Speed: 1.7ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 48 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 50 persons, 39.6ms\n",
      "Speed: 1.5ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.7ms\n",
      "Speed: 2.1ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 41.3ms\n",
      "Speed: 1.8ms preprocess, 41.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 41.3ms\n",
      "Speed: 2.1ms preprocess, 41.3ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.7ms\n",
      "Speed: 1.7ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.0ms\n",
      "Speed: 1.8ms preprocess, 39.0ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 38.8ms\n",
      "Speed: 1.8ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 38.8ms\n",
      "Speed: 1.9ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 38.8ms\n",
      "Speed: 2.0ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 38.8ms\n",
      "Speed: 1.8ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 38.8ms\n",
      "Speed: 2.0ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 38.8ms\n",
      "Speed: 1.6ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 38.8ms\n",
      "Speed: 1.8ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 38.8ms\n",
      "Speed: 1.8ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 38.8ms\n",
      "Speed: 1.7ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 37.9ms\n",
      "Speed: 1.9ms preprocess, 37.9ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 37.4ms\n",
      "Speed: 2.0ms preprocess, 37.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 36.8ms\n",
      "Speed: 1.8ms preprocess, 36.8ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 36.1ms\n",
      "Speed: 2.0ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 36.1ms\n",
      "Speed: 1.9ms preprocess, 36.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 36.1ms\n",
      "Speed: 1.9ms preprocess, 36.1ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 36.1ms\n",
      "Speed: 2.0ms preprocess, 36.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 36.1ms\n",
      "Speed: 2.0ms preprocess, 36.1ms inference, 2.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 36.1ms\n",
      "Speed: 1.7ms preprocess, 36.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 36.1ms\n",
      "Speed: 1.9ms preprocess, 36.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 36.1ms\n",
      "Speed: 1.8ms preprocess, 36.1ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 2.3ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 41.3ms\n",
      "Speed: 1.9ms preprocess, 41.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 41.3ms\n",
      "Speed: 1.9ms preprocess, 41.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 41.3ms\n",
      "Speed: 1.8ms preprocess, 41.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 42.1ms\n",
      "Speed: 1.8ms preprocess, 42.1ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 42.2ms\n",
      "Speed: 1.9ms preprocess, 42.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 2.2ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 47 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 41.3ms\n",
      "Speed: 1.7ms preprocess, 41.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 41.3ms\n",
      "Speed: 1.8ms preprocess, 41.3ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 2.2ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 37 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 40.2ms\n",
      "Speed: 2.0ms preprocess, 40.2ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 40.4ms\n",
      "Speed: 1.7ms preprocess, 40.4ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.6ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.7ms\n",
      "Speed: 2.0ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.7ms\n",
      "Speed: 1.9ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 2.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.9ms\n",
      "Speed: 2.8ms preprocess, 39.9ms inference, 1.8ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.7ms\n",
      "Speed: 1.7ms preprocess, 39.7ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.7ms\n",
      "Speed: 2.3ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.7ms\n",
      "Speed: 2.5ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 40.4ms\n",
      "Speed: 1.9ms preprocess, 40.4ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.7ms\n",
      "Speed: 2.0ms preprocess, 39.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 43 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 46 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 45 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 44 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 2.1ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 38 persons, 39.7ms\n",
      "Speed: 1.8ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 41.1ms\n",
      "Speed: 1.8ms preprocess, 41.1ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 41.3ms\n",
      "Speed: 1.9ms preprocess, 41.3ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.5ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 39 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.9ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 41 persons, 39.6ms\n",
      "Speed: 1.7ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.4ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.7ms\n",
      "Speed: 2.1ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 42 persons, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n",
      "\n",
      "0: 704x704 40 persons, 39.7ms\n",
      "Speed: 2.0ms preprocess, 39.7ms inference, 1.2ms postprocess per image at shape (1, 3, 704, 704)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "#import motmetrics as mm  # New import for MOTA calculation\n",
    "\n",
    "def enhance_tracking_performance(input_path, output_path):\n",
    "    # Load the YOLO model with more robust tracking\n",
    "    model2 = YOLO('/kaggle/input/trackingyolo/pytorch/default/6/best.pt')\n",
    "    \n",
    "    # Enhanced tracking parameters\n",
    "    tracking_config = {\n",
    "        'iou_threshold': 0.5,  # Adaptive IOU threshold\n",
    "        'conf_threshold': 0.3,  # Confidence threshold with some wiggle room\n",
    "        'max_age': 30,  # Maximum frames to keep a track alive without detection\n",
    "        'min_hits': 3,  # Minimum number of hits to consider a track valid\n",
    "    }\n",
    "    \n",
    "    # Additional tracking enhancement techniques\n",
    "    tracking_results = []\n",
    "    track_history = {}\n",
    "    occlusion_tracker = {}\n",
    "    \n",
    "    # Load ground truth for MOTA calculation\n",
    "    #gt_data = pd.read_csv(gt_path, header=None, names=['frame', 'id', 'x', 'y', 'w', 'h', 'conf', 'class', 'visibility'])\n",
    "    \n",
    "    for img_name in sorted(os.listdir(input_path)):\n",
    "        frame_number = int(img_name.split(\".\")[0])\n",
    "        img_path = os.path.join(input_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is None:\n",
    "            continue\n",
    "        \n",
    "        # Resize and preprocess image\n",
    "        img_resized = cv2.resize(img, (704, 704))\n",
    "        \n",
    "        # Advanced tracking with more robust parameters\n",
    "        results = model2.track(\n",
    "            img_resized, \n",
    "            persist=True, \n",
    "            iou=tracking_config['iou_threshold'], \n",
    "            conf=tracking_config['conf_threshold']\n",
    "        )[0]\n",
    "        \n",
    "        for track in results.boxes:\n",
    "            # Enhanced track ID and confidence handling\n",
    "            track_id = int(track.id[0].item()) if track.id is not None and len(track.id) > 0 else -1\n",
    "            conf = track.conf[0].item()\n",
    "            cls = int(track.cls[0].item())\n",
    "            \n",
    "            if cls == 0:  # Person class\n",
    "                # Coordinate scaling and conversion\n",
    "                x_center, y_center, w, h = track.xywh[0].cpu().numpy()\n",
    "                scale_x = 1920 / 704\n",
    "                scale_y = 1080 / 704\n",
    "                \n",
    "                x_center *= scale_x\n",
    "                y_center *= scale_y\n",
    "                w *= scale_x\n",
    "                h *= scale_y\n",
    "                \n",
    "                x = int(x_center - w / 2)\n",
    "                y = int(y_center - h / 2)\n",
    "                w, h = int(w), int(h)\n",
    "                \n",
    "                # Occlusion and ID stability handling\n",
    "                if track_id in track_history:\n",
    "                    prev_x, prev_y = track_history[track_id]\n",
    "                    distance = np.sqrt((x_center - prev_x)**2 + (y_center - prev_y)**2)\n",
    "                    \n",
    "                    # Adaptive ID management\n",
    "                    if distance > 100:  # Large movement might indicate track switch\n",
    "                        track_id = max(track_history.keys()) + 1 if track_history else 1\n",
    "                \n",
    "                track_history[track_id] = (x_center, y_center)\n",
    "                \n",
    "                # Robust tracking result storage\n",
    "                tracking_results.append([\n",
    "                    frame_number, track_id, x, y, w, h, \n",
    "                    round(conf, 4), 1, 1\n",
    "                ])\n",
    "    \n",
    "    # Post-processing for track stability\n",
    "    track_id_data = {}\n",
    "    for row in tracking_results:\n",
    "        frame_number, track_id = row[0], row[1]\n",
    "        if track_id not in track_id_data:\n",
    "            track_id_data[track_id] = []\n",
    "        track_id_data[track_id].append(row)\n",
    "    \n",
    "    # Remove very short tracks\n",
    "    track_id_data = {k: v for k, v in track_id_data.items() if len(v) >= 20}\n",
    "    \n",
    "    # Save processed tracking results\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for track_id in sorted(track_id_data.keys()):\n",
    "            for row in sorted(track_id_data[track_id], key=lambda x: x[0]):\n",
    "                f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "# Usage\n",
    "enhance_tracking_performance(\n",
    "    \"/kaggle/input/surveillance-for-retail-stores/tracking/test/01/img1\", \n",
    "    \"/kaggle/working/mot_res.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import motmetrics as mm\n",
    "import numpy as np\n",
    "\n",
    "# Load ground truth and predicted tracks\n",
    "def load_tracks(file_path):\n",
    "    data = np.loadtxt(file_path, delimiter=',')\n",
    "    return data\n",
    "\n",
    "gt_file = '/kaggle/input/filteredgt/filtered_gt.txt'\n",
    "pred_file = '/kaggle/working/bytetracking_ftlast_results.txt'\n",
    "\n",
    "gt_data = load_tracks(gt_file)\n",
    "pred_data = load_tracks(pred_file)\n",
    "\n",
    "# Create an accumulator to store results\n",
    "acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "# Iterate through each frame\n",
    "for frame in np.unique(gt_data[:, 0]):\n",
    "    # Extract ground truth and predictions for the current frame\n",
    "    gt_objects = gt_data[gt_data[:, 0] == frame]\n",
    "    pred_objects = pred_data[pred_data[:, 0] == frame]\n",
    "\n",
    "    # Extract bounding boxes and IDs\n",
    "    gt_boxes = gt_objects[:, 2:6]  # x, y, width, height\n",
    "    gt_ids = gt_objects[:, 1]\n",
    "\n",
    "    pred_boxes = pred_objects[:, 2:6]\n",
    "    pred_ids = pred_objects[:, 1]\n",
    "\n",
    "    # Compute pairwise distances (e.g., IoU)\n",
    "    dist_matrix = mm.distances.iou_matrix(gt_boxes, pred_boxes, max_iou=0.5)\n",
    "\n",
    "    # Update the accumulator\n",
    "    acc.update(gt_ids, pred_ids, dist_matrix)\n",
    "\n",
    "# Compute metrics\n",
    "mh = mm.metrics.create()\n",
    "summary = mh.compute(acc, metrics=['mota', 'motp', 'idf1', 'precision', 'recall'], name='acc')\n",
    "\n",
    "# Print the results\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10699058,
     "sourceId": 89993,
     "sourceType": "competition"
    },
    {
     "datasetId": 6735726,
     "sourceId": 10864605,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6763911,
     "sourceId": 10884972,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6773681,
     "sourceId": 10899243,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6773507,
     "sourceId": 10899458,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6773597,
     "sourceId": 10899544,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6774774,
     "sourceId": 10900708,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6779644,
     "sourceId": 10907214,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6791564,
     "sourceId": 10923786,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6791578,
     "sourceId": 10923959,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6808722,
     "sourceId": 10946645,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6814677,
     "sourceId": 10954534,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6827128,
     "sourceId": 10971833,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6808928,
     "sourceId": 11120154,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6955864,
     "sourceId": 11149400,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 255602,
     "modelInstanceId": 233889,
     "sourceId": 273682,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 268482,
     "modelInstanceId": 246939,
     "sourceId": 288795,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 268482,
     "modelInstanceId": 246939,
     "sourceId": 288818,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 275409,
     "modelInstanceId": 253988,
     "sourceId": 296733,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
